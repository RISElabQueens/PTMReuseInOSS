signature,github_url,stars,fileID,file_path,param,desc
transformers.AutoTokenizer.from_pretrained;transformers.CanineTokenizer.from_pretrained,github.com/bminixhofer/wtpsplit,422,005a31ef6f80b21ad5f735b7975730bc,bminixhofer_wtpsplit/wtpsplit/wtpsplit/evaluation/punct_annotation.py,unknown;google/canine-s,Code for Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForCausalLM.from_pretrained,github.com/salesforce/CodeRL,436,99bb01e055289bf9dc38a7536069f7dd,salesforce_CodeRL/CodeRL/transformers/examples/flax/language-modeling/run_clm_flax.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,config=config,seed=unknown,dtype=getattr",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/tart,130,28fe50e58029e8f206bd4a9d99d435c8,facebookresearch_tart/tart/TART/interactive.py,model_name_or_path;model_name_or_path,"Code and model release for the paper ""Task-aware Retrieval with Instructions"" by Asai et al."
transformers.AutoTokenizer.from_pretrained.save_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.eval,github.com/salesforce/CodeRL,436,58ec4986628dd73d605cee21602fb86a,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/seq2seq-distillation/make_student.py,save_path;,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.TFEncoderDecoderModel.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,58d3a7e3c248674afb3e4dc0d001faa6,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py,"pretrained_model_name_or_path,unknown,None=kwargs","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained.to,github.com/Guzpenha/transformer_rankers,155,044aca96fd3934ad144a8c5754137b5f,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/scripts/apply_response2context.py,model_checkpoint;device,A library to conduct ranking experiments with transformers.
transformers.RagRetriever.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,572db94f7c573dd220705d384adad448,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag-end2end-retriever/eval_rag.py,"checkpoint,None=model_kwargs",Source code for Zalo AI 2021 submission
transformers.GPT2LMHeadModel.from_pretrained,github.com/kmeng01/memit,325,3eda6d14bb0a253cc59496200da00e51,kmeng01_memit/memit/baselines/mend/algs/mend.py,gpt2,Mass-editing thousands of facts into a transformer memory (ICLR 2023)
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForCausalLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,8ac71dd2a3917b769ca7f148aef46488,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/flax/language-modeling/run_clm_flax.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,config=config,seed=unknown,dtype=getattr",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,36ea8fe5dfe74deefff32701ea8d5992,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/text2sql/scripts/mmqa/annotate_binder_program.py,pretrained_model_name_or_path=os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/qiuhuachuan/smile,203,ee4840b2dc945e992215ca54b0c49952,qiuhuachuan_smile/smile/src/chatglm6b_lora_eval.py,"THUDM/chatglm-6b,revision=v0.1.0,trust_remote_code=True;THUDM/chatglm-6b,trust_remote_code=True",SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/naver/gdc,112,6c8791de3f648a0ae87ef7ba7a3540d5,naver_gdc/gdc/rm_vs_dm/gdc/gdc/scorer.py,gpt2-medium;gpt2-medium,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.MBartConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,e932dc9c7a2e84272dadd59e469cbd7d,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py,"hf_config_path,vocab_size=vocab_size","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,d5295616044635175d827474ae04d46d,naver_gdc/gdc/rm_vs_dm/gdc/gdc/metrics.py,gpt2,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.FlavaImageCodebookConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,0db4d3ab2b913c9fb9daa296e6cf749a,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/flava/convert_dalle_to_flava_codebook.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoFeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,b0149366ebde854c9edf5e32b29b4998,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/resnet/convert_resnet_to_pytorch.py,facebook/convnext-base-224-22k-1k,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.Wav2Vec2Config.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,e104d9af1f95c30ed1efe1f36f3a853d,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py,unknown;unknown,Source code for Zalo AI 2021 submission
transformers.models.bart.BartForConditionalGeneration.from_pretrained,github.com/seujung/KoBART-summarization,179,910e400155cf628983e47f8d91cfe785,seujung_KoBART-summarization/KoBART-summarization/infer.py,./kobart_summary,Summarization module based on KoBART
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,9d760af1abcbf47e684cf34c09d7acd4,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/proton/utils/example.py,os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,14340cb3fef47d9b3e126c545200ec52,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/question-answering/run_squad_trainer.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=False;unknown,from_tf=bool,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/shibing624/MedicalGPT,1876,774d19f12a25acffea65aaf462453544,shibing624_MedicalGPT/MedicalGPT/rl_training.py,"unknown,load_in_8bit=unknown,cache_dir=unknown,torch_dtype=torch_dtype;unknown,None=tokenizer_kwargs",MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. 训练医疗大模型，实现了包括增量预训练、有监督微调、RLHF(奖励建模、强化学习训练)和DPO(直接偏好优化)。
transformers.GPT2LMHeadModel.from_pretrained,github.com/salesforce/CodeRL,436,c21930494ee0aac744fe925407548b98,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/bertology/run_prune_gpt.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.HubertConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f5dfc69170993ec83289f3837ed32cad,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py,config_path,Source code for Zalo AI 2021 submission
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,51afd9e51af4c82e25c975b1825f0495,ylsung_VL_adapter/VL_adapter/VL-T5/src/refcoco_data.py,"unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.XLMProphetNetForConditionalGeneration.from_pretrained;transformers.ProphetNetForConditionalGeneration.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,ca21e112eaf90ddf7cdcc6a725ac8b46,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py,"prophetnet_checkpoint_path,output_loading_info=True;prophetnet_checkpoint_path,output_loading_info=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,0d088ea896b9ac93d0bb790b164e7cfc,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/testunseen-nometa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/qiuhuachuan/smile,203,d0d25be7b061a44cbc1a08d8813d0de3,qiuhuachuan_smile/smile/src/finetune.py,"THUDM/chatglm-6b,trust_remote_code=True;THUDM/chatglm-6b,revision=v0.1.0,load_in_8bit=True,trust_remote_code=True,device_map=auto",SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,8e7b3f41b0544bda7b941bea66f2716e,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/text-classification/run_tf_text_classification.py,"unknown,cache_dir=unknown;unknown,num_labels=len,label2id=label2id,id2label=Dict,finetuning_task=text-classification,cache_dir=unknown;unknown,from_pt=bool,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.BertModel.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/zjunlp/OntoProtein,122,2901466cd18d95b24de3bb697dfe0c24,zjunlp_OntoProtein/OntoProtein/src/benchmark/GNN_PPI/convert.py,pretrained_model_path;pretrained_model_path,"Code and datasets for the ICLR2022 paper ""OntoProtein: Protein Pretraining With Gene Ontology Embedding"""
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForMultipleChoice.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,2b798b75c37acd38e6c2bf1c3926a280,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/multiple-choice/run_swag.py,"config_path,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;model_path,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.ViTForImageClassification.from_pretrained;transformers.ViTFeatureExtractor.from_pretrained,github.com/qanastek/HugsVision,185,1c0a0ae59f9914181969c01a8a34ac76,qanastek_HugsVision/HugsVision/recipes/pneumothorax/binary_classification/train_example_vit.py,"huggingface_model,num_labels=len,label2id=label2id,id2label=id2label;huggingface_model",HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,a311671df5078a1aeeeee8889b0ce138,salesforce_CodeRL/CodeRL/transformers/examples/flax/image-captioning/create_model_from_encoder_decoder_models.py,unknown;unknown;unknown;unknown;unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertForSequenceClassification.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/NorskRegnesentral/skweak,899,c5be57cfc265c579a3d754de322692f8,NorskRegnesentral_skweak/skweak/examples/sentiment/sentiment_models.py,"../data/sentiment/models/sst,num_labels=3;bert-base-multilingual-uncased",skweak: A software toolkit for weak supervision applied to NLP tasks
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/Mikubill/naifu-diffusion,222,1d6729d46a8b38b745146e43272552d9,Mikubill_naifu-diffusion/naifu-diffusion/experiment/custom_encoder.py,"unknown,subfolder=tokenizer;unknown,subfolder=text_encoder",Train stable diffusion model with Diffusers and Pytorch Lightning
transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained,github.com/tcapelle/apple_m1_pro_python,160,5ec62abfeaf39e327c8d2fd33fad91bf,tcapelle_apple_m1_pro_python/apple_m1_pro_python/tensorflow/train_bert.py,"model_name;model_name,num_labels=num_labels",A collection of ML scripts to test the M1 Pro MacBook Pro 
transformers.AutoConfig.from_pretrained,github.com/facebookresearch/tart,130,ebf2ce7db96b972135f23d1bc227514a,facebookresearch_tart/tart/TART/finetuning_tart_full.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown","Code and model release for the paper ""Task-aware Retrieval with Instructions"" by Asai et al."
transformers.Wav2Vec2ForCTC.from_pretrained,github.com/salesforce/CodeRL,436,bf53e20e38b8de381bf683fdc746f651,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/wav2vec2/run_common_voice.py,"unknown,cache_dir=unknown,activation_dropout=unknown,attention_dropout=unknown,hidden_dropout=unknown,feat_proj_dropout=unknown,mask_time_prob=unknown,gradient_checkpointing=unknown,layerdrop=unknown,ctc_loss_reduction=mean,pad_token_id=unknown,vocab_size=len",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained.to.eval,github.com/thunlp/KernelGAT,166,d81fe7876d0d2126b7c545d81c04f698,thunlp_KernelGAT/KernelGAT/scikgat/rationale_selection/transformer.py,unknown;,The source codes for Fine-grained Fact Verification with Kernel Graph Attention Network.
transformers.BartConfig.from_pretrained,github.com/fastnlp/CPT,448,d17916439a5fd43e5fbe8162fb09c883,fastnlp_CPT/CPT/pretrain/megatron/model/bart_model.py,unknown,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.HubertConfig.from_pretrained;transformers.HubertForSequenceClassification.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,fcecc05faf0dca6a449bdc2d3905411c,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py,"config_path;base_model_name,config=hf_congfig;base_model_name,return_attention_mask=True,do_normalize=False",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.MT5EncoderModel.from_pretrained;transformers.BertModel.from_pretrained;transformers.XLMRobertaModel.from_pretrained.half,github.com/ai-forever/Kandinsky-2,2534,199acf45c156a2c918fbd55807600c90,ai-forever_Kandinsky-2/Kandinsky-2/kandinsky2/model/text_encoders.py,config;model_path;model_path;model_path;,Kandinsky 2 — multilingual text2image latent diffusion model
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,680246b0c972aadf67bb1b5b3b17daf7,yangheng95_PyABSA/PyABSA/pyabsa/tasks/RNARegression/instructor/rnar_instructor.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/andreamad8/FSB,119,bd9b3b3e52772c921d76ebf8a9762478,andreamad8_FSB/FSB/retrievers/test_tries.py,model_checkpoint;model_checkpoint,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
transformers.RobertaTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,8cb76660ba38f58b43a3a719404176a3,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/vision_encoder_decoder/convert_trocr_unilm_to_pytorch.py,roberta-large,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d6a2a07f5a034902b0c2e4919f2396d1,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/data_all_in/map_subword_question.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.models.auto.configuration_auto.AutoConfig.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,afed1141635cf7d0039366c502a6684c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/seq2seq/models/auto_factory.py,"pretrained_model_name_or_path,return_unused_kwargs=True,trust_remote_code=trust_remote_code,None=kwargs",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.LlamaForCausalLM.from_pretrained;transformers.LlamaTokenizer.from_pretrained,github.com/danielgross/LlamaAcademy,1206,56f0aa311725ee2f57401e8876446ea5,danielgross_LlamaAcademy/LlamaAcademy/main.py,"jeffwan/vicuna-13b,load_in_8bit=True,device_map=device_map;jeffwan/vicuna-13b,model_max_length=2048,padding_side=right,use_fast=False",A school for camelids
transformers.RobertaTokenizerFast.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,2e0c2b532fb396ae66f96756c1ebf597,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/spectra/spectra-trippy/utils_dst.py,roberta-base;bert-base-uncased,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.models.bert.tokenization_bert.BertTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,0289deb14336ceb613f030d3ee18fc12,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/mlm_wwm/run_chinese_ref.py,unknown,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,74acc1544a6fa316dd2f71ff3c59e5d3,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/seq2seq/finetune_trainer.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=unknown,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.RobertaTokenizer.from_pretrained,github.com/haoheliu/AudioLDM,2018,434724dce990b1f54e823c929b1fe0aa,haoheliu_AudioLDM/AudioLDM/audioldm/clap/training/infer_demo.py,roberta-base,"AudioLDM: Generate speech, sound effects, music and beyond, with text."
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/DavidHuji/CapDec,153,4ad8569da4d798baacbd63f2ebb3205f,DavidHuji_CapDec/CapDec/train.py,gpt2_type;gpt2,"CapDec: SOTA Zero Shot Image Captioning Using CLIP and GPT2, EMNLP 2022 (findings)"
transformers.BertTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,c3cc008504d610604f06a6cb4ae08a8c,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/vilt/convert_vilt_original_to_pytorch.py,bert-base-uncased,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.FlaxAutoModel.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.FlaxAutoModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f97c1f56b2af54b29fd3cdabefb4c5e8,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/jax-projects/hybrid_clip/modeling_hybrid_clip.py,"text_model_name_or_path;text_model_name_or_path,unknown,None=kwargs_text;vision_model_name_or_path;vision_model_name_or_path,unknown,None=kwargs_vision",Source code for Zalo AI 2021 submission
transformers.BertTokenizerFast.from_pretrained,github.com/IBM/zshot,285,aacadca53a961fb469fdec4bee15de53,IBM_zshot/zshot/zshot/mentions_extractor/mentions_extractor_smxm.py,"bert-large-cased,truncation_side=left",Zero and Few shot named entity & relationships recognition
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,57965dafd18f23a75641637d7be543c2,ylsung_VL_adapter/VL_adapter/VL-T5/src/gqa_raw_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,d748e7eac223999ee3627695b0593f0d,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/flax/summarization/run_summarization_flax.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,config=config,seed=unknown,dtype=getattr",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,4944106c31e83bc31e6dd3078d88041e,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_grapter_128.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.MarianMTModel.from_pretrained,github.com/salesforce/CodeRL,436,05bd1245cdfb2acc498149efa053d502,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/seq2seq-distillation/_test_bash_script.py,MARIAN_MODEL,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForTokenClassification.from_pretrained.to,github.com/bminixhofer/wtpsplit,422,80b41d10a54a2080deb76ffe21519e08,bminixhofer_wtpsplit/wtpsplit/wtpsplit/evaluation/extrinsic.py,unknown,Code for Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,470f88c18972a08049f8e12f5abd9e02,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/translation/run_translation_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained,github.com/yangheng95/PyABSA,779,be59bf1184d4586c529f38ca92d6534f,yangheng95_PyABSA/PyABSA/pyabsa/framework/trainer_class/trainer_template.py,unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoModelForSeq2SeqLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/robustness-gym/summvis,249,d9387fe24cc65536b789b7daabda9a15,robustness-gym_summvis/summvis/generation.py,DEVICE;model_name_or_path,SummVis is an interactive visualization tool for text summarization.
transformers.models.auto.AutoConfig.from_pretrained;transformers.models.auto.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,2e55b4913fbd216d4b7bd5ab9bac21b0,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/seq2seq/run_seq2seq_eval.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown,max_length=unknown,num_beams=unknown,num_beam_groups=unknown,diversity_penalty=unknown,gradient_checkpointing=unknown,use_cache=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoFeatureExtractor.from_pretrained,github.com/salesforce/UniControl,525,6cb2999d7d11bac070ee6a92f34a2723,salesforce_UniControl/UniControl/utils.py,safety_model_id,Unified Controllable Visual Generation Model
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,8cf70dc60d04783c04953dd9e3e336c8,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/run_bert_avg.py,unknown;model_path,NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,56ed8a1168f50494bfc563de7e37fca0,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/bertology/run_bertology.py,"unknown,num_labels=num_labels,finetuning_task=unknown,output_attentions=True,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,6af93aa888619f9557865cef15f576dc,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/data_systhesis/snowball/eval.py,"facebook/bart-large,cache_dir=None",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/NyanNyanovich/nyan,131,f318f233db81c6e572d2e8dfafe88bc9,NyanNyanovich_nyan/nyan/scripts/train_embedder.py,"model_path;model_name,do_lower_case=False",Automatic news aggregator in Telegram / Автоматический агрегатор новостей в Телеграме
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained;transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/demi6od/ChatBot,173,b027a346c28b40b453cfe943d5299198,demi6od_ChatBot/ChatBot/ChatBotBertGPT/model.py,bert-base-uncased;bert-base-uncased;distilgpt2;distilgpt2,"Pytorch Generative ChatBot (Dialog System) based on RNN, Transformer, Bert and GPT2"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,340b2dfc78f48882a608064867474701,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/testseen-nometa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,2a8ecddc896307e2c0aa6bc4d5fd6ed9,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/codeparrot/scripts/human_eval.py,unknown;unknown,Source code for Zalo AI 2021 submission
transformers.T5ForConditionalGeneration.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.GPTNeoForCausalLM.from_pretrained.to.eval;transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.GPTNeoForCausalLM.from_pretrained.to.eval;transformers.AutoTokenizer.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/sylinrl/TruthfulQA,365,bcd2a21263bfa77394d6e0c3eab2db2a,sylinrl_TruthfulQA/TruthfulQA/truthfulqa/models.py,"device;engine,cache_dir=cache_dir;device;engine,cache_dir=cache_dir;device;engine,cache_dir=cache_dir;EleutherAI/gpt-neo-2.7B;;EleutherAI/gpt-neo-2.7B,cache_dir=cache_dir;EleutherAI/gpt-neo-2.7B;;EleutherAI/gpt-neo-2.7B,cache_dir=cache_dir;device;engine,cache_dir=cache_dir",TruthfulQA: Measuring How Models Imitate Human Falsehoods
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/allenai/comet-atomic-2020,201,4619537e1fb0a5bb6452dc86d91e37f2,allenai_comet-atomic-2020/comet-atomic-2020/models/comet_atomic2020_bart/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",
transformers.BertModel.from_pretrained,github.com/yahshibu/nested-ner-tacl2020-transformers,138,244ca1ffa950939d6df153a53ca81c3d,yahshibu_nested-ner-tacl2020-transformers/nested-ner-tacl2020-transformers/model/sequence_labeling.py,bert_model,Implementation of Nested Named Entity Recognition using BERT
transformers.PegasusForConditionalGeneration.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/Shark-NLP/CoNT,142,2eebf942e877421177d0405c376b01da,Shark-NLP_CoNT/CoNT/model/model.py,model_name;model_name,[NeurIPS'22 Spotlight] CoNT: Contrastive Neural Text Generation 
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,1f79f9a36993b294c059271ceedcb832,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/retrieval/interactive_retrieval.py,bert-base-uncased;bert-base-uncased,Multi-hop dense retrieval for question answering
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForTokenClassification.from_pretrained,github.com/salesforce/CodeRL,436,647581888d981bde76ee33a6a9d9e9fa,salesforce_CodeRL/CodeRL/transformers/examples/legacy/token-classification/run_ner.py,"unknown,num_labels=num_labels,id2label=label_map,label2id=Dict,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,ca5dffdd17915dbaca07250ba7e7e14c,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/text-classification/run_xnli.py,"unknown,num_labels=num_labels,finetuning_task=xnli,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,do_lower_case=unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.configuration_fsmt.FSMTConfig.from_pretrained,github.com/songhaoyu/BoB,134,796c39666cfe03d7657d286d7f75d6a4,songhaoyu_BoB/BoB/xlibs/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py,pytorch_dump_folder_path,The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained.to,github.com/AlibabaResearch/DAMO-ConvAI,788,fe720a0c88c1504e08a46fa99a8020ee,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/PRO/train/utils/metrics.py,model_name;model_device;model_name;model_device,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.modeling_xlm_prophetnet.XLMProphetNetForConditionalGeneration.from_pretrained;transformers.modeling_prophetnet.ProphetNetForConditionalGeneration.from_pretrained,github.com/songhaoyu/BoB,134,f7360a1a169fbff5c02ee8a70f6a0583,songhaoyu_BoB/BoB/xlibs/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py,"prophetnet_checkpoint_path,output_loading_info=True;prophetnet_checkpoint_path,output_loading_info=True",The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,3db7af6182ae778f9d6e000b848e93a9,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/trocr/processing_trocr.py,"pretrained_model_name_or_path,None=kwargs;pretrained_model_name_or_path,None=kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.ResNetConfig.from_pretrained;transformers.ResNetConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,b472e16fa9e078af05c498cbce64a4d6,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/maskformer/convert_maskformer_resnet_to_pytorch.py,"microsoft/resnet-101,out_features=List;microsoft/resnet-50,out_features=List","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,e73b48c0e7daf6936b6788e3d59e248f,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/checkpointing.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.T5EncoderModel.from_pretrained,github.com/facebookresearch/tart,130,44830395a4378885580c5f2f9d57a999,facebookresearch_tart/tart/TART/src/utils.py,model_name,"Code and model release for the paper ""Task-aware Retrieval with Instructions"" by Asai et al."
transformers.FSMTConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,3a1072d9b0c00cb88c80fae6b8f9e566,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py,pytorch_dump_folder_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoProcessor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained,github.com/sail-sg/EditAnything,2813,53425704c90144f4940316b5a95b3a8e,sail-sg_EditAnything/EditAnything/sam2groundingdino_edit.py,"Salesforce/blip2-opt-2.7b;Salesforce/blip2-opt-2.7b,torch_dtype=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/farizrahman4u/loopgpt,1317,34703d93557f8b8774522ac728f16a5b,farizrahman4u_loopgpt/loopgpt/loopgpt/models/hf.py,"model;model,torch_dtype=unknown,load_in_8bit=load_in_8bit,device_map=auto,offload_folder=./offload",Modular Auto-GPT Framework
transformers.AutoModel.from_pretrained,github.com/salesforce/CodeRL,436,7cb96d20699eaf557566cebbb1f0038b,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/fsner/src/fsner/model.py,"pretrained_model_name_or_path,return_dict=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/siddk/voltron-robotics,147,0a5e77373c1819f565f55244fd8e1bac,siddk_voltron-robotics/voltron-robotics/voltron/models/core/vdual.py,"language_model,cache_dir=hf_cache;language_model,cache_dir=hf_cache",Voltron: Language-Driven Representation Learning for Robotics
transformers.Wav2Vec2ForPreTraining.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,e6b5959f87499821066f010f8e6f9e04,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/test_examples.py,tmp_dir,Source code for Zalo AI 2021 submission
transformers.AutoModelForSeq2SeqLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,19a25c38c41f9892f24d2374fc0d241f,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/seq2seq-distillation/run_eval.py,device;model_name,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,1519833769fae86b40c43cc066fe67f8,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/eval/eval_mhop_fever.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,7ea2d0dc4b0e2ec839c69a8695e6f70a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/Condenser/helper/create_train.py,"unknown,use_fast=True",Source code for Zalo AI 2021 submission
transformers.AutoModel.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,21b3441047bc4757900088da7d87427e,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/retrieval/models/mhop_retriever.py,unknown,Multi-hop dense retrieval for question answering
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,29d7c33816a8e6756391c4d640100445,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/language-modeling/run_mlm_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.TransfoXLCorpus.from_pretrained;transformers.TransfoXLLMHeadModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f912da882ba14fe079138fef1097563c,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/run_transfo_xl.py,unknown;unknown,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForCausalLM.from_pretrained;transformers.TFAutoModelForCausalLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,181c1895f2ed3568e7e35f4aeb47d12f,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/language-modeling/run_clm.py,"unknown;unknown;unknown;unknown;checkpoint,config=config;unknown,config=config",Source code for Zalo AI 2021 submission
transformers.models.auto.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,7c93bf5222090d9b26591d06f1a9af67,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/onnx/__main__.py,unknown,Source code for Zalo AI 2021 submission
transformers.LongformerModel.from_pretrained;transformers.LongformerForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,1f09147f6d6783d7dbaac989746de1e9,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py,longformer_model;longformer_model,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/tart,130,757f97bf9ffd6ad1f7dd421d45a30f72,facebookresearch_tart/tart/TART/eval_cross_task.py,model_name_or_path;model_name_or_path,"Code and model release for the paper ""Task-aware Retrieval with Instructions"" by Asai et al."
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,25006a0a619167dcbc474331f828c808,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/pcll/generate.py,"gpt2,cache_dir=cache_dir;gpt2,cache_dir=cache_dir",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,3fd5461738fc27c087b4be1a8dc13e07,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/s2sql/utils/example.py,os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,a64448924e6540e96950aeec316c412f,ylsung_VL_adapter/VL_adapter/VL-T5/src/caption_clip_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.Wav2Vec2Config.from_pretrained,github.com/salesforce/CodeRL,436,32b251eb86a322080f976b77a141eee7,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/wav2vec2/run_pretrain.py,"unknown,cache_dir=unknown,do_normalize=True;unknown,cache_dir=unknown,gradient_checkpointing=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.RobertaTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,a0507ddee8c9ae1a9ae3cb02a58745a9,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/r2sql/sparc/reranker/predict.py,"roberta-base,max_len=128",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/ypwhs/CreativeChatGLM,203,e51182fa3532b09f4e74fd8c5066b885,ypwhs_CreativeChatGLM/CreativeChatGLM/predictors/chatglm_predictor.py,"model_name,trust_remote_code=True,resume_download=True;model_name,trust_remote_code=True,resume_download=True;model_name,trust_remote_code=True,resume_download=True;model_name,trust_remote_code=True,resume_download=True,low_cpu_mem_usage=True,torch_dtype=unknown,device_map=Dict",👋 欢迎来到 ChatGLM 创意世界！你可以使用修订和续写的功能来生成创意内容！
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,7d93d24dc85c13f494e414df7b7dd3ae,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/hubconf.py,"unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,bd924f3f3caf1bef4753f9f4ac676770,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/qa/qa_trainer.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/the-crypt-keeper/can-ai-code,308,019d02590494096813566b372e32566a,the-crypt-keeper_can-ai-code/can-ai-code/interview-transformers-modal.py,"unknown,trust_remote_code=True;unknown,device_map=auto,torch_dtype=torch_dtype,trust_remote_code=True",Self-evaluating interview for AI coders
transformers.T5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/sail-sg/EditAnything,2813,7f691482e55213865a5524ed8872f2f0,sail-sg_EditAnything/EditAnything/ldm/modules/encoders/modules.py,version;version;version;version,"Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,e77628ffd15baf808d1953c58692602d,ashkamath_mdetr/mdetr/datasets/gqa.py,unknown,
transformers.AutoProcessor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained,github.com/sail-sg/EditAnything,2813,8211c985143564c79b57e6b5f93f6511,sail-sg_EditAnything/EditAnything/sam2vlpart_edit.py,"Salesforce/blip2-opt-2.7b;Salesforce/blip2-opt-2.7b,torch_dtype=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.LlamaTokenizer.from_pretrained,github.com/Vahe1994/SpQR,466,06a08849e1234fd74b86f662e71dd0a6,Vahe1994_SpQR/SpQR/lm-evaluation-harness/lm_eval/models/huggingface.py,pretrained,
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,110684412aab52738bd48eb9f9c7a971,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/adversarial/run_hans.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BartConfig.from_pretrained;transformers.BartTokenizer.from_pretrained.encode.unsqueeze,github.com/salesforce/CodeRL,436,2b5b47227deaf80751b35160dc16617e,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py,hf_checkpoint_name;0,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.XLNetConfig.from_pretrained;transformers.XLNetTokenizerFast.from_pretrained;transformers.XLNetForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,da70bd4cdd67df409ce6701d9b332bc3,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py,"unknown;unknown;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.UniSpeechConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,e9e8f686348d76fdf32fa02f949a49e6,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py,config_path,Source code for Zalo AI 2021 submission
transformers.LlamaForCausalLM.from_pretrained;transformers.LlamaTokenizer.from_pretrained,github.com/danielgross/LlamaAcademy,1206,6bab6a06713f5227be47400a816e1c76,danielgross_LlamaAcademy/LlamaAcademy/inference.py,"model_name,load_in_8bit=load_8bit,low_cpu_mem_usage=True,None=kwargs;jeffwan/vicuna-13b,use_fast=False",A school for camelids
transformers.VisionEncoderDecoderModel.from_pretrained;transformers.ViTFeatureExtractor.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/lupantech/ScienceQA,448,ad06bf418aa83f0263a72aa78db56ea3,lupantech_ScienceQA/ScienceQA/tools/generate_caption.py,model_name;feature_extractor_name;tokenizer_name,"Data and code for NeurIPS 2022 Paper ""Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering""."
transformers.CLIPTextModel.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained;transformers.BertTokenizerFast.from_pretrained,github.com/cloneofsimo/paint-with-words-sd,601,34d7ac8f3a163e4b029abe1a1e832076,cloneofsimo_paint-with-words-sd/paint-with-words-sd/change_model_path.py,openai/clip-vit-large-patch14;openai/clip-vit-large-patch14;CompVis/stable-diffusion-safety-checker;bert-base-uncased,Implementation of Paint-with-words with Stable Diffusion : method from eDiff-I that let you generate image from text-labeled segmentation map.
transformers.T5Tokenizer.from_pretrained,github.com/allenai/comet-atomic-2020,201,38ee265c4e2d21291d8483dae6eeedc4,allenai_comet-atomic-2020/comet-atomic-2020/scripts/calculate_max_len.py,t5-large,
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.half.cuda,github.com/guifaChild/text_to_vedio,158,d4c21723dbaf9ad2bd1a55606af0fc7d,guifaChild_text_to_vedio/text_to_vedio/ChatGLM-6B-main/cli_demo.py,"THUDM/chatglm-6b,trust_remote_code=True;",这是一个由文本直接生成视频的项目
transformers.RagConfig.from_pretrained;transformers.RagTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,c941046c6d5e5ce07a0b972c04cba282,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag/distributed_ray_retriever.py,"retriever_name_or_path,None=kwargs;retriever_name_or_path,config=config",Source code for Zalo AI 2021 submission
transformers.RobertaTokenizer.from_pretrained,github.com/zhvng/open-musiclm,398,ce101fac6eae46883be0ca36295b52b3,zhvng_open-musiclm/open-musiclm/open_musiclm/laion_clap/hook.py,roberta-base,"Implementation of MusicLM, a text to music model published by Google Research, with a few modifications."
transformers.RobertaTokenizer.from_pretrained;transformers.LukeTokenizer.from_pretrained;transformers.LukeTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,a5a0957284a274819d68272784f41e29,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py,"metadata;pytorch_dump_folder_path;pytorch_dump_folder_path,task=entity_classification",Source code for Zalo AI 2021 submission
transformers.DeiTForImageClassification.from_pretrained;transformers.DeiTFeatureExtractor.from_pretrained,github.com/qanastek/HugsVision,185,e48ce4a39a82fe42e2af205ff5db1764,qanastek_HugsVision/HugsVision/recipes/pneumothorax/binary_classification/train_example_deit.py,"huggingface_model,num_labels=len,label2id=label2id,id2label=id2label;huggingface_model",HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,c71b49886530b82f0d7c666230152fa1,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/train_mhop.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.T5ForConditionalGeneration.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,25b041dd8f1ec918cf9831777bf112cd,ylsung_VL_adapter/VL_adapter/VL-T5/src/my_transformers/modeling_t5.py,t5-base;t5-base,"PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.LongformerModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ab6220e2bc1374b1c618a1bc8dd68f93,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dstc11-simmc/task3/model/backbone.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.XLMProphetNetForConditionalGeneration.from_pretrained;transformers.ProphetNetForConditionalGeneration.from_pretrained,github.com/salesforce/CodeRL,436,c2b2c078f6ea38bec62116f54bd704b5,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py,"prophetnet_checkpoint_path,output_loading_info=True;prophetnet_checkpoint_path,output_loading_info=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/robustness-gym/summvis,249,b7e52b5c901fd716fdbd659b4c08a24d,robustness-gym_summvis/summvis/align.py,"model_name,use_fast=False",SummVis is an interactive visualization tool for text summarization.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,fd1ecad5351f49948a334704f5c00c57,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizerFast.from_pretrained,github.com/Guzpenha/transformer_rankers,155,5fb55fcc3ae1a44952def8bbfe6d2e83,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/examples/pointwise_bert_ranker.py,unknown,A library to conduct ranking experiments with transformers.
transformers.BartConfig.from_pretrained;transformers.BartTokenizer.from_pretrained.encode.unsqueeze,github.com/songhaoyu/BoB,134,0a91accaefec7b21860f3126af5cf53b,songhaoyu_BoB/BoB/xlibs/convert_bart_original_pytorch_checkpoint_to_pytorch.py,hf_checkpoint_name;0,The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/aws-samples/aws-genai-llm-chatbot,564,e13c48c140b43c001e645f0c610c84df,aws-samples_aws-genai-llm-chatbot/aws-genai-llm-chatbot/lib/large-language-model/hf-custom-script-model/samples/basic/inference.py,"model_dir;model_dir,trust_remote_code=True","A modular and comprehensive solution to deploy a Multi-LLM and Multi-RAG powered chatbot (Amazon Bedrock, Anthropic, HuggingFace, OpenAI, AI21, Cohere) using AWS CDK on AWS"
transformers.AutoModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,99d16731a86bbe739aae52674eb22266,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/s2sql/model/encoder/graph_input.py,os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/prakhar21/TextAugmentation-GPT2,183,666fccfe7707ec4d0e389e32598f1cd2,prakhar21_TextAugmentation-GPT2/TextAugmentation-GPT2/train.py,gpt2-medium;gpt2-medium,Fine-tuned pre-trained GPT2 for custom topic specific text generation. Such system can be used for Text Augmentation.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,216306e9461e212b084f0b9590de0385,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/Condenser/run_co_pre_training.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=False;unknown,cache_dir=unknown,use_fast=False",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,6c34d735c7379e3280a702f6f9b86250,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/scripts/tabfact/run_cloze.py,pretrained_model_name_or_path=../../utils_file/gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ai-forever/Kandinsky-2,2534,da3f52f50f9f56b63c7ca1d9e6ebd0e4,ai-forever_Kandinsky-2/Kandinsky-2/kandinsky2/kandinsky2_model.py,unknown;unknown,Kandinsky 2 — multilingual text2image latent diffusion model
transformers.AutoTokenizer.from_pretrained,github.com/SCUTlihaoyu/open-chat-video-editor,2354,6e8539b6976f33854bbbb6ee9a5b449d,SCUTlihaoyu_open-chat-video-editor/open-chat-video-editor/generator/image/retrieval/models/clip_model.py,model_name,Open source short video automatic generation tool
transformers.Wav2Vec2Config.from_pretrained,github.com/salesforce/CodeRL,436,ac027e17a66b2bccaac93c2998084d37,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/huggingface/instruction-tuned-sd,127,87fadb6ad5b562273d57db3afe4d755d,huggingface_instruction-tuned-sd/instruction-tuned-sd/finetune_instruct_pix2pix.py,"unknown,subfolder=tokenizer,revision=unknown;unknown,subfolder=text_encoder,revision=unknown",Code for instruction-tuning Stable Diffusion.
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/Mikubill/naifu-diffusion,222,ebfce0254a74f7b7f49dc5be4f7d01ac,Mikubill_naifu-diffusion/naifu-diffusion/lib/utils.py,"stabilityai/stable-diffusion-2,subfolder=tokenizer;openai/clip-vit-large-patch14;openai/clip-vit-large-patch14",Train stable diffusion model with Diffusers and Pytorch Lightning
transformers.EfficientNetConfig.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,cf0ce588d5a60867a0f2ede65d375f13,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/align/convert_align_tf_to_hf.py,google/efficientnet-b7;bert-base-uncased,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ff7f6b4c5beb861ca42f93f3abeaab2e,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/tkk/models/unified/prefixtuning.py,"unknown,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d226148e33d68a1ff6f97859a4b45a3b,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/tkk/models/unified/finetune.py,"unknown,use_fast=False;unknown;unknown,config=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoFeatureExtractor.from_pretrained,github.com/timothybrooks/instruct-pix2pix,5428,8eebc1e69e6f7c14595d7891ddd93ceb,timothybrooks_instruct-pix2pix/instruct-pix2pix/stable_diffusion/scripts/txt2img.py,safety_model_id,
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/dqshuai/MetaFormer,189,40faf9b2727958c5dc1fa1d1dd226b59,dqshuai_MetaFormer/MetaFormer/inference.py,bert-base-uncased;bert-base-uncased,"A PyTorch implementation of ""MetaFormer: A Unified Meta Framework for Fine-Grained Recognition"". A reference PyTorch implementation of “CoAtNet: Marrying Convolution and Attention for All Data Sizes”"
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,493dc21d9c0fa5fc99481ca53a1408cf,yangheng95_PyABSA/PyABSA/pyabsa/tasks/RNAClassification/instructor/rnac_instructor.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.Wav2Vec2ForSequenceClassification.from_pretrained;transformers.Wav2Vec2ForAudioFrameClassification.from_pretrained;transformers.Wav2Vec2ForXVector.from_pretrained;transformers.Wav2Vec2Config.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,8e1bc83ef2f3e6fc24f405ff72c565c5,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py,"base_model_name,config=hf_config;base_model_name,config=hf_config;base_model_name,config=hf_config;config_path;base_model_name,return_attention_mask=True,do_normalize=False","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d68c4472f723e05fd547533721f2792d,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/cycleablationmeta.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.OPTConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,f39912c80c7371e2f76d5a393a5cfbe4,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/opt/convert_opt_original_pytorch_checkpoint_to_pytorch.py,config,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.GPT2LMHeadModel.from_pretrained,github.com/kmeng01/memit,325,5c7347c654fd24294522b1bc9d735bfe,kmeng01_memit/memit/baselines/mend/algs/enn.py,gpt2,Mass-editing thousands of facts into a transformer memory (ICLR 2023)
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,695ffc6048760361a6512a6247d93aec,naver_gdc/gdc/dpg/gdc/examples/run-distributional.py,config,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/tart,130,ecf11440ac9129b42f2b6031c1c473a8,facebookresearch_tart/tart/TART/preprocess.py,"unknown,local_files_only=True;unknown,local_files_only=False","Code and model release for the paper ""Task-aware Retrieval with Instructions"" by Asai et al."
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForCausalLM.from_pretrained,github.com/salesforce/CodeRL,436,6235925283ac06188fe110a9be686c3e,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/jax-projects/model_parallel/run_clm_mp.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,seed=unknown,dtype=getattr",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5Tokenizer.from_pretrained;transformers.T5Tokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,876d690ccde1442abe8d4ec881f3c92b,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/ssll/unifymodel/generate.py,t5-base;tokz_path,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/michiyasunaga/LinkBERT,369,e097aa34e468416ed239ebe6256dd4e9,michiyasunaga_LinkBERT/LinkBERT/src/seqcls/run_seqcls.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",[ACL 2022] LinkBERT: A Knowledgeable Language Model 😎 Pretrained with Document Links
transformers.AutoConfig.from_pretrained,github.com/salesforce/CodeRL,436,8f8440503a91e5e247c11ba156f7fc3b,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/seq2seq-distillation/_test_make_student.py,TINY_BART,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.MBartConfig.from_pretrained,github.com/salesforce/CodeRL,436,6eaf2c0b031032a112b997d43c12bb64,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py,"hf_config_path,vocab_size=vocab_size",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/siddk/voltron-robotics,147,de218e1f9d013fedf60cbee207b33767,siddk_voltron-robotics/voltron-robotics/voltron/models/core/vcond.py,"language_model,cache_dir=hf_cache;language_model,cache_dir=hf_cache",Voltron: Language-Driven Representation Learning for Robotics
transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/HHousen/TransformerSum,405,70a27f69ab9f993b652c870232859ce4,HHousen_TransformerSum/TransformerSum/src/abstractive.py,"unknown,gradient_checkpointing=unknown;unknown,use_fast=True",Models to perform neural summarization (extractive and abstractive) using machine learning transformers and a tool to convert abstractive summarization datasets to the extractive task.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForTokenClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,d4d71bb3ada8fa9356072bc880a2ad40,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/token-classification/run_ner.py,"unknown,num_labels=num_labels;unknown,num_labels=num_labels;tokenizer_name_or_path,use_fast=True,add_prefix_space=True;tokenizer_name_or_path,use_fast=True;unknown,config=config",Source code for Zalo AI 2021 submission
transformers.BertModel.from_pretrained,github.com/StanleyLsx/entity_extractor_by_pointer,123,dd94d8eeb5b5316215627251586b8234,StanleyLsx_entity_extractor_by_pointer/entity_extractor_by_pointer/engines/models/GlobalPointer.py,bert-base-chinese,使用torch整合两种经典的指针NER抽取范式，分别是SpanBert和苏神的GlobalPointer，简单加了些tricks，配置后一键运行
transformers.AutoConfig.from_pretrained,github.com/tabtoyou/KoLLaVA,133,29fed39f0fbac69e5a6ed6953995e973,tabtoyou_KoLLaVA/KoLLaVA/llava/model/utils.py,config,KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.Wav2Vec2Config.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,6c3a583318edc0d2c8496757fbe7c057,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py,config_path,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,b8cee8689448fc4dda4c9c2acb46202c,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/question-answering/run_qa_no_trainer.py,"unknown;unknown;unknown,use_fast=True;unknown,use_fast=True;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.Wav2Vec2ForCTC.from_pretrained.to;transformers.Wav2Vec2Tokenizer.from_pretrained,github.com/winddori2002/TriAAN-VC,101,2bc2bb0bd5da16da5b02314fd8f36814,winddori2002_TriAAN-VC/TriAAN-VC/src/metric.py,unknown;facebook/wav2vec2-large-960h-lv60-self,TriAAN-VC: Triple Adaptive Attention Normalization for Any-to-Any Voice Conversion
transformers.T5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/KU-CVLAB/3DFuse,667,f0736b575a172fe86f2f26cf75de01c8,KU-CVLAB_3DFuse/3DFuse/ldm/modules/encoders/modules.py,version;version;version;version,"Official implementation of ""Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation"""
transformers.T5Config.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/awslabs/pptod,148,212996edcde3d78814e5fbe037c4f325,awslabs_pptod/pptod/IC/modelling/T5Model.py,"model_path;model_path,config=t5_config,resume_download=True",Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.models.auto.AutoConfig.from_pretrained;transformers.models.auto.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,54414c115be4c0518371010769af7925,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/seq2seq/run_seq2seq_train.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown,max_length=unknown,num_beams=unknown,num_beam_groups=unknown,diversity_penalty=unknown,gradient_checkpointing=unknown,use_cache=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoFeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,0067f43ad17e233a91db37ef211d3ea7,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/swin/convert_swin_timm_to_pytorch.py,format,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/sail-sg/EditAnything,2813,90cdc0ae8d861e462f5573909923cea5,sail-sg_EditAnything/EditAnything/tools/train_dreambooth_inpaint.py,"unknown;unknown,subfolder=tokenizer;unknown,subfolder=text_encoder","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.SEWDConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,7e7d37ac90f1248839a7dc011b9bb2ea,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py,config_path,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoModelForCTC.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,438053707745e687a0b7399c3f4e34ae,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py,"unknown,cache_dir=unknown,use_auth_token=unknown;unknown,config=config_for_tokenizer,tokenizer_type=tokenizer_type,unk_token=[UNK],pad_token=[PAD],word_delimiter_token=|,use_auth_token=unknown;unknown,cache_dir=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,config=config,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/bofenghuang/vigogne,448,291a3604819b9965ab96d729ef096347,bofenghuang_vigogne/vigogne/vigogne/train/train_sft_seq2seq.py,"unknown,load_in_8bit=unknown,device_map=Dict,use_cache=unknown;unknown,padding_side=right,use_fast=True",French instruction-following and chat models
transformers.GPTNeoXForCausalLM.from_pretrained.half;transformers.GPTNeoXConfig.from_pretrained;transformers.GPTNeoXForCausalLM.from_pretrained.half,github.com/EleutherAI/gpt-neox,6196,9d09d1990c6710336e8f8be66b0f0dfc,EleutherAI_gpt-neox/gpt-neox/tools/convert_hf_to_sequential.py,";unknown,revision=unknown,cache_dir=os.path.join;","An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library."
transformers.models.bert.tokenization_bert.BertTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/fastnlp/CPT,448,7e3ee34629203ca38d941b80865fef19,fastnlp_CPT/CPT/finetune/classification/run_clue_classifier.py,"unknown,do_lower_case=unknown,cache_dir=unknown;pretrained_model_name_or_path=unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown;pretrained_model_name_or_path=unknown,from_tf=bool,config=config,cache_dir=unknown;unknown;unknown",CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,1b60b74e07cb5c25fec7e6938cc2812c,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/text-classification/run_glue_no_trainer.py,"unknown,num_labels=num_labels,finetuning_task=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.Wav2Vec2Config.from_pretrained;transformers.Speech2Text2Config.from_pretrained,github.com/salesforce/CodeRL,436,ee3b33342e48331537e1bec1d7a047fe,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/speech_encoder_decoder/convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py,"encoder_config_path;decoder_config_path,vocab_size=vocab_size,decoder_layers=num_decoder_layers,do_stable_layer_norm=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,9aec24b246722d40769886a9484d6c49,ylsung_VL_adapter/VL_adapter/VL-T5/src/video/tvqa_matching_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.models.data2vec.configuration_data2vec_audio.Data2VecAudioConfig.from_pretrained;transformers.Wav2Vec2Processor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,a35603c885aa9d8d7b4a21763781b71d,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/data2vec/convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py,config_path;facebook/wav2vec2-large-lv60,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,4d3d0a04228fcb6e3e1c4bad1b3b8afc,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/qa/train_ranker.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/lensterxyz/lenster,21455,8924eb7ecb5193f22e266a1a15996302,lensterxyz_lenster/lenster/packages/ai/download_models.py,tagger_hf;tagger_hf;locale_detector_hf;locale_detector_hf,Hey is a decentralized and permissionless social media app built with Lens Protocol 🌿
transformers.T5Tokenizer.from_pretrained;transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,0385beb4c904850ee99166dce84c3adc,awslabs_pptod/pptod/E2E_TOD/inference_pptod.py,pretrained_path;unknown,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/AdvancedLiterateMachinery,388,58030cd5b748a8ddd6453638c318ff3e,AlibabaResearch_AdvancedLiterateMachinery/AdvancedLiterateMachinery/DocumentUnderstanding/GeoLayoutLM/preprocess/funsd_el/preprocess.py,"VOCA,do_lower_case=True","A collection of original, innovative ideas and algorithms towards Advanced Literate Machinery. This project is maintained by the OCR Team in the Language Technology Lab, Alibaba DAMO Academy."
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,88812bb83019b22474c5569d28409fdc,ashkamath_mdetr/mdetr/datasets/refexp.py,unknown,
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/wangyuxinwhy/uniem,572,e4770954961ac126217a06e2a5da678a,wangyuxinwhy_uniem/uniem/uniem/finetuner.py,model_name_or_path;model_name_or_path,unified embedding model
transformers.AutoConfig.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/amazon-science/sccl,276,1549263744884f5ae85a6f626b60d7be,amazon-science_sccl/sccl/utils/.ipynb_checkpoints/optimizer-checkpoint.py,"BERT_CLASS;BERT_CLASS,config=config;BERT_CLASS","Pytorch implementation of Supporting Clustering with Contrastive Learning, NAACL 2021"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,5c29fa771aa9cc02669b11ec509f0db4,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/tracking.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.Wav2Vec2Config.from_pretrained;transformers.Speech2Text2Config.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,06f9017f654bbe806d741cb8f9463b1a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/speech_encoder_decoder/convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py,"encoder_config_path;decoder_config_path,vocab_size=vocab_size,decoder_layers=num_decoder_layers,do_stable_layer_norm=True",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForCausalLM.from_pretrained;transformers.TFAutoModelForCausalLM.from_pretrained,github.com/salesforce/CodeRL,436,c7b18a7be0a3ce9f6e9631330c4c4dfd,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/language-modeling/run_clm.py,"unknown;unknown;unknown;unknown;checkpoint,config=config;unknown,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,e67dd0ea7941a43034241c1f1683053b,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/sentence_transformers/losses/DenoisingAutoEncoderLoss.py,"decoder_name_or_path;decoder_name_or_path;decoder_name_or_path,None=kwargs_decoder",NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,49f21bab51e2efa185eefcbad488bbdb,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/retrieval_based_dialog/data_loader.py,directory,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModelForSeq2SeqLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/allenai/comet-atomic-2020,201,45699dc38186d37fbb23432303bbd6e6,allenai_comet-atomic-2020/comet-atomic-2020/models/comet_atomic2020_bart/generation_example.py,unknown;model_path,
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/salesforce/CodeRL,436,702ef12f8f29e6f36c74fbb18777ab38,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/language-modeling/run_mlm_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/songhaoyu/BoB,134,c6a85e04ae4e394f12ff700df84de7d7,songhaoyu_BoB/BoB/xlibs/commands/download.py,"unknown,cache_dir=unknown,force_download=unknown;unknown,cache_dir=unknown,force_download=unknown",The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/phellonchen/X-LLM,241,060115880ea426f0f493f27be52f6d83,phellonchen_X-LLM/X-LLM/xllm/models/xllm_models/lamumo.py,bert-base-chinese;bert-base-chinese,X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages
transformers.GPTJForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/dpfried/incoder,277,406cd16f542542b323fac9118afdc861,dpfried_incoder/incoder/evaluation/models.py,"model_name,torch_dtype=unknown,low_cpu_mem_usage=True;unknown;model_name,revision=float16,torch_dtype=unknown,low_cpu_mem_usage=True;model_name;model_name",Generative model for code infilling and synthesis
transformers.AutoTokenizer.from_pretrained,github.com/Shark-NLP/CoNT,142,0afc7638b0bd3e281ea58498de4eee35,Shark-NLP_CoNT/CoNT/model/dataloader.py,tokenizer_name,[NeurIPS'22 Spotlight] CoNT: Contrastive Neural Text Generation 
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/Mikubill/naifu-diffusion,222,f60d0b12cc392323941fd10bbe247d06,Mikubill_naifu-diffusion/naifu-diffusion/experiment/noisy_learning.py,/root/dataset/animesfw/tokenizer;/root/dataset/animesfw/text_encoder,Train stable diffusion model with Diffusers and Pytorch Lightning
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,16d40cd663a4dbd734e3a68f301ea334,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/summarization/run_summarization.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.DeiTFeatureExtractor.from_pretrained;transformers.DeiTForImageClassification.from_pretrained,github.com/qanastek/HugsVision,185,913b39c5dbf50a95b471a5e7361ee200,qanastek_HugsVision/HugsVision/docker/API-CPU-ONLY/server/main.py,model_path;model_path,HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,99596d4fe30cbe89e1c6528d15b98251,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/seq2seq/pack_dataset.py,unknown,Source code for Zalo AI 2021 submission
transformers.DPRContextEncoder.from_pretrained.to;transformers.DPRContextEncoderTokenizerFast.from_pretrained;transformers.RagRetriever.from_pretrained;transformers.RagSequenceForGeneration.from_pretrained;transformers.RagTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,13b5621a73b27fe619c9fd7d28f79e58,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag/use_own_knowledge_dataset.py,"device=device;unknown;unknown,index_name=custom,indexed_dataset=dataset;unknown,retriever=retriever;unknown",Source code for Zalo AI 2021 submission
transformers.Wav2Vec2Config.from_pretrained;transformers.MBartConfig.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,50b92883a729601df3debbfd9697446e,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/speech_encoder_decoder/convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py,"encoder_config_path,add_adapter=True,adapter_stride=adapter_stride,adapter_kernel_size=adapter_kernel_size,use_auth_token=True,output_hidden_size=encoder_output_dim;decoder_config_path;encoder_config_path,use_auth_token=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.GPT2LMHeadModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,71b8ae5af8a8a8d56b28c88216edc283,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/bertology/run_prune_gpt.py,unknown,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,be8900f16091fef89ec61d2509826433,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/sdsql/train.py,"unknown;unknown,do_lower_case=do_lower_case;unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,0c2c3e962510f60edf37b1e7976ca807,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/tkk/models/unified/combined_prefixtuning.py,"unknown,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/phellonchen/X-LLM,241,ff86772093abcd18a20f33b1a948f40f,phellonchen_X-LLM/X-LLM/xllm/models/xllm_models/xllm.py,bert-base-chinese;bert-base-chinese,X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages
transformers.AutoModel.from_pretrained,github.com/VinAIResearch/XPhoneBERT,237,331977bfb78ec86e0d172e29a92fafaa,VinAIResearch_XPhoneBERT/XPhoneBERT/VITS_with_XPhoneBERT/models.py,bert,XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech (INTERSPEECH 2023)
transformers.GPT2LMHeadModel.from_pretrained;transformers.GPT2Tokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/stanfordnlp/chirpycardinal,125,f79d307275681565a617aecb63953411,stanfordnlp_chirpycardinal/chirpycardinal/docker/responseranker/app/remote_module.py,MODEL_CHECKPOINT;MODEL_CHECKPOINT;microsoft/DialogRPT-updown;microsoft/DialogRPT-updown,Stanford's Alexa Prize socialbot
transformers.DPRContextEncoder.from_pretrained.to;transformers.DPRContextEncoderTokenizerFast.from_pretrained,github.com/salesforce/CodeRL,436,3c1e14a668e1359e5e89737f21c2da22,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag-end2end-retriever/use_own_knowledge_dataset.py,device=device;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5Tokenizer.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/Yui010206/SeViLA,121,2b692bee2cd6f8702534c7e5fef3b7d6,Yui010206_SeViLA/SeViLA/lavis/models/img2prompt_models/img2prompt_vqa.py,google/t5-large-lm-adapt;google/t5-large-lm-adapt,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.AutoTokenizer.from_pretrained,github.com/tabtoyou/KoLLaVA,133,d78b37f4cdb1d643a01aa4edc7cf8d7c,tabtoyou_KoLLaVA/KoLLaVA/llava/data/split_long_conversation.py,"unknown,model_max_length=unknown,padding_side=right,use_fast=False",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.AutoConfig.from_pretrained,github.com/ucinlp/autoprompt,497,a73a3c9cb6fee87ead8bbbbeaccd3908,ucinlp_autoprompt/autoprompt/autoprompt/popsicle.py,"pretrained_model_name_or_path,None=kwargs",AutoPrompt: Automatic Prompt Construction for Masked Language Models.
transformers.AutoConfig.from_pretrained,github.com/yangheng95/PyABSA,779,3f024f2867519992d76da60b4ad96165,yangheng95_PyABSA/PyABSA/pyabsa/tasks/RNAClassification/models/__classic__/mhsa.py,bert-base-uncased,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/neulab/knn-transformers,247,e9e10a3af966d83c14207bdfea066cdd,neulab_knn-transformers/knn-transformers/run_translation.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown","PyTorch + HuggingFace code for RetoMaton: ""Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval"" (ICML 2022), including an implementation of kNN-LM and kNN-MT"
transformers.RagConfig.from_pretrained;transformers.RagTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,2bf61b324804c03b5eba96a9665c5f14,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag-end2end-retriever/distributed_ray_retriever.py,"retriever_name_or_path,None=kwargs;retriever_name_or_path,config=config",Source code for Zalo AI 2021 submission
transformers.BertModel.from_pretrained,github.com/kmeng01/memit,325,72dc8a7278de4a093a54a2b8745eeae7,kmeng01_memit/memit/baselines/mend/models.py,"model_name,cache_dir=scr",Mass-editing thousands of facts into a transformer memory (ICLR 2023)
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/dpfried/incoder,277,c2db62fd31ba781171db19525095ee2a,dpfried_incoder/incoder/example_usage.py,"model_name,None=kwargs;model_name",Generative model for code infilling and synthesis
transformers.BertTokenizer.from_pretrained;transformers.BertConfig.from_pretrained;transformers.BertTokenizer.from_pretrained;transformers.BertConfig.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,0d4a7b3db3e99aebfa663a2b67da02df,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/retrieval_based_dialog/main.py,unknown;unknown;unknown;unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizerFast.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/ChenWu98/cycle-diffusion,440,1024181a631dc1452ff9f6d293f8fa5c,ChenWu98_cycle-diffusion/cycle-diffusion/model/lib/stable_diffusion/ldm/modules/encoders/modules.py,bert-base-uncased;version;version,[ICCV 2023] Zero-shot image editing with stochastic diffusion models
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained,github.com/salesforce/CodeRL,436,8df0485a11f2b1602e5949bef05df404,salesforce_CodeRL/CodeRL/transformers/examples/legacy/multiple_choice/run_multiple_choice.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BartTokenizer.from_pretrained;transformers.BartConfig.from_pretrained;transformers.BartForConditionalGeneration.from_pretrained,github.com/RUC-GSAI/YuLan-IR,181,b77b9d3789d5298dc2830ce3c1be5302,RUC-GSAI_YuLan-IR/YuLan-IR/WebBrain/generator/run_model_test.py,"unknown;unknown;unknown,config=config",YuLan-IR: Information Retrieval Boosted LMs
transformers.AutoConfig.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,23964b386ff774bb450914a01eff9f04,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/PRO/train/utils/process_manager.py,"unknown;unknown,config=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.WavLMForSequenceClassification.from_pretrained;transformers.WavLMForAudioFrameClassification.from_pretrained;transformers.WavLMForXVector.from_pretrained;transformers.WavLMConfig.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,e479dab82959e5016413e8b8084a28ce,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/wavlm/convert_wavlm_original_s3prl_checkpoint_to_pytorch.py,"base_model_name,config=hf_config;base_model_name,config=hf_config;base_model_name,config=hf_config;config_path;base_model_name,return_attention_mask=True,do_normalize=False",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,6c100a542d48a3ae4006932da119e105,ylsung_VL_adapter/VL_adapter/VL-T5/src/caption_raw_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.RobertaTokenizer.from_pretrained,github.com/shmsw25/FActScore,119,6e336f10f031d59766de4c3e337bf539,shmsw25_FActScore/FActScore/factscore/retrieval.py,roberta-large,"A package to evaluate factuality of long-form generation. Original implementation of our EMNLP 2023 paper ""FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"""
transformers.AutoTokenizer.from_pretrained;transformers.CLIPImageProcessor.from_pretrained;transformers.CLIPVisionModel.from_pretrained.cuda,github.com/tabtoyou/KoLLaVA,133,6b447bb018abf97939e7c8cd17f28671,tabtoyou_KoLLaVA/KoLLaVA/llava/eval/run_llava.py,"model_name;unknown,torch_dtype=unknown;",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.RobertaTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,8f0956d5676ccebfa84fd4f8fbc8b6a1,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/trocr/convert_trocr_unilm_to_pytorch.py,roberta-large,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/zjunlp/OntoProtein,122,5569df515029efb205713c28bf376cc9,zjunlp_OntoProtein/OntoProtein/run_pretrain.py,unknown;data/model_data/PubMedBERT,"Code and datasets for the ICLR2022 paper ""OntoProtein: Protein Pretraining With Gene Ontology Embedding"""
transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/tart,130,6f459946018485644083d6f52cdd1ea2,facebookresearch_tart/tart/TART/src/rerank.py,model_name_or_path;model_name_or_path,"Code and model release for the paper ""Task-aware Retrieval with Instructions"" by Asai et al."
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,98ac0c2a785ed93b14a8ed3b9488fc68,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/cross_validation.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,a983d94e76341da33cabc9ffefbc0f6a,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/sentence_transformers/cross_encoder/CrossEncoder.py,"model_name;model_name,config=unknown;model_name,None=tokenizer_args",NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,7277145729e7b18dc46ca17d7f14da70,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/retrieval/train_single.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.BartConfig.from_pretrained;transformers.BartTokenizer.from_pretrained.encode.unsqueeze,github.com/CuongNN218/zalo_ltr_2021,138,692314102e94140c65ffe538ff02d00e,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py,hf_checkpoint_name;0,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,1b672d540ec30ff155de1dc4343b2f8d,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_grapter.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/salesforce/CodeRL,436,477f27838930f053a40fa287346d8112,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/language-modeling/run_clm_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/naver/gdc,112,c9edae16a22f3d3148ff3244f555c206,naver_gdc/gdc/dpg/gdc/gdc/scorer.py,gpt2-medium;gpt2-medium,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.AutoModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,66373ebd13a2f24d9352b8034138cf1d,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/proton/model/encoder/graph_input.py,os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained,github.com/facebookresearch/tart,130,5d91f63c3f9d3cbf2e0a826fa49f3c10,facebookresearch_tart/tart/BERRI/denoising.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown","Code and model release for the paper ""Task-aware Retrieval with Instructions"" by Asai et al."
transformers.Wav2Vec2Processor.from_pretrained;transformers.HubertModel.from_pretrained,github.com/YuanGongND/whisper-at,199,4166b16d2d0801cd6f18ed8c50a17ead,YuanGongND_whisper-at/whisper-at/src/noise_robust_asr/intermediate_feat_extract/esc-50/extract_esc50_hubert_xl_all_pool.py,facebook/hubert-xlarge-ls960-ft;mdl_size,"Code and Pretrained Models for Interspeech 2023 Paper ""Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong Audio Event Taggers"""
transformers.RobertaForMaskedLM.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/salesforce/CodeRL,436,6082d130b125d6b35c52bc52c78104bc,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/distillation/scripts/extract.py,unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.UniSpeechSatConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,32d8b84bcc3a6877ff6a3afaacebe8a9,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/unispeech_sat/convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.HubertConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,60cd196aac6ffda5228a39825c418e94,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/hubert/convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py,config_path,Source code for Zalo AI 2021 submission
transformers.BertTokenizer.from_pretrained;transformers.RobertaTokenizer.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/salesforce/CodeRL,436,8316204346032aee332930dca409d593,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/distillation/scripts/binarized_data.py,unknown;unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.GPT2LMHeadModel.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,9f512519c8b43c857cf796a7e72f8b4e,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/pplm/run_pplm.py,"pretrained_model,output_hidden_states=True;pretrained_model",Source code for Zalo AI 2021 submission
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,62209a78e914e19c8341e780afc934d0,ylsung_VL_adapter/VL_adapter/VL-T5/src/vqa_clip_data.py,"unknown,max_length=unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,2cfad30e8cbc825e6319916387ea4b32,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/src/accelerate/test_utils/scripts/external_deps/test_performance.py,"model_name;model_name,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.GPT2TokenizerFast.from_pretrained,github.com/YuanGongND/whisper-at,199,0dda47711864ba487b1371fd74f4d46f,YuanGongND_whisper-at/whisper-at/src/noise_robust_asr/intermediate_feat_extract/whisper_feat_extracrt/whisper/tokenizer.py,path,"Code and Pretrained Models for Interspeech 2023 Paper ""Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong Audio Event Taggers"""
transformers.AutoModelForCausalLM.from_pretrained,github.com/Vahe1994/SpQR,466,9e0c2a435aec0165cffee1db26abe5dd,Vahe1994_SpQR/SpQR/modelutils.py,"pretrained_model_name_or_path=model_path,trust_remote_code=True,torch_dtype=dtype",
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ec94945bf8cb39e8b68762031289c4ce,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/diana/downstream/dianawometa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained.eval.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained.eval.to,github.com/facebookresearch/ParlAI,10365,66154df0d93d7bd2a5723310a26b898c,facebookresearch_ParlAI/ParlAI/projects/roscoe/score.py,"unknown;unknown;nli_model;nli_model;model_id,add_prefix_space=True;unknown;model_id;unknown",A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.modeling_roberta.RobertaModel.config_class.from_pretrained,github.com/snap-stanford/GreaseLM,200,2d12fd34302dcba02bb8747d21ae66aa,snap-stanford_GreaseLM/GreaseLM/modeling/modeling_greaselm.py,"roberta-large,cache_dir=None,return_unused_kwargs=True,force_download=False,output_hidden_states=True",[ICLR 2022 spotlight]GreaseLM: Graph REASoning Enhanced Language Models for Question Answering
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.half.cuda,github.com/guifaChild/text_to_vedio,158,bbd7b0a8506f1207e0ab095ec123377e,guifaChild_text_to_vedio/text_to_vedio/ChatGLM-6B-main/web_demo_old.py,"THUDM/chatglm-6b,trust_remote_code=True;",这是一个由文本直接生成视频的项目
transformers.T5TokenizerFast.from_pretrained,github.com/Yui010206/SeViLA,121,57e51ef24a91aaf846f8656b1b82d0f3,Yui010206_SeViLA/SeViLA/lavis/models/sevila_models/sevila.py,t5_model,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/memray/OpenNMT-kpg-release,210,4abe84543ed85569794f9a8ce028680f,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/keyphrase/run_infer_hfkpg.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Keyphrase Generation
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,a0b219b7f2e8f1f7fb8556600136c82c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/diana/downstreamdeca/toknize.py,./t5-base,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/allenai/real-toxicity-prompts,125,a931b4c508b527779205c1cb8820510b,allenai_real-toxicity-prompts/real-toxicity-prompts/scripts/finetuning/finetune_affect_lm.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown",
transformers.LlamaForCausalLM.from_pretrained;transformers.LlamaForCausalLM.from_pretrained,github.com/facebookresearch/ParlAI,10365,4b826a2b756c4543a74c52a0778c05ca,facebookresearch_ParlAI/ParlAI/parlai/agents/hugging_face/llama.py,llama_path;llama_path,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelWithLMHead.from_pretrained,github.com/andreamad8/FSB,119,e3286a6eba921eee084dcb908a083e10,andreamad8_FSB/FSB/data/wow/KILT/kilt/readers/t5/base_transformer.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir;unknown,config=unknown,cache_dir=cache_dir",The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
transformers.FlaxWhisperForConditionalGeneration.from_pretrained;transformers.WhisperProcessor.from_pretrained,github.com/sanchit-gandhi/whisper-jax,3647,7aef3955d6a2c12ac6744ac884b669ab,sanchit-gandhi_whisper-jax/whisper-jax/benchmarks/run_pmap.py,"openai/whisper-large-v2,_do_init=False,dtype=unknown;openai/whisper-tiny.en",JAX implementation of OpenAI's Whisper model for up to 70x speed-up on TPU.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.half.cuda,github.com/guifaChild/text_to_vedio,158,1dc963131b8c65bd3b5f432b60127380,guifaChild_text_to_vedio/text_to_vedio/ChatGLM-6B-main/web_demo.py,"THUDM/chatglm-6b,trust_remote_code=True;",这是一个由文本直接生成视频的项目
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.GenerationConfig.from_pretrained,github.com/Xpitfire/symbolicai,720,0872205e7fa10bd9caee99360ec3c86b,Xpitfire_symbolicai/symbolicai/symai/backend/services/huggingface_causallm_server.py,"unknown,use_fast=False,local_files_only=False,output_hidden_states=True;unknown,torch_dtype=dtype,low_cpu_mem_usage=True,local_files_only=False,pad_token_id=unknown;unknown",Compositional Differentiable Programming Library
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoModel.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,a9c112eee180a81d381c6da11c1f1407,AIGC-Audio_AudioGPT/AudioGPT/NeuralSeq/modules/commons/rel_transformer.py,"model_name;model_name;model_name,config=config","AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.RobertaTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/salesforce/CodeRL,436,caa68710aacb64b6ff7844baf529d188,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py,roberta-base;pytorch_dump_folder_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.SEWDConfig.from_pretrained,github.com/salesforce/CodeRL,436,83a5ba7bab7c59d379a607e60fb3f1e7,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.RobertaTokenizerFast.from_pretrained,github.com/memray/OpenNMT-kpg-release,210,6d26e76c6df5cc93f7cb45dbae8487ee,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/inputters/news_dataset.py,"tokenizer_name,cache_dir=cache_dir;roberta-base,__slow_tokenizer=tokenizer,tokenizer_file=None,vocab_file=bpe_vocab,merges_file=bpe_merges",Keyphrase Generation
transformers.UniSpeechSatConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,ecaee9ca436983793785cead76a5f6fd,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/unispeech_sat/convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py,config_path,Source code for Zalo AI 2021 submission
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,a49eb6055ba23f24ec2d41a185f7d35c,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/pplm/run_pplm_discrim_train.py,pretrained_model;pretrained_model,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForMaskedLM.from_pretrained,github.com/salesforce/CodeRL,436,4567457d1df3bf33fd9aea691941fc5d,salesforce_CodeRL/CodeRL/transformers/examples/flax/language-modeling/run_mlm_flax.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,config=config,seed=unknown,dtype=getattr",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.MarianConfig.from_pretrained,github.com/songhaoyu/BoB,134,caee29a4137e8480e399d29f5ff174c1,songhaoyu_BoB/BoB/xlibs/data/test_generation_utils.py,sshleifer/tiny-marian-en-de,The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.T5Tokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.PegasusTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/Shark-NLP/CoNT,142,987c8ca0bab102aebc2aa2f8f58cf35e,Shark-NLP_CoNT/CoNT/inference.py,t5-small;Salesforce/codet5-base;google/pegasus-xsum;unknown,[NeurIPS'22 Spotlight] CoNT: Contrastive Neural Text Generation 
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,e506fae502684ba9901d60ae85be87ab,naver_gdc/gdc/dpg/gdc/metrics.py,gpt2,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.ChineseCLIPConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,21282bb32ff18e0a676f8781c5803aee,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/chinese_clip/convert_chinese_clip_original_pytorch_to_hf.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoModelForSeq2SeqLM.from_pretrained.cuda;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,29339c276be28591779485a64cafb57f,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/seq2seq/run_distributed_eval.py,;model_name,Source code for Zalo AI 2021 submission
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,3c9ec8c3cb2f0417490fd3c772e9b072,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectSentimentTripletExtraction/models/model.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.TFBertForQuestionAnswering.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/abhijithneilabraham/tableQA,261,902a6c1533934b38b4d035c439f66210,abhijithneilabraham_tableQA/tableQA/tableqa/nlp.py,"bert-large-uncased-whole-word-masking-finetuned-squad;bert-large-uncased-whole-word-masking-finetuned-squad,padding=True",AI Tool for querying natural language on tabular data.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,9a8789411a4a99e33e80d9cd6912c3c9,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/text2sql/scripts/annotate_binder_program.py,pretrained_model_name_or_path=os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.FSMTConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,143422b932af4df97a9b51f409b4d258,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py,pytorch_dump_folder_path,Source code for Zalo AI 2021 submission
transformers.FSMTTokenizer.from_pretrained;transformers.FSMTConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,53c5a60540cad2dc7a6510e010128b80,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/scripts/fsmt/fsmt-make-tiny-model.py,mname;mname,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/wzhouad/ATLOP,174,c33cba38a8f968bdc4bce9db00ad7d31,wzhouad_ATLOP/ATLOP/train_bio.py,"unknown,num_labels=unknown;unknown;unknown,from_tf=bool,config=config","Source code for paper ""Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling"", AAAI 2021"
transformers.AutoModelForSequenceClassification.from_pretrained,github.com/shibing624/MedicalGPT,1876,f2db6260d8358e660ca6c2f21a275381,shibing624_MedicalGPT/MedicalGPT/merge_peft_adapter.py,"base_model_path,num_labels=1,load_in_8bit=False,torch_dtype=unknown,trust_remote_code=True,device_map=auto",MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. 训练医疗大模型，实现了包括增量预训练、有监督微调、RLHF(奖励建模、强化学习训练)和DPO(直接偏好优化)。
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,7f16ad793a87e453f172244beb5b517a,salesforce_CodeRL/CodeRL/transformers/hubconf.py,"unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs;unknown,None=kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/andreamad8/FSB,119,ecc8ce09191eff7e454574ac471b941b,andreamad8_FSB/FSB/utils/utils.py,"model_checkpoint,low_cpu_mem_usage=True;model_checkpoint;model_checkpoint;model_checkpoint",The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
transformers.AutoModelForSeq2SeqLM.from_pretrained.cuda;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,0a0150dad652c489e636402ad56c17b6,salesforce_CodeRL/CodeRL/transformers/examples/legacy/seq2seq/run_distributed_eval.py,;model_name,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.MarianTokenizer.from_pretrained,github.com/songhaoyu/BoB,134,03a652dc7f6c4638ceb0a772e605da2f,songhaoyu_BoB/BoB/xlibs/convert_marian_to_pytorch.py,str,The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,3a78b7d188d0f4217db749546318f54f,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/run_swag.py,"unknown;unknown;unknown,from_tf=bool,config=config;unknown;unknown;checkpoint;checkpoint",Source code for Zalo AI 2021 submission
transformers.CLIPProcessor.from_pretrained;transformers.CLIPVisionModel.from_pretrained,github.com/johannakarras/DreamPose,748,abd68385c379f77bb889259c81b7b987,johannakarras_DreamPose/DreamPose/pipelines/dual_encoder_pipeline.py,openai/clip-vit-base-patch32;openai/clip-vit-base-patch32,"Official implementation of ""DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"""
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.T5Config.from_pretrained;transformers.T5Config.from_pretrained;transformers.FlaxT5ForConditionalGeneration.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,8d2fa21425e922846afa4a946f3d0b5f,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/flax/language-modeling/run_t5_mlm_flax.py,"unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,vocab_size=len;unknown,cache_dir=unknown;unknown,config=config,seed=unknown,dtype=getattr",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained,github.com/zjunlp/OntoProtein,122,85da2bf03f57501375ee99b57b89ba8f,zjunlp_OntoProtein/OntoProtein/src/models.py,config_path,"Code and datasets for the ICLR2022 paper ""OntoProtein: Protein Pretraining With Gene Ontology Embedding"""
transformers.RobertaTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,39c4a98aacdcfff69d30e55731c80ca3,salesforce_CodeRL/CodeRL/datasets/apps_dataset.py,Salesforce/codet5-base,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.Wav2Vec2Processor.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained.to,github.com/Rongjiehuang/GenerSpeech,284,a7e740348752625d0429fb7dfaf30e31,Rongjiehuang_GenerSpeech/GenerSpeech/inference/base_tts_infer.py,facebook/wav2vec2-base-960h;unknown,PyTorch Implementation of GenerSpeech (NeurIPS'22): a text-to-speech model towards zero-shot style transfer of OOD custom voice.
transformers.AutoConfig.from_pretrained;transformers.AutoModelForImageClassification.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,945353b385ee082397a916344253858b,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/image-classification/run_image_classification.py,"unknown,num_labels=len,label2id=label2id,id2label=id2label,finetuning_task=image-classification,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.half;transformers.AutoTokenizer.from_pretrained,github.com/scutcyr/SoulChat,206,198e615bcbba11898582a09bef5ce975,scutcyr_SoulChat/SoulChat/soulchat_app.py,"model_name_or_path,trust_remote_code=True;;model_name_or_path,trust_remote_code=True",中文领域心理健康对话大模型SoulChat
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,205d10ad7eee7b8f2e32baaacd58ef98,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/summarization/run_summarization.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/qiuhuachuan/smile,203,fbf3dabbf98a6a8becfa1e46f910c34b,qiuhuachuan_smile/smile/MeChat_local.py,"THUDM/chatglm-6b,revision=v0.1.0,trust_remote_code=True;THUDM/chatglm-6b,trust_remote_code=True",SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support
transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,884cc2a3758f07c2d50bb12c7937f79a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py,"model_path;model_path,use_fast=use_fast_tokenizer;unknown,num_labels=len;unknown,use_fast=unknown",Source code for Zalo AI 2021 submission
transformers.AutoModelForSeq2SeqLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,175fca22d2dce104a48c8e93c237503d,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/seq2seq/run_eval.py,device;model_name,Source code for Zalo AI 2021 submission
transformers.RobertaTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,9ec91f95763067b2a855a108bd68db2b,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py,roberta-base;pytorch_dump_folder_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.PerceiverTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,9c2b9c69d99f16652ac978a3a64ae81b,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py,/Users/NielsRogge/Documents/Perceiver/Tokenizer files,Source code for Zalo AI 2021 submission
transformers.BertTokenizer.from_pretrained;transformers.BertConfig.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,3619768c6c7ed8b70fb87d1fe07456a0,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/retrieval/single_trainer.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,aaa410cdc0749bdf598d2d6d60926549,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/proton/preprocess/test_g.py,device;os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,07acd2b73a16a9066e4aacc36bf408b0,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/complete_nlp_example.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoTokenizer.from_pretrained,github.com/wangyuxinwhy/uniem,572,276ae02cf6eb272255b418940fe3ab3b,wangyuxinwhy_uniem/uniem/scripts/train_m3e.py,model_name_or_path,unified embedding model
transformers.CLIPConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,d7e207b731d27c72d66a4e8b728e599a,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.PegasusTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,06ba0b91a35e975b49502d2316b4ee44,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/pegasus/convert_pegasus_tf_to_pytorch.py,"sshleifer/pegasus,model_max_length=desired_max_model_length","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.GPT2TokenizerFast.from_pretrained,github.com/Vahe1994/SpQR,466,fc80131c84121a9e77cb9bbffbf7b39c,Vahe1994_SpQR/SpQR/lm-evaluation-harness/lm_eval/models/gpt3.py,gpt2,
transformers.WhisperProcessor.from_pretrained;transformers.WhisperProcessor.from_pretrained,github.com/sanchit-gandhi/whisper-jax,3647,7c260086888b609d6b049a0e580909f7,sanchit-gandhi_whisper-jax/whisper-jax/benchmarks/run_pipeline_dataloader.py,unknown;unknown,JAX implementation of OpenAI's Whisper model for up to 70x speed-up on TPU.
transformers.TFVisionEncoderDecoderModel.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,8e281403916c921a2bec132e55d73325,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py,"pretrained_model_name_or_path,unknown,None=kwargs","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,106c546675a5dae3c0ee36688d51d66a,ylsung_VL_adapter/VL_adapter/VL-T5/src/video/yc2c_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.RobertaTokenizer.from_pretrained;transformers.LukeTokenizer.from_pretrained;transformers.LukeTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,88db77cc792ff966d2c4ec27601214a8,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py,"metadata;pytorch_dump_folder_path;pytorch_dump_folder_path,task=entity_classification","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/allenai/comet-atomic-2020,201,838abf3f475c40b3a8b821568e43f8d0,allenai_comet-atomic-2020/comet-atomic-2020/models/gpt2_zeroshot/gpt2-zeroshot.py,model_name;model_name,
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/the-crypt-keeper/can-ai-code,308,5c82447c57dd495faa521b04e5de63ad,the-crypt-keeper_can-ai-code/can-ai-code/interview-awq-modal.py,"unknown,trust_remote_code=True;unknown,trust_remote_code=True",Self-evaluating interview for AI coders
transformers.ViTMAEModel.from_pretrained,github.com/ssundaram21/dreamsim,229,f48b3d0e11d9be0608f15adba834037e,ssundaram21_dreamsim/dreamsim/dreamsim/feature_extraction/load_mae_as_vit.py,"facebook/vit-mae-base,cache_dir=load_dir",DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data (NeurIPS 2023 Spotlight)
transformers.AutoTokenizer.from_pretrained,github.com/YuxinWenRick/tree-ring-watermark,145,8f21c9bb5ba02a08b15e974009fa6d0c,YuxinWenRick_tree-ring-watermark/tree-ring-watermark/open_clip/tokenizer.py,tokenizer_name,
transformers.RagTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,930d202de586a1aced010ee429a7e19c,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag/finetune_rag.py,unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/zetaalphavector/inPars,138,86aa9c87d7af412705de06669c1ef4cf,zetaalphavector_inPars/inPars/inpars/rerank.py,"model_name_or_path;model_name_or_path,None=model_args;model_name_or_path;model_name_or_path,None=model_args;model_name_or_path",Inquisitive Parrots for Search
transformers.GPT2Config.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/thunlp/OpenBackdoor,114,060ec2c56c81ccc914e9c30a881fec89,thunlp_OpenBackdoor/OpenBackdoor/openbackdoor/attackers/poisoners/trojanlm_poisoner.py,"model_path;model_path,config=unknown;model_path","An open-source toolkit for textual backdoor attack and defense (NeurIPS 2022 D&B, Spotlight)"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,7ae8aba265417e46b46f74e46de4dcee,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/gloc/processor/__init__.py,pretrained_model_name_or_path=facebook/bart-large,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,110dbea23b87a207cda98c8f73b8e577,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/seq2seq-distillation/_test_seq2seq_examples.py,m;BART_TINY,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.models.bert.tokenization_bert.BertTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,76ba1c78ef1d9b6df77717781d123509,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/mlm_wwm/run_chinese_ref.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/bofenghuang/vigogne,448,9e08dfbecad92fb122d834a9c912724f,bofenghuang_vigogne/vigogne/vigogne/demo/demo_instruct.py,"base_model_name_or_path,padding_side=right,use_fast=False;base_model_name_or_path,load_in_8bit=load_8bit,torch_dtype=unknown,device_map=auto;base_model_name_or_path,device_map=Dict,torch_dtype=unknown;base_model_name_or_path,device_map=Dict,low_cpu_mem_usage=True",French instruction-following and chat models
transformers.T5ForConditionalGeneration.from_pretrained;transformers.T5Tokenizer.from_pretrained,github.com/danielgross/teleprompter,317,f8f8a2333ca76c70f66012470e411fa5,danielgross_teleprompter/teleprompter/main.py,model_name;model_name,
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained.save_pretrained,github.com/salesforce/CodeRL,436,1a19748a2ca717a05cd12ff63bbfd377,salesforce_CodeRL/CodeRL/transformers/examples/legacy/seq2seq/save_randomly_initialized_model.py,"config_name,None=config_kwargs;save_dir",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.Wav2Vec2Processor.from_pretrained;transformers.HubertForCTC.from_pretrained.to,github.com/YuanGongND/whisper-at,199,ab5f18c91dd5aaa32dfe0acb14f130a0,YuanGongND_whisper-at/whisper-at/src/noise_robust_asr/asr_experiments/transcribe_hubert_large.py,facebook/hubert-large-ls960-ft;device,"Code and Pretrained Models for Interspeech 2023 Paper ""Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong Audio Event Taggers"""
transformers.BertTokenizer.from_pretrained,github.com/Yui010206/SeViLA,121,c84fe0c356bc62c9231114c8472f02a2,Yui010206_SeViLA/SeViLA/lavis/models/albef_models/__init__.py,bert-base-uncased,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.AutoTokenizer.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,15a32cfe768293cb4c2e1825b3403e60,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/src/accelerate/test_utils/training.py,bert-base-cased,We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,324ff5a10272bf5b4271355f241d53cd,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/train_qa.py,/private/home/span-bert;bert-large-cased;unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/tabtoyou/KoLLaVA,133,21b84d6941dc2b51b78ffaa24e98e961,tabtoyou_KoLLaVA/KoLLaVA/llava/model/apply_delta.py,"base_model_path,torch_dtype=unknown,low_cpu_mem_usage=True;delta_path",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.models.data2vec.data2vec_text.Data2VecTextModel.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,be58ef80098134ed87b7f1113445e452,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/data2vec/convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py,"data2vec_checkpoint_dir,checkpoint_file=data2vec_checkpoint_file_name","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained.cuda,github.com/tabtoyou/KoLLaVA,133,6960e71746f9b41095751d060338fb17,tabtoyou_KoLLaVA/KoLLaVA/llava/eval/model_qa.py,model_name;,KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,5f70931909e7d502a7bd69fc03d32830,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/question-answering/run_squad.py,"unknown,cache_dir=unknown;unknown,do_lower_case=unknown,cache_dir=unknown,use_fast=False;unknown,from_tf=bool,config=config,cache_dir=unknown;unknown;unknown,do_lower_case=unknown,use_fast=False;checkpoint",Source code for Zalo AI 2021 submission
transformers.AutoFeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,eed08914085e7493d2a3310e1ae69398,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/swinv2/convert_swinv2_timm_to_pytorch.py,format,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.PegasusTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,4a15917e891dd5c6ad7abc34472d2541,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/pegasus/convert_pegasus_tf_to_pytorch.py,"sshleifer/pegasus,model_max_length=desired_max_model_length",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,128c630378b64528f8daad731d2307d1,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/sunsql/utils/example.py,os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.XLMRobertaTokenizerFast.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,ed171b5862b9b4f36538e7749d03883c,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/donut/convert_donut_to_pytorch.py,"model_name,from_slow=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoModel.from_pretrained;transformers.BertForNextSentencePrediction.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ca4b1bb19cee5775681eb13734844b87,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dial-start/model.py,"unknown;unknown,num_labels=2,output_attentions=False,output_hidden_states=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModelForCausalLM.from_pretrained;transformers.LlamaTokenizer.from_pretrained,github.com/shmsw25/FActScore,119,9b369846eae6629a1be1d0fcc3e71d7b,shmsw25_FActScore/FActScore/factscore/clm.py,unknown;unknown,"A package to evaluate factuality of long-form generation. Original implementation of our EMNLP 2023 paper ""FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"""
transformers.models.bert.BertTokenizer.from_pretrained;transformers.models.bert.BertTokenizer.from_pretrained;transformers.models.roberta.RobertaTokenizer.from_pretrained;transformers.models.bert.BertConfig.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,f742b2bb7f6357140bb0833e521c0590,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/retrievermodel/hf_models.py,"pretrained_cfg_name,do_lower_case=do_lower_case;/mnt/netapp7/ohadr/prompt_sel/DPR/bert-base-uncased,do_lower_case=do_lower_case;pretrained_cfg_name,do_lower_case=do_lower_case;unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.Wav2Vec2Processor.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained,github.com/jonatasgrosman/wav2vec2-sprint,143,00206dc548ecbd60c23e2bff9020cff2,jonatasgrosman_wav2vec2-sprint/wav2vec2-sprint/common_voice_usage.py,MODEL_ID;MODEL_ID,
transformers.BertTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,b816f2f4c980b96be334b777dac90644,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/run_chinese_ref.py,unknown,Source code for Zalo AI 2021 submission
transformers.RagRetriever.from_pretrained,github.com/salesforce/CodeRL,436,06a8e5e24d577198dbcbe857acd6fcb7,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag/eval_rag.py,"checkpoint,None=model_kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.BlipProcessor.from_pretrained;transformers.BlipForConditionalGeneration.from_pretrained.to,github.com/AIGC-Audio/AudioGPT,9397,cf7cc7c53be231e796deb264c3b6aee4,AIGC-Audio_AudioGPT/AudioGPT/audio-chatgpt.py,Gustavosta/MagicPrompt-Stable-Diffusion;Gustavosta/MagicPrompt-Stable-Diffusion;Salesforce/blip-image-captioning-base;unknown,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,cdbe77c1df622b58096e6129b2415208,yangheng95_PyABSA/PyABSA/pyabsa/tasks/RNAClassification/prediction/rna_classifier.py,find_cwd_dir;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.to,github.com/facebookresearch/ParlAI,10365,e62ee545af958b6a8b11090c6b0a3f1a,facebookresearch_ParlAI/ParlAI/parlai/tasks/reasoning/reason_types/step_by_step.py,Vamsi/T5_Paraphrase_Paws;unknown,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,7331509501355e3082f2da48afdac69c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_grapter_512_ft.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,e8658edda6f44e29e1d1b10aa3288b48,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/end2end.py,roberta-base;roberta-base;google/electra-large-discriminator;google/electra-large-discriminator,Multi-hop dense retrieval for question answering
transformers.AutoConfig.from_pretrained,github.com/songhaoyu/BoB,134,97f78e2510fba63d0e352246af752841,songhaoyu_BoB/BoB/xlibs/benchmark/benchmark_utils.py,model_name,The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained;transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,8a104dfe2a4465717d040b2eed015d5c,ylsung_VL_adapter/VL_adapter/VL-T5/src/classification_raw_data.py,"unknown,max_length=unknown,do_lower_case=unknown;unknown,do_lower_case=unknown;unknown,max_length=unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,a68e6e6ef38384003de326727b5d1a3e,yangheng95_PyABSA/PyABSA/pyabsa/tasks/_Archive/RNAClassification/instructor/rnac_instructor.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.CLIPModel.from_pretrained.to;transformers.CLIPProcessor.from_pretrained,github.com/NyanNyanovich/nyan,131,8777c772af8d42f88c334d868edadf9f,NyanNyanovich_nyan/nyan/nyan/clip.py,device;model_name,Automatic news aggregator in Telegram / Автоматический агрегатор новостей в Телеграме
transformers.models.bert.BertTokenizer.from_pretrained;transformers.models.bert.BertTokenizer.from_pretrained;transformers.models.roberta.RobertaTokenizer.from_pretrained;transformers.models.bert.BertConfig.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c5a1cc285be8664e571abf36a94e5c36,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/dpr/models/hf_models.py,"pretrained_cfg_name,do_lower_case=do_lower_case;/mnt/netapp7/ohadr/prompt_sel/DPR/bert-base-uncased,do_lower_case=do_lower_case;pretrained_cfg_name,do_lower_case=do_lower_case;unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.models.auto.AutoConfig.from_pretrained;transformers.models.auto.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,fa740e5eacc78cd111d91fd604550a29,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/seq2seq/run_peteshaw_dev.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown,max_length=unknown,num_beams=unknown,num_beam_groups=unknown,diversity_penalty=unknown,gradient_checkpointing=unknown,use_cache=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertConfig.from_pretrained;transformers.BertForMaskedLM.from_pretrained,github.com/fastnlp/CPT,448,6fc22c52c7f6173a18618f658437ec22,fastnlp_CPT/CPT/pretrain/megatron/model/hfbert_model.py,"roberta-zh/base;roberta-zh/base,config",CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.BertTokenizer.from_pretrained;transformers.RobertaTokenizer.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,31797d47171370de9a11549fe3ed09fb,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/distillation/scripts/binarized_data.py,unknown;unknown;unknown,Source code for Zalo AI 2021 submission
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,0aa123da7d244ba3b4c5b24f0af352c8,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/slot_based_dialog/chinese_t5/T5.py,./t5_chinese_small,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/naver/gdc,112,aed8c92c3e0ffdaab7f0c8373e90980a,naver_gdc/gdc/dpg/gdc/scorer.py,gpt2-medium;gpt2-medium,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.WhisperProcessor.from_pretrained,github.com/sanchit-gandhi/whisper-jax,3647,8becce1af1dfa4ca6b9efc0682003f24,sanchit-gandhi_whisper-jax/whisper-jax/benchmarks/run_pjit_dataloader.py,openai/whisper-large-v2,JAX implementation of OpenAI's Whisper model for up to 70x speed-up on TPU.
transformers.AutoConfig.from_pretrained;transformers.FlaxAutoModel.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.FlaxAutoModel.from_pretrained,github.com/salesforce/CodeRL,436,58f4732d51761426c88a046fb3699155,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/jax-projects/hybrid_clip/modeling_hybrid_clip.py,"text_model_name_or_path;text_model_name_or_path,unknown,None=kwargs_text;vision_model_name_or_path;vision_model_name_or_path,unknown,None=kwargs_vision",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,0bea2b4ed493cba0fe1a20a5202f688d,yangheng95_PyABSA/PyABSA/examples-v2/train_custom_BPE_tokenizers.py,bpe_tokenizer,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,0f2b0c387c557e3e70df7a0adf5fde0f,salesforce_CodeRL/CodeRL/transformers/examples/legacy/seq2seq/old_test_datasets.py,"tok_name;tok;facebook/mbart-large-cc25;MARIAN_TINY;tok_name,use_fast=False",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.DeiTForImageClassification.from_pretrained;transformers.DeiTFeatureExtractor.from_pretrained,github.com/qanastek/HugsVision,185,e151a3a24695cca4112dab2552b436fd,qanastek_HugsVision/HugsVision/recipes/kvasir_v2/binary_classification/train_example_deit.py,"huggingface_model,num_labels=len,label2id=label2id,id2label=id2label;huggingface_model",HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,8c79a67ae4be69a990b3b171720cd182,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/seq2seq-distillation/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/fastnlp/CPT,448,781a57f1e1cd00e1bf357f007097191c,fastnlp_CPT/CPT/finetune/classification/run_clue_prompt.py,unknown;pretrained_model_name_or_path=unknown;unknown,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoModelForCTC.from_pretrained;transformers.AutoProcessor.from_pretrained;transformers.Wav2Vec2Processor.from_pretrained,github.com/salesforce/CodeRL,436,24e38614fff1523d7f9dec7b061eb92c,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py,"unknown,cache_dir=unknown,use_auth_token=unknown;tokenizer_name_or_path,use_auth_token=unknown,None=tokenizer_kwargs;unknown,cache_dir=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,config=config,use_auth_token=unknown;unknown;unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.LlamaForCausalLM.from_pretrained;transformers.LlamaConfig.from_pretrained,github.com/ypwhs/CreativeChatGLM,203,124b2af7a0e4e777d9cf41978a6de2c5,ypwhs_CreativeChatGLM/CreativeChatGLM/gptq/llama.py,"model,torch_dtype=auto;model",👋 欢迎来到 ChatGLM 创意世界！你可以使用修订和续写的功能来生成创意内容！
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,7aec7f4c3a1048e6a89d493ed29bbbb3,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/language-modeling/run_mlm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,9c5d064ce15a5a1e952cabcb24121078,AIGC-Audio_AudioGPT/AudioGPT/text_to_audio/Make_An_Audio/wav_evaluation/models/CLAPWrapper.py,unknown,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.T5Tokenizer.from_pretrained;transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,0e17aa33d59cb4e6f393014d66849777,awslabs_pptod/pptod/DST/learn.py,unknown;unknown,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/zetaalphavector/inPars,138,f33acd6da92eea4ae23bfb579d8f012e,zetaalphavector_inPars/inPars/inpars/inpars.py,"base_model,padding_side=left;base_model,revision=revision;base_model,None=model_kwargs",Inquisitive Parrots for Search
transformers.AutoFeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,44e6b2da76cfadbd90d043eaead1188f,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/swin/convert_swin_timm_to_pytorch.py,format,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.QDQBertConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.QDQBertForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,80d0635c508a693269b8b91493a31759,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/quantization-qdqbert/run_quant_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.eval.cuda,github.com/bofenghuang/vigogne,448,1a76b06a22d0f157f3b432f552d03082,bofenghuang_vigogne/vigogne/vigogne/data/translate_oasst_trees.py,model_name_or_path;,French instruction-following and chat models
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/VideoCrafter/VideoCrafter,3181,0c0e7872a0f9f03b5764101d11dbcbc4,VideoCrafter_VideoCrafter/VideoCrafter/lvdm/models/modules/condition_modules.py,version;version,VideoCrafter1: Open Diffusion Models for High-Quality Video Generation
transformers.DetrForObjectDetection.from_pretrained,github.com/qanastek/HugsVision,185,31da07c2dfa912d8b221e2f65a3f28e6,qanastek_HugsVision/HugsVision/hugsvision/models/Detr.py,model_path,HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/guifaChild/text_to_vedio,158,94d4b1bbcdfa9ae4a4ca11f286177ee6,guifaChild_text_to_vedio/text_to_vedio/ChatGLM-6B-main/ptuning/web_demo.py,"unknown,trust_remote_code=True;unknown,trust_remote_code=True;unknown,config=config,trust_remote_code=True;unknown,config=config,trust_remote_code=True",这是一个由文本直接生成视频的项目
transformers.AutoModel.from_pretrained,github.com/memray/OpenNMT-kpg-release,210,9003d02f41f252bd06a1a465d47bb744,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/encoders/pretrained_encoder.py,"model_name,cache_dir=cache_dir",Keyphrase Generation
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,8e6d40e1971c4251e275e9683b6a2052,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/prefixtuning.py,"unknown,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoProcessor.from_pretrained;transformers.AutoModelForCTC.from_pretrained.to,github.com/ashawkey/RAD-NeRF,683,cb0b2989084be973022d9fd2d6847605,ashawkey_RAD-NeRF/RAD-NeRF/nerf/asr.py,unknown;unknown,Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial Decomposition
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,123ef7fddce00befb3ed0c9cb13be2a5,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/data_systhesis/snowball/run_snowball.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.LlamaForCausalLM.from_pretrained,github.com/ypwhs/CreativeChatGLM,203,d8d1a60458e697bd135304e7a7ebc95a,ypwhs_CreativeChatGLM/CreativeChatGLM/predictors/llama.py,"model_name,resume_download=True;model_name,low_cpu_mem_usage=True,resume_download=True,torch_dtype=unknown,device_map=Dict",👋 欢迎来到 ChatGLM 创意世界！你可以使用修订和续写的功能来生成创意内容！
transformers.CLIPVisionModel.from_pretrained.cuda;transformers.CLIPProcessor.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTokenizer.from_pretrained,github.com/johannakarras/DreamPose,748,658545002fd600377d15f9416f700ade,johannakarras_DreamPose/DreamPose/finetune-unet.py,";openai/clip-vit-base-patch32;unknown,revision=unknown;unknown,subfolder=tokenizer,revision=unknown","Official implementation of ""DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"""
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,99608598750a60784466d981885ec4d5,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/scripts/wtq/run_cloze.py,pretrained_model_name_or_path=../../utils_file/gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,0976387a1b94dd1121de983b9f816bd2,awslabs_pptod/pptod/data/multiwoz/utlis/postprocessing_dataset.py,model_path,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/KRR-Oxford/DeepOnto,119,2712bdfecde40831084420566ddb7060,KRR-Oxford_DeepOnto/DeepOnto/src/deeponto/subs/bertsubs/bert_classifier.py,bert_checkpoint;bert_checkpoint,A package for ontology engineering with deep learning and language models.
transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,da29f063dbf71b40632299f34adcf11e,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/quantization-qdqbert/evaluate-hf-trt-qa.py,"unknown,use_fast=True",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f074de70b139a5d01330bd7bf193b61c,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py,"unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/tabtoyou/KoLLaVA,133,5fce4042ab2348178fd7bfba390dfb0c,tabtoyou_KoLLaVA/KoLLaVA/llava/serve/cli.py,"model_name;model_name,low_cpu_mem_usage=True,None=kwargs",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,817bbb8495d0eae5a1e5293e0d99fb87,awslabs_pptod/pptod/IC/learn.py,unknown,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/lensterxyz/lenster,21455,ef461698c512fad66c6a07a79224e2cd,lensterxyz_lenster/lenster/packages/ai/locale_route.py,locale_hf;locale_hf,Hey is a decentralized and permissionless social media app built with Lens Protocol 🌿
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,c4439a7644d2d3f3f3c476bff71d3754,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag-end2end-retriever/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.GPT2Tokenizer.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/AdvancedLiterateMachinery,388,9b8afc8be5a07095d8fb87b6b8845bc3,AlibabaResearch_AdvancedLiterateMachinery/AdvancedLiterateMachinery/OCR/MGP-STR/utils.py,gpt2;bert-base-uncased,"A collection of original, innovative ideas and algorithms towards Advanced Literate Machinery. This project is maintained by the OCR Team in the Language Technology Lab, Alibaba DAMO Academy."
transformers.UniSpeechConfig.from_pretrained,github.com/salesforce/CodeRL,436,f4b642fa4d1af2fd4d4179e70b1011c9,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,679d1783afd0918e742f68ea8a469207,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag-end2end-retriever/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",Source code for Zalo AI 2021 submission
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained.to,github.com/omerbt/MultiDiffusion,774,b3d194c073801563b3bf15e17595332e,omerbt_MultiDiffusion/MultiDiffusion/panorama.py,"model_key,subfolder=tokenizer;unknown","Official Pytorch Implementation for ""MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation"" presenting ""MultiDiffusion"" (ICML 2023)"
transformers.LongformerModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d6f2e482026fee62c0c6100a6726ce0f,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dstc11-simmc/task1/model/backbone.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/cloneofsimo/paint-with-words-sd,601,aac28965aad5ba258dd838d361e0062c,cloneofsimo_paint-with-words-sd/paint-with-words-sd/paint_with_words/paint_with_words.py,"model_path,subfolder=tokenizer;model_path,subfolder=text_encoder",Implementation of Paint-with-words with Stable Diffusion : method from eDiff-I that let you generate image from text-labeled segmentation map.
transformers.RagRetriever.from_pretrained,github.com/salesforce/CodeRL,436,cce11be8fff2d3eb91f1d9d2f9ffa61a,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag-end2end-retriever/eval_rag.py,"checkpoint,None=model_kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.Wav2Vec2CTCTokenizer.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained,github.com/salesforce/CodeRL,436,c7162381a6c7378180612a125008cd6e,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/wav2vec2/run_asr.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown,do_lower_case=unknown,word_delimiter_token=unknown;unknown,cache_dir=unknown,gradient_checkpointing=unknown,vocab_size=len",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,f7567dc7285e1fe85fd88d4b4e376f75,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/LGESQL/sparc/model/encoder/graph_input.py,os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,4d6480a75bb365b8acbb3b9b9278676a,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/slot_based_dialog/chinese_t5/train.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.DetrFeatureExtractor.from_pretrained,github.com/qanastek/HugsVision,185,22e792664b485a6acd7f0bbb06b4dd69,qanastek_HugsVision/HugsVision/build/lib/hugsvision/nnet/ObjectDetectionTrainer.py,unknown,HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.OPTConfig.from_pretrained.to_dict;transformers.OPTConfig.from_pretrained.to_dict;transformers.T5Config.from_pretrained.to_dict;transformers.T5Config.from_pretrained.to_dict;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,081a1a683a6cb54b2cceaeb69997d2a6,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/blip_2/convert_blip_2_original_to_pytorch.py,;;;;facebook/opt-2.7b;google/flan-t5-xl,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoModel.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,ca488b6e713938f2a9a4f19b6cd1fc0e,AIGC-Audio_AudioGPT/AudioGPT/text_to_audio/Make_An_Audio/ldm/modules/encoders/CLAP/clap.py,text_model,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/shmsw25/FActScore,119,9d72b5a3b6eb46e4ede78504bc85a526,shmsw25_FActScore/FActScore/factscore/npm.py,unknown;unknown,"A package to evaluate factuality of long-form generation. Original implementation of our EMNLP 2023 paper ""FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"""
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoModelForCTC.from_pretrained;transformers.AutoProcessor.from_pretrained;transformers.Wav2Vec2Processor.from_pretrained,github.com/salesforce/CodeRL,436,3ee33356c80a99068af2c119d738fcbc,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py,"unknown,cache_dir=unknown,use_auth_token=unknown;tokenizer_name_or_path,use_auth_token=unknown,None=tokenizer_kwargs;unknown,cache_dir=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,config=config,use_auth_token=unknown;unknown;unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoProcessor.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/theovercomer8/captionr,133,9697466bc23ddf01e959b720a5c7ddcc,theovercomer8_captionr/captionr/captionr/git_cap.py,unknown;unknown,GIT/BLIP/CLIP Caption tool
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/siddk/voltron-robotics,147,d683091f654a6ffff512e53cb0950bad,siddk_voltron-robotics/voltron-robotics/voltron/models/core/vgen.py,"language_model,cache_dir=hf_cache;language_model,cache_dir=hf_cache",Voltron: Language-Driven Representation Learning for Robotics
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/huggingface/instruction-tuned-sd,127,438ab8096b02ac0d7b56e5a3cb3dc821,huggingface_instruction-tuned-sd/instruction-tuned-sd/train_instruct_pix2pix.py,"unknown,subfolder=tokenizer,revision=unknown;unknown,subfolder=text_encoder,revision=unknown",Code for instruction-tuning Stable Diffusion.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c5c8bb092733293c1e4ba7711beecc66,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/adaptertuning.py,"unknown,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/salesforce/CodeRL,436,d8016a507e29dd7dc70d5353c43198af,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/pplm/run_pplm_discrim_train.py,pretrained_model;pretrained_model,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.LongformerTokenizerFast.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,29309de42fd04e511614db8ef4737bd6,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dstc11-simmc/task1/eval_dstc11_task1.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.PreTrainedTokenizerFast.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d945b67519227355476d9c6637604674,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/deep-thinking/models/huggingface.py,"sign,padding_side=padding_side,cache_dir=str;sign,padding_side=padding_side,cache_dir=str;sign,cache_dir=str,device_map=auto,load_in_8bit=in_8bit",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/Xpitfire/symbolicai,720,75d151a24185ee164b4608dbb9a8d624,Xpitfire_symbolicai/symbolicai/symai/backend/services/huggingface_seq2seqlm_server.py,"unknown,use_fast=False,local_files_only=False,output_hidden_states=True;unknown,torch_dtype=dtype,pad_token_id=unknown",Compositional Differentiable Programming Library
transformers.T5Config.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/awslabs/pptod,148,583421bab0d31cdb68298f6f6338fcf8,awslabs_pptod/pptod/Pretraining/modelling/T5Model.py,"model_path;model_path,config=t5_config,resume_download=True",Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,5990c28962ad58261d033ea45979732b,awslabs_pptod/pptod/data/pre-training_corpora/utlis/tokenize_all_datasets.py,tokenizer_path,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelWithLMHead.from_pretrained,github.com/salesforce/CodeRL,436,b9db356205348ddaa815a24464c7ad12,salesforce_CodeRL/CodeRL/transformers/examples/legacy/run_language_modeling.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.CLIPTokenizer.from_pretrained,github.com/cross-domain-compositing/cross-domain-compositing,167,438dd2af10be9337982ea928cd6a18ad,cross-domain-compositing_cross-domain-compositing/cross-domain-compositing/cdc/prompt_masking.py,openai/clip-vit-large-patch14,
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/sail-sg/EditAnything,2813,2745b7e7d5adb496fae376ec954e5798,sail-sg_EditAnything/EditAnything/utils/texutal_inversion.py,"unknown;unknown,subfolder=tokenizer;unknown,subfolder=text_encoder,revision=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,3923a9d760ca51d9205f6bb7640f3c8a,naver_gdc/gdc/rm_vs_dm/gdc/examples/run.py,config,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,7da391cfa89cdb4e3d75b8afdf92f251,ashkamath_mdetr/mdetr/datasets/mixed.py,unknown,
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForMaskedLM.from_pretrained,github.com/salesforce/CodeRL,436,985badfd84021c6c4ed6d61cb975ba5c,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,config=config,seed=unknown,dtype=getattr",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,a1763218b69cf4c8554c2803fcd69f3c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/diana/downstreamdeca/dianawotask.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Model.from_pretrained,github.com/facebookresearch/ParlAI,10365,834512e2d84d4f5f1284f659153d418b,facebookresearch_ParlAI/ParlAI/parlai/agents/hugging_face/gpt2.py,fle_key,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.T5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/salesforce/UniControl,525,0818a3cecbba5046d5a34e07065c9e4f,salesforce_UniControl/UniControl/ldm/modules/encoders/modules.py,version;version;version;version,Unified Controllable Visual Generation Model
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.HubertModel.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.HubertModel.from_pretrained,github.com/fishaudio/fish-diffusion,500,497fbe1929bb894f6339287346140db7,fishaudio_fish-diffusion/fish-diffusion/fish_diffusion/modules/feature_extractors/chinese_hubert.py,TencentGameMate/chinese-hubert-base;TencentGameMate/chinese-hubert-base;model;model,An easy to understand TTS / SVS / SVC framework
transformers.WavLMForSequenceClassification.from_pretrained;transformers.WavLMForAudioFrameClassification.from_pretrained;transformers.WavLMForXVector.from_pretrained;transformers.WavLMConfig.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,6371e24d82b20b41dad7eee3a6b6145e,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/wavlm/convert_wavlm_original_s3prl_checkpoint_to_pytorch.py,"base_model_name,config=hf_config;base_model_name,config=hf_config;base_model_name,config=hf_config;config_path;base_model_name,return_attention_mask=True,do_normalize=False","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertTokenizerFast.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained;transformers.CLIPVisionModel.from_pretrained;transformers.CLIPVisionModel.from_pretrained,github.com/kangyeolk/Paint-by-Sketch,156,36494907b3f6087d9ad67bb3597e9ef6,kangyeolk_Paint-by-Sketch/Paint-by-Sketch/ldm/modules/encoders/modules.py,bert-base-uncased;version;version;version;version;version;version,Stable Diffusion-based image manipulation method with a sketch and reference image
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,1d29ec2d7e155f9b2050e8bb4e2311f3,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/diana/downstream/create_l2p.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModelForTokenClassification.from_pretrained,github.com/bminixhofer/wtpsplit,422,81c96be301f932bafeb5986892fd1b10,bminixhofer_wtpsplit/wtpsplit/scripts/export_to_onnx.py,unknown,Code for Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained,github.com/qiuhuachuan/smile,203,465ae89235275be3318f89929920aa29,qiuhuachuan_smile/smile/src/tokenize_dataset_rows.py,"model_name,trust_remote_code=True;model_name,trust_remote_code=True,device_map=auto",SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support
transformers.LlamaTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,3363892ab3ecac930c27604cab8e3a27,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/PRO/train/utils/data_manager.py,"tokenizer_path,use_fast=False;tokenizer_path,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,4ceef994ee5e643ac2177c85f75ea8ce,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/question-answering/run_seq2seq_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/bofenghuang/vigogne,448,3733bb83b49c56ed6d15de440da4f17e,bofenghuang_vigogne/vigogne/vigogne/data/select_by_length.py,"model_name_or_path,padding_side=right,use_fast=False",French instruction-following and chat models
transformers.BertTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f2b7de2fa08faa41d834bdc193445e72,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/bertabs/convert_bertabs_original_pytorch_checkpoint.py,bert-base-uncased,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained,github.com/salesforce/CodeRL,436,29d49c29152ea72c42aae6c7191dd594,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/multiple-choice/run_swag.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,6cfd3103bba5d660dd2edde0ba3be68d,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/summarization/run_summarization.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,686fa300cbf31199effc153e32303586,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/data_all_in/map_subword_schema.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/andreamad8/FSB,119,46eea402393e6a3e72c691087f834f01,andreamad8_FSB/FSB/demo/app_parsing.py,"model_checkpoint,low_cpu_mem_usage=True;gpt2;model_checkpoint;model_checkpoint",The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,de40dc1af063f85de6f5fd5955f604ab,yangheng95_PyABSA/PyABSA/pyabsa/tasks/CodeDefectDetection/prediction/code_defect_detector.py,find_cwd_dir;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.BartForSequenceClassification.from_pretrained;transformers.tokenization_bart.BartTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,0ccee11dc0cde98815ec82f9220b81ff,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/data_systhesis/snowball/evaluator/models/adversarial_evaluator.py,config_name;config_name,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.LlamaForCausalLM.from_pretrained,github.com/chrisociepa/allamo,103,9453d532cc4c2a407a6f827248cdfee8,chrisociepa_allamo/allamo/export_to_hf.py,"tmp_model_path,torch_dtype=torch_dtype,low_cpu_mem_usage=True","Simple, hackable and fast implementation for training/finetuning medium-sized LLaMA-based models"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.CLIPImageProcessor.from_pretrained;transformers.CLIPVisionModel.from_pretrained.cuda;transformers.CLIPImageProcessor.from_pretrained,github.com/tabtoyou/KoLLaVA,133,7bc71da41afebfde027d90263b098acb,tabtoyou_KoLLaVA/KoLLaVA/llava/eval/model_vqa.py,"config;model_name;unknown,torch_dtype=unknown;;unknown,torch_dtype=unknown",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.PegasusTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,999d64ba065ef00000aa3eec2aa1116a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/pegasus/convert_pegasus_tf_to_pytorch.py,"sshleifer/pegasus,model_max_length=desired_max_model_length",Source code for Zalo AI 2021 submission
transformers.FSMTConfig.from_pretrained,github.com/salesforce/CodeRL,436,5fc5fc3c4aaf8a9fafce302337f31558,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py,pytorch_dump_folder_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.GPT2Tokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,9af6f3b4a0639344c6229e5bdfc422cb,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/codeparrot/scripts/bpe_training.py,unknown,Source code for Zalo AI 2021 submission
transformers.HubertConfig.from_pretrained;transformers.HubertForSequenceClassification.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,d40e4ddfd3189a9747a825cec1948b19,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py,"config_path;base_model_name,config=hf_congfig;base_model_name,return_attention_mask=True,do_normalize=False",Source code for Zalo AI 2021 submission
transformers.Wav2Vec2ForSequenceClassification.from_pretrained;transformers.Wav2Vec2ForAudioFrameClassification.from_pretrained;transformers.Wav2Vec2ForXVector.from_pretrained;transformers.Wav2Vec2Config.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,14283c20d6ee23636016afcf7e2de0ab,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py,"base_model_name,config=hf_config;base_model_name,config=hf_config;base_model_name,config=hf_config;config_path;base_model_name,return_attention_mask=True,do_normalize=False",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,3917a9981f8c3716405b7333df9e55a3,salesforce_CodeRL/CodeRL/transformers/examples/flax/question-answering/run_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown,seed=unknown,dtype=getattr",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoFeatureExtractor.from_pretrained,github.com/kangyeolk/Paint-by-Sketch,156,a8080425eea7fd5d3e2cb7c0a9606636,kangyeolk_Paint-by-Sketch/Paint-by-Sketch/scripts/folder_inference.py,safety_model_id,Stable Diffusion-based image manipulation method with a sketch and reference image
transformers.AutoModelForSeq2SeqLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,d186fee43a14ea60ac858742194d05b1,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/seq2seq-distillation/run_eval.py,device;model_name,Source code for Zalo AI 2021 submission
transformers.T5Tokenizer.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/clue-ai/ChatYuan,1859,3c012bc0887c6149bd78fbb6f9d95eed,clue-ai_ChatYuan/ChatYuan/distributed-training/train.py,model_params;model_params,ChatYuan: Large Language Model for Dialogue in Chinese and English
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,8aa7d533de1dc497765163520234055d,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/gradient_accumulation.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,d9a1cdc0ddc1be70ba2be352832ef52f,yangheng95_PyABSA/PyABSA/pyabsa/utils/text_utils/bpe_tokenizer.py,base_tokenizer;rna_bpe_tokenizer2,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.UniSpeechSatForSequenceClassification.from_pretrained;transformers.UniSpeechSatForAudioFrameClassification.from_pretrained;transformers.UniSpeechSatForXVector.from_pretrained;transformers.UniSpeechSatConfig.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,a656cc08ea7c97a45dac26b533a394f6,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/unispeech_sat/convert_unispeech_original_s3prl_checkpoint_to_pytorch.py,"base_model_name,config=hf_config;base_model_name,config=hf_config;base_model_name,config=hf_config;config_path;base_model_name,return_attention_mask=True,do_normalize=False",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,d0e48cde528f9c27b361fffc4fb97280,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/text-classification/run_glue.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.OwlViTConfig.from_pretrained;transformers.CLIPTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,b810f1bd00c34cab4589f4ac254a3e57,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/owlvit/convert_owlvit_original_flax_to_hf.py,"config_path;openai/clip-vit-base-patch32,pad_token=!,model_max_length=16","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.SEWConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,ba6beb990712bf24482cb5f17409d057,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py,config_path,Source code for Zalo AI 2021 submission
transformers.MarianTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,95066126112863a87e2e22e34a827fcf,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/marian/convert_marian_to_pytorch.py,str,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/aws-samples/aws-genai-llm-chatbot,564,e160e3bb710bf0c3a11f28c396eb1d5f,aws-samples_aws-genai-llm-chatbot/aws-genai-llm-chatbot/lib/semantic-search/embeddings-model/inference.py,embeddings_model_dir;embeddings_model_dir;cross_encoder_model_dir;cross_encoder_model_dir,"A modular and comprehensive solution to deploy a Multi-LLM and Multi-RAG powered chatbot (Amazon Bedrock, Anthropic, HuggingFace, OpenAI, AI21, Cohere) using AWS CDK on AWS"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,fb7eb3a6b6b45ce174e73813093c3b72,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/mm-imdb/run_mmimdb.py,"unknown;unknown,do_lower_case=unknown,cache_dir=unknown;unknown,config=transformer_config,cache_dir=unknown;unknown",Source code for Zalo AI 2021 submission
transformers.Wav2Vec2Processor.from_pretrained;transformers.HubertForCTC.from_pretrained;transformers.Wav2Vec2Tokenizer.from_pretrained;transformers.Wav2Vec2ForMaskedLM.from_pretrained;transformers.Wav2Vec2Processor.from_pretrained;transformers.HubertForCTC.from_pretrained,github.com/jim-schwoebel/allie,129,98815b36d3cef5ce5042e72b4b13f6f8,jim-schwoebel_allie/allie/features/audio_features/featurize.py,facebook/hubert-large-ls960-ft;facebook/hubert-large-ls960-ft;facebook/wav2vec2-base-960h;facebook/wav2vec2-base-960h;facebook/hubert-large-ls960-ft;facebook/hubert-large-ls960-ft,"🤖 An automated machine learning framework for audio, text, image, video, or .CSV files (50+ featurizers and 15+ model trainers). Python 3.6 required."
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained.to,github.com/songrise/AvatarCraft,157,2359e8d5fd2a78e86b5fb4d690a0f316,songrise_AvatarCraft/AvatarCraft/models/diffusion.py,"unknown,subfolder=tokenizer;unknown",[ICCV23] AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control
transformers.BertConfig.from_pretrained;transformers.BertConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,64ba832e249d1be1c66e5665a6b696bb,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/performer/run_mlm_performer.py,"unknown,cache_dir=unknown;unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown",Source code for Zalo AI 2021 submission
transformers.BertTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,2fcf39ef191c8d5b9730508a9800013c,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/vilt/convert_vilt_original_to_pytorch.py,bert-base-uncased,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertTokenizer.from_pretrained,github.com/china-ai-law-challenge/CAIL2020,150,3cd63a4d68bd51ef601dec7ae9f2e3d0,china-ai-law-challenge_CAIL2020/CAIL2020/ydlj/baseline/data_process.py,unknown,
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/wzhouad/ATLOP,174,7c9006b7719b45e2bf3d6c3000ce8946,wzhouad_ATLOP/ATLOP/train.py,"unknown,num_labels=unknown;unknown;unknown,from_tf=bool,config=config","Source code for paper ""Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling"", AAAI 2021"
transformers.Wav2Vec2ForPreTraining.from_pretrained;transformers.ViTMAEForPreTraining.from_pretrained,github.com/salesforce/CodeRL,436,f620091018ba1e59972e7b3505e895c3,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/test_examples.py,tmp_dir;tmp_dir,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelWithLMHead.from_pretrained,github.com/allenai/real-toxicity-prompts,125,0feeefccb1a282decf61672d84694ecc,allenai_real-toxicity-prompts/real-toxicity-prompts/scripts/evaluation/run_language_modeling_webtext.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",
transformers.BertTokenizerFast.from_pretrained,github.com/StanleyLsx/entity_extractor_by_pointer,123,833d8d587e54920aec3af591dd66d406,StanleyLsx_entity_extractor_by_pointer/entity_extractor_by_pointer/engines/data.py,bert-base-chinese,使用torch整合两种经典的指针NER抽取范式，分别是SpanBert和苏神的GlobalPointer，简单加了些tricks，配置后一键运行
transformers.BertTokenizer.from_pretrained;transformers.BertConfig.from_pretrained;transformers.BertTokenizer.from_pretrained;transformers.BertConfig.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,7f4a383307ec0cb810d5ef48fc26f2fa,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/flow_based_dialog/main.py,unknown;unknown;unknown;unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.Wav2Vec2Config.from_pretrained;transformers.MBartConfig.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,4293e3c000914538282e412c9815817e,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/speech_encoder_decoder/convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py,"encoder_config_path,add_adapter=True,adapter_stride=adapter_stride,adapter_kernel_size=adapter_kernel_size,use_auth_token=True,output_hidden_size=encoder_output_dim;decoder_config_path;encoder_config_path,use_auth_token=True",Source code for Zalo AI 2021 submission
transformers.GPT2TokenizerFast.from_pretrained,github.com/EleutherAI/gpt-neox,6196,3f8d5a765696b706c3449e2f9595722d,EleutherAI_gpt-neox/gpt-neox/megatron/tokenizer/tokenizer.py,vocab_file,"An implementation of model parallel autoregressive transformers on GPUs, based on the DeepSpeed library."
transformers.AutoFeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,3d889fe75913f04a159cf01227b59ff8,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/regnet/convert_regnet_to_pytorch.py,"facebook/convnext-base-224-22k-1k,size=size","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ea4285d6d035a3cbb1c64306bbc062f5,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/tkk/utils/processor/table_truncate.py,pretrained_model_name_or_path=facebook/bart-large,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertConfig.from_pretrained;transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/alirezazareian/ovr-cnn,195,4c5cb2366864cb05e5e0f407ede46589,alirezazareian_ovr-cnn/ovr-cnn/maskrcnn_benchmark/modeling/language_backbone/transformers.py,"bert-base-uncased;bert-base-uncased;bert-base-uncased,config=unknown","A new framework for open-vocabulary object detection, based on maskrcnn-benchmark"
transformers.AutoConfig.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/thunlp/OpenBackdoor,114,fc676ef39844fde2671ad1af324b51a0,thunlp_OpenBackdoor/OpenBackdoor/openbackdoor/victims/mlms.py,"path;path,config=unknown;path","An open-source toolkit for textual backdoor attack and defense (NeurIPS 2022 D&B, Spotlight)"
transformers.RagConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,f80bdf4338f40e82f531c33ea7f86270,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag/consolidate_rag_checkpoint.py,config_name_or_path;generator_name_or_path;question_encoder_name_or_path;generator_tokenizer_name_or_path;question_encoder_tokenizer_name_or_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,aa4163fe40293d05acade028affd9690,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectTermExtraction/prediction/aspect_extractor.py,"find_cwd_dir;unknown;find_cwd_dir,do_lower_case=unknown;unknown,do_lower_case=unknown","Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/RUCAIBox/UniSRec,129,56b551802344e3f566873bcca5ffd7f5,RUCAIBox_UniSRec/UniSRec/dataset/preprocessing/utils.py,model_name;model_name,"[KDD'22] Official PyTorch implementation for ""Towards Universal Sequence Representation Learning for Recommender Systems""."
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.FlaxAutoModelForImageClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f01d058548abfc20d03f524e0af1b8f8,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/flax/vision/run_image_classification.py,"unknown,num_labels=len,image_size=unknown,cache_dir=unknown;unknown,num_labels=len,image_size=unknown,cache_dir=unknown;unknown,config=config,seed=unknown,dtype=getattr",Source code for Zalo AI 2021 submission
transformers.LlamaForCausalLM.from_pretrained,github.com/bofenghuang/vigogne,448,5ed934302945bba45dc836cc5f3d7424,bofenghuang_vigogne/vigogne/scripts/export_state_dict_checkpoint.py,"base_model_name_or_path,load_in_8bit=False,torch_dtype=unknown,device_map=Dict",French instruction-following and chat models
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,c78f11618544667242a415edd5556ced,ylsung_VL_adapter/VL_adapter/VL-T5/src/video/tvqa_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,e4d647a8c3669158e2fa8c309d4b0a4e,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_pad.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModelForTokenClassification.from_pretrained.to,github.com/bminixhofer/wtpsplit,422,91685b4032558b2cacc2895c9bd4976a,bminixhofer_wtpsplit/wtpsplit/wtpsplit/evaluation/punct_annotation_wtp.py,unknown,Code for Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation
transformers.GPT2Tokenizer.from_pretrained,github.com/awslabs/pptod,148,8b311fb3b6ef6e308bf51d1d9143fcaa,awslabs_pptod/pptod/data/multiwoz/utlis/reader.py,gpt_path,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.SEWConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,69e9c3cf7bd366a2b57fcba72708daeb,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.GPT2Tokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,1fbbfc2abec1faa7263604fc14bd2e67,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/pcll/lltrain.py,gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoFeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,da1de2bbbf05b9e1ddec6fa5eb0af35d,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/robust-speech-event/eval.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/dpfried/incoder,277,bf6c37cfd21d65b8eef0bee92780c172,dpfried_incoder/incoder/example_batched_usage.py,"model_name,None=kwargs;model_name",Generative model for code infilling and synthesis
transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,951855bae3a211c52e6629754a27b125,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py,tokenizer_model_name,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,5c660f55c1c895b97de37ff3c96bc51b,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/tkk/models/unified/adaptertuning.py,"unknown,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/salesforce/CodeRL,436,cd8d7722d19fa77fb7a0afd5b01fb458,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/language-modeling/run_clm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c5f273cc7161de8768145fd5b830f0f5,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/cyclekd.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModelForCausalLM.from_pretrained.cuda;transformers.AutoTokenizer.from_pretrained,github.com/kmeng01/memit,325,22cc23bbe4761bc0b9d841b312ae1114,kmeng01_memit/memit/experiments/evaluate.py,;model_name,Mass-editing thousands of facts into a transformer memory (ICLR 2023)
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForTokenClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,a12c8c685c48291f2a6477e677e5ebbd,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/token-classification/run_tf_ner.py,"unknown,num_labels=num_labels,id2label=label_map,label2id=Dict,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,from_pt=bool,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.MarianTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,557717148e6824804f54ff0e2ab04d4d,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/marian/convert_marian_to_pytorch.py,str,Source code for Zalo AI 2021 submission
transformers.T5Tokenizer.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/Guzpenha/transformer_rankers,155,440c1c7477a2638163acac23b5b12ef1,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/examples/t5_ranker.py,unknown;unknown,A library to conduct ranking experiments with transformers.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,81725b014d811e1faa18cfbc8be99603,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/multiple-choice/run_swag_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained.to;transformers.CLIPVisionModel.from_pretrained.to;transformers.CLIPVisionModel.from_pretrained.to;transformers.CLIPFeatureExtractor.from_pretrained,github.com/junshutang/Make-It-3D,1517,b8baad649af0270cc1db6b53287ea3ff,junshutang_Make-It-3D/Make-It-3D/nerf/sd.py,"model_key,subfolder=tokenizer;unknown;unknown;unknown;openai/clip-vit-large-patch14",[ICCV 2023] Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior
transformers.DeiTFeatureExtractor.from_pretrained;transformers.DeiTForImageClassification.from_pretrained,github.com/qanastek/HugsVision,185,37b3894fd2de4227c74a4f1ed682ec79,qanastek_HugsVision/HugsVision/recipes/HAM10000/binary_classification/gradio/app.py,model_path;model_path,HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,e918968ccb0e1404194cabb7e8057d8a,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/ablationknowledge.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,0ff9ae0434cc70d606df1e97e5f37a9f,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/gen_seldev.py,./bert-base-uncased,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizer.from_pretrained,github.com/fastnlp/CPT,448,97c3fb77ba5dd1a65928f0c5beff27d6,fastnlp_CPT/CPT/finetune/generation/run_gen.py,unknown,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.BigBirdTokenizerFast.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,e67f1edd48e2196a8fd865c94b82849b,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/jax-projects/big_bird/evaluate.py,model_id,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,000304078fd32e7c794b09502dda172e,salesforce_CodeRL/CodeRL/transformers/examples/flax/summarization/run_summarization_flax.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,config=config,seed=unknown,dtype=getattr",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.half.cuda,github.com/guifaChild/text_to_vedio,158,c9816b3d0e7ce9ea86b2681643d1b5b9,guifaChild_text_to_vedio/text_to_vedio/ChatGLM-6B-main/web_demo2.py,"THUDM/chatglm-6b,trust_remote_code=True;",这是一个由文本直接生成视频的项目
transformers.BertTokenizer.from_pretrained,github.com/phellonchen/X-LLM,241,290992d347af070b418d9b1d879b2d44,phellonchen_X-LLM/X-LLM/xllm/models/xllm_models/xllm_base.py,bert-base-uncased,X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,fbfa7f95bede5e5dce0bb9e44ae55bcf,salesforce_CodeRL/CodeRL/transformers/examples/legacy/pytorch-lightning/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.OpenAIGPTTokenizer.from_pretrained;transformers.OpenAIGPTDoubleHeadsModel.from_pretrained;transformers.OpenAIGPTDoubleHeadsModel.from_pretrained;transformers.OpenAIGPTTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,0598d962950c190b19f9acdebc565839,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/run_openai_gpt.py,unknown;unknown;unknown;unknown,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/ai-forever/Kandinsky-2,2534,bda722a7fa85dcc248f5cc9ee82f973c,ai-forever_Kandinsky-2/Kandinsky-2/kandinsky2/kandinsky2_1_model.py,unknown,Kandinsky 2 — multilingual text2image latent diffusion model
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,26e87c29c3d73b7136a6cc9a7bfb4d89,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/question-answering/run_qa_no_trainer.py,"unknown;unknown;unknown,use_fast=True;unknown,use_fast=True;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,c4f6fa94ec86bfad9b3b25b1954bc851,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/codeparrot/scripts/codeparrot_training.py,unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.Pipeline.from_pretrained,github.com/gitmylo/audio-webui,657,9f03298f7712106b66afd1d7fa1a9d6e,gitmylo_audio-webui/audio-webui/webui/modules/models.py,"task=unknown,model=path",A webui for different audio related Neural Networks
transformers.AutoTokenizer.from_pretrained,github.com/VinAIResearch/XPhoneBERT,237,9ce2fc681729b28655fe70c81a6737c4,VinAIResearch_XPhoneBERT/XPhoneBERT/VITS_with_XPhoneBERT/train.py,unknown,XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech (INTERSPEECH 2023)
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,0b70472eb30260d3edad6eee045df5e0,ylsung_VL_adapter/VL_adapter/VL-T5/src/video/how2qa_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,a32defedb260b85aaad3c3e76cc8f749,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/codeparrot/scripts/bpe_training.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.ResNetConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,7e0f49e056de38593fb8a4d506270de5,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/detr/convert_detr_to_pytorch.py,microsoft/resnet-101,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.HubertConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,c4cdb7e95b848b59e5c8dad82ef2fbde,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/hubert/convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.SpeechT5HifiGanConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,f4eec99d78cc6d3f3bd1beede00e7180,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/speecht5/convert_hifigan.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,386d9d9e78eee2fe4a649e2149a792a9,ylsung_VL_adapter/VL_adapter/VL-T5/src/modeling_t5.py,t5-base;t5-base,"PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.BigBirdTokenizerFast.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,ef754d0da6c0f0ad768d344ed6558cfb,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/jax-projects/big_bird/train.py,unknown,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained.save_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,7766460c985c2c00c8d08f79b51840d7,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/seq2seq/save_randomly_initialized_model.py,"config_name,None=config_kwargs;save_dir",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,701e33c40dcb5f945ba695f81c9ca28e,yangheng95_PyABSA/PyABSA/pyabsa/tasks/CodeDefectDetection/models/__plm__/bert.py,unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.LlamaTokenizer.from_pretrained,github.com/chrisociepa/allamo,103,e748e5466fb3760d2c42b388d8070f84,chrisociepa_allamo/allamo/sample.py,llama_tokenizer_path,"Simple, hackable and fast implementation for training/finetuning medium-sized LLaMA-based models"
transformers.AutoFeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,2604fd6c58abbc79f93c4d7996b1eb41,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/van/convert_van_to_pytorch.py,facebook/convnext-base-224-22k-1k,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/zetaalphavector/inPars,138,86f8520f6af010f19c352f853cf19455,zetaalphavector_inPars/inPars/inpars/train.py,"unknown;unknown;unknown;unknown,config=config",Inquisitive Parrots for Search
transformers.BigBirdTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,fdb6321cd8f4bd25a14c4ed177a10110,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/jax-projects/big_bird/prepare_natural_questions.py,google/bigbird-roberta-base,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/salesforce/CodeRL,436,d85c54dd5b1853b00598bd1782ed1aa5,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/codeparrot/scripts/human_eval.py,unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,5505a266bd223e22411d6fd08c25b2a0,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/proton/preprocess/test_prob.py,device;os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertForMaskedLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,f2734b6446044b6e53a3baf2316472b4,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/space-3/trippy/bert_models.py,model_name_or_path,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.SpeechT5Config.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,8452be6c607abe89609d32659cb10579,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/speecht5/convert_speecht5_original_pytorch_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,243983ad18c96f2dea4b2c088b45e758,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/seq2seq-distillation/convert_pl_checkpoint_to_hf.py,hf_src_model_dir;hf_src_model_dir,Source code for Zalo AI 2021 submission
transformers.T5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.ByT5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/Mikubill/naifu-diffusion,222,8130d263a321b55010247bebcf249105,Mikubill_naifu-diffusion/naifu-diffusion/lib/sgm/encoder.py,version;version;version;version;version;version,Train stable diffusion model with Diffusers and Pytorch Lightning
transformers.BertModel.from_pretrained;transformers.BertModel.from_pretrained,github.com/facebookresearch/ParlAI,10365,cd2c0dac70c2bb53711261a3c4119bcb,facebookresearch_ParlAI/ParlAI/parlai/agents/rag/conversion_utils.py,bert-base-uncased;model_path,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.CLIPVisionModel.from_pretrained;transformers.CLIPProcessor.from_pretrained,github.com/johannakarras/DreamPose,748,644f2cee00447c8b93b4351d99f80996,johannakarras_DreamPose/DreamPose/finetune-vae.py,openai/clip-vit-base-patch32;openai/clip-vit-base-patch32,"Official implementation of ""DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"""
transformers.RobertaTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,faf9d42b5b705cea3061f85d292583c7,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/r2sql/cosql/reranker/predict.py,"roberta-base,max_len=512",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.CLIPProcessor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,1057d160f0b9732a8b919273a38e4b68,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/groupvit/convert_groupvit_nvlab_to_hf.py,openai/clip-vit-base-patch32,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.FSMTTokenizer.from_pretrained;transformers.FSMTConfig.from_pretrained,github.com/salesforce/CodeRL,436,f365c2a1b41c84baae039122e6e67684,salesforce_CodeRL/CodeRL/transformers/scripts/fsmt/fsmt-make-tiny-model.py,mname;mname,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,92b6223cd9e5bafa90903d14c05a5dce,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectPolarityClassification/instructor/ensembler.py,"find_cwd_dir,do_lower_case=unknown;find_cwd_dir;unknown,do_lower_case=unknown;unknown;unknown","Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,eaef6b06840e35add0cb358af45c1244,yangheng95_PyABSA/PyABSA/pyabsa/utils/text_utils/mlm.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.TFGPT2LMHeadModel.from_pretrained,github.com/rinnakk/japanese-pretrained-models,553,613d97672f5e9da3194d9adf257c3c96,rinnakk_japanese-pretrained-models/japanese-pretrained-models/src/task/pretrain_gpt2/checkpoint2huggingface.py,"unknown,from_pt=True","Code for producing Japanese pretrained models provided by rinna Co., Ltd."
transformers.AutoModel.from_pretrained.half.cuda;transformers.AutoModel.from_pretrained.half,github.com/guifaChild/text_to_vedio,158,7b4986b087ce7675162ef156ad8e3217,guifaChild_text_to_vedio/text_to_vedio/ChatGLM-6B-main/utils.py,;,这是一个由文本直接生成视频的项目
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,a6bd0fcc43d2136f2dcfb68b8422a7f3,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/codeparrot/scripts/codeparrot_training.py,unknown;unknown,Source code for Zalo AI 2021 submission
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,34af3286b585b3cdab54fea50e293907,naver_gdc/gdc/dpg/examples/run.py,config,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,9297d6ac11430089557c4007efe8e255,yangheng95_PyABSA/PyABSA/pyabsa/tasks/_Archive/ProteinRegression/prediction/protein_regressor.py,find_cwd_dir;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.T5Model.from_pretrained;transformers.T5Tokenizer.from_pretrained,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,a4488d5b48e853c9f7fecffd879e736e,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/sentence_transformers/models/T5.py,"model_name_or_path,None=model_args;model_name_or_path,None=tokenizer_args",NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
transformers.T5Config.from_pretrained,github.com/awslabs/pptod,148,fe4af9f81697a016b8fb5fdb85bdab4b,awslabs_pptod/pptod/E2E_TOD/dataclass.py,model_name,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.RagTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,476357dd6e761cba28ac50f5571affea,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag/finetune_rag.py,unknown;unknown,Source code for Zalo AI 2021 submission
transformers.XLNetConfig.from_pretrained;transformers.XLNetTokenizerFast.from_pretrained;transformers.XLNetForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,e029288571f42a9d8ac53cb6fc415434,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py,"unknown;unknown;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.GPT2Tokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,7c6600c2bc4dd98d70f8fae6032a7776,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/pcll/dataset.py,gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,843df074c7ea46b9dbb784146f611141,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/summarization/run_summarization_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,cfa664ea06f197a1a03549f3c8c394cd,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/retrieval/models/retriever.py,unknown;unknown;unknown;unknown;unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoTokenizer.from_pretrained,github.com/ypwhs/CreativeChatGLM,203,ea30bbb6be1a9e15cfd2dbea8149176a,ypwhs_CreativeChatGLM/CreativeChatGLM/predictors/llama_gptq.py,"model_name,resume_download=True",👋 欢迎来到 ChatGLM 创意世界！你可以使用修订和续写的功能来生成创意内容！
transformers.LlamaForCausalLM.from_pretrained,github.com/danielgross/LlamaAcademy,1206,93f11f87e75fa1881519c7c3e0d6e669,danielgross_LlamaAcademy/LlamaAcademy/export_hf.py,"unknown,load_in_8bit=False,torch_dtype=unknown,device_map=Dict",A school for camelids
transformers.RobertaTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,16aacaaf78715162c5d072835d7cc8ca,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/vision_encoder_decoder/convert_trocr_unilm_to_pytorch.py,roberta-large,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c5219cf867e23bcb4ae5619bf51504c6,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/data_all_in/preprocess/common_utils_proton.py,unknown;google/electra-large-discriminator,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.MBartConfig.from_pretrained,github.com/songhaoyu/BoB,134,832c2581d4cea28630de38e858442374,songhaoyu_BoB/BoB/xlibs/convert_mbart_original_checkpoint_to_pytorch.py,"hf_config_path,vocab_size=vocab_size",The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.T5Config.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/awslabs/pptod,148,3e9b6644ffb0e4d94e4cfb49d71cb26e,awslabs_pptod/pptod/DST/modelling/T5Model.py,"model_path;model_path,config=t5_config,resume_download=True;model_path",Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,ff18d1eeebd7109e3a5076e31aea7fdc,ylsung_VL_adapter/VL_adapter/VL-T5/src/classification_clip_data.py,"unknown,max_length=unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,5799e879aa1e8e764f6c6fa0e1183c6a,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/retrieval/models/unified_retriever.py,unknown;unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained.to,github.com/thunlp/KernelGAT,166,cc887543fd47ce00b01fbaea3308fd00,thunlp_KernelGAT/KernelGAT/scikgat/training/rationale_selection_scifact_train.py,unknown;device,The source codes for Fine-grained Fact Verification with Kernel Graph Attention Network.
transformers.BertTokenizerFast.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/cross-domain-compositing/cross-domain-compositing,167,595fdf8ec3443c28676ec9c15f56fc42,cross-domain-compositing_cross-domain-compositing/cross-domain-compositing/ldm/modules/encoders/modules.py,bert-base-uncased;version;version,
transformers.DPRContextEncoderTokenizerFast.from_pretrained;transformers.DPRContextEncoder.from_pretrained;transformers.RagTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.DPRConfig.from_pretrained;transformers.DPRContextEncoderTokenizerFast.from_pretrained,github.com/salesforce/CodeRL,436,e597506e94da14368897265e3b6e2303,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag-end2end-retriever/finetune_rag.py,facebook/dpr-ctx_encoder-multiset-base;unknown;unknown;unknown;unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/csinva/gpt-paper-title-generator,132,b9080769307b08513d3cd963097d5f2e,csinva_gpt-paper-title-generator/gpt-paper-title-generator/gptneo/04_generate_samples.py,checkpoint;EleutherAI/gpt-neo-2.7B,Generating paper titles (and more!) with GPT trained on data scraped from arXiv.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,3308a8274b28bdf1b370897d4fa3042f,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/automatic_gradient_accumulation.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,0a443310932b924ef937d6894a70a232,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/deepspeed_with_config_support.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,459aaeefdb89961396670ba1f3db5789,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/proton/preprocess/cu.py,unknown;os.path.join;device;os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizer.from_pretrained;transformers.BertTokenizer.from_pretrained;transformers.BertTokenizer.from_pretrained;transformers.BertTokenizer.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,1cc4749be86902ac60a6e90a9387c589,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/pace/pace/datamodules/datamodule_base.py,"from_pretrained,truncation_side=left,do_lower_case=unknown,never_split=special_tokens;from_pretrained,truncation_side=left,do_lower_case=unknown;from_pretrained,truncation_side=left,do_lower_case=unknown,never_split=special_tokens;from_pretrained,truncation_side=left,do_lower_case=unknown;from_pretrained,truncation_side=left,do_lower_case=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.ElectraConfig.from_pretrained;transformers.ElectraConfig.from_pretrained;transformers.ElectraTokenizerFast.from_pretrained;transformers.ElectraForMaskedLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,87ea222429c559dcb912ec93789d5356,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/pretrain/pretrain_inbatch.py,unknown;unknown;unknown;unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.DeiTFeatureExtractor.from_pretrained;transformers.DeiTForImageClassification.from_pretrained,github.com/qanastek/HugsVision,185,477bcbe2cd139f27e802f9cad20bee2f,qanastek_HugsVision/HugsVision/recipes/kvasir_v2/binary_classification/predict_deit.py,unknown;unknown,HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.WavLMConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,49ef6bb56af09a5ee20fb985b556e7ae,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/wavlm/convert_wavlm_original_pytorch_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoModelWithLMHead.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ucinlp/autoprompt,497,98ae0b7c678b005a208a445bacd67132,ucinlp_autoprompt/autoprompt/autoprompt/label_search.py,"model_name;model_name,config=config;model_name",AutoPrompt: Automatic Prompt Construction for Masked Language Models.
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/phellonchen/X-LLM,241,dcccd8822ae2603b0a36dc7b1de02e25,phellonchen_X-LLM/X-LLM/xllm/models/xllm_models/xllm_speech.py,bert-base-chinese;bert-base-chinese,X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages
transformers.BertTokenizer.from_pretrained;transformers.BertForQuestionAnswering.from_pretrained.to,github.com/stanfordnlp/chirpycardinal,125,a32dcc7be249b307f73b9f934586012c,stanfordnlp_chirpycardinal/chirpycardinal/docker/qa/app/remote_module.py,bert-base-uncased;cuda:0,Stanford's Alexa Prize socialbot
transformers.BertModel.from_pretrained,github.com/declare-lab/conv-emotion,1210,592dfca58f0b812908f646259d89f3bc,declare-lab_conv-emotion/conv-emotion/emotion-cause-extraction/Rank-Emotion-Cause/src/networks/rank_cp.py,unknown,This repo contains implementation of different architectures for emotion recognition in conversations.
transformers.models.roberta.tokenization_roberta.RobertaTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,27d690a12e5d9213ab3a62bdb034824c,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/trocr/processing_trocr.py,"pretrained_model_name_or_path,None=kwargs",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,3cebcd693c4fdb51fe58f8da947fa563,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dial2vec/network.py,huggingface_mapper;huggingface_mapper,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.HubertConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,2ca0d841b440a3c9c11bc5c97bece6f3,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,1ff63d5ff3579bbcbd78562e14cb40b9,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/nlp_example.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.CamembertTokenizer.from_pretrained;transformers.CamembertForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,984a815a7a8736101c89c14570f50555,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/run_camembert.py,camembert-base;camembert-base,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,0586f18e02a90d60d4fae0536903f4d5,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/seq2seq-distillation/_test_seq2seq_examples.py,m;BART_TINY,Source code for Zalo AI 2021 submission
transformers.AutoModelForTokenClassification.from_pretrained.to,github.com/bminixhofer/wtpsplit,422,99f3cc13497eaaf5b1c6183c1ff28630,bminixhofer_wtpsplit/wtpsplit/wtpsplit/evaluation/intrinsic.py,unknown,Code for Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation
transformers.BartForConditionalGeneration.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,75826ced2fde322368f29a43efaa2709,ylsung_VL_adapter/VL_adapter/VL-T5/src/my_transformers/modeling_bart.py,facebook/bart-base;facebook/bart-base,"PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForTokenClassification.from_pretrained,github.com/salesforce/CodeRL,436,29658a2517b66973b8f6616ab9d2cdfd,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/token-classification/run_ner_no_trainer.py,"unknown,num_labels=num_labels;unknown,num_labels=num_labels;tokenizer_name_or_path,use_fast=True,add_prefix_space=True;tokenizer_name_or_path,use_fast=True;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.SwitchTransformersConfig.from_pretrained;transformers.SwitchTransformersForConditionalGeneration.from_pretrained;transformers.T5Tokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,2ac80aea22cb539e45f9fdc72c013fbb,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/switch_transformers/convert_big_switch.py,"google/switch-base-8;/home/arthur_huggingface_co/transformers/switch_converted,device_map=auto;t5-small","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertForMaskedLM.from_pretrained,github.com/salesforce/CodeRL,436,57a790759b7a8b1de616e47f57bf153e,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/distillation/scripts/extract_distilbert.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BartTokenizer.from_pretrained;transformers.BartConfig.from_pretrained;transformers.BartForConditionalGeneration.from_pretrained,github.com/RUC-GSAI/YuLan-IR,181,d20c865f927d4116c72233aa7529da2b,RUC-GSAI_YuLan-IR/YuLan-IR/WebBrain/generator/run_model_train.py,"unknown;unknown;unknown,config=config",YuLan-IR: Information Retrieval Boosted LMs
transformers.Wav2Vec2Processor.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained,github.com/jonatasgrosman/wav2vec2-sprint,143,edcd21efc44bc0f1a2dee9f2ce5f278e,jonatasgrosman_wav2vec2-sprint/wav2vec2-sprint/run_common_voice.py,"unknown;unknown,cache_dir=unknown,activation_dropout=unknown,attention_dropout=unknown,hidden_dropout=unknown,feat_proj_dropout=unknown,mask_time_prob=unknown,gradient_checkpointing=unknown,layerdrop=unknown,ctc_loss_reduction=mean,pad_token_id=unknown,vocab_size=len,ctc_zero_infinity=True",
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/aws-samples/aws-genai-llm-chatbot,564,097a7c4b884826644dd7a0326d221d9c,aws-samples_aws-genai-llm-chatbot/aws-genai-llm-chatbot/lib/large-language-model/hf-custom-script-model/samples/pipeline/inference.py,"model_dir;model_dir,device_map=auto,torch_dtype=unknown,load_in_8bit=True","A modular and comprehensive solution to deploy a Multi-LLM and Multi-RAG powered chatbot (Amazon Bedrock, Anthropic, HuggingFace, OpenAI, AI21, Cohere) using AWS CDK on AWS"
transformers.TFConvBertModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,a534551ae9e27e356b445774e35e2e40,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py,"pytorch_dump_path,from_pt=True",Source code for Zalo AI 2021 submission
transformers.RobertaTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,77314532991d2fe8e4ce16b4573508e4,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/r2sql/cosql/reranker/dataset.py,"roberta-base,max_len=128",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,6a42bf9df1d93db62abeccefb617b33c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/pace/pace/utils/write_simmc2_dst.py,"bert-base-uncased,do_lower_case=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizer.from_pretrained;transformers.BartConfig.from_pretrained;transformers.BartConfig.from_pretrained,github.com/fastnlp/CPT,448,5ac501636bc31cc4cc5c232331106b91,fastnlp_CPT/CPT/finetune/ner/train_msra.py,hfl/chinese-bert-wwm;ptm;ptm,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,d0f10827d18bdd4d28b0b90778ce0506,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/translation/run_translation.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoFeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,dc70a15009cf6f8fd3cbd1e0a35e9136,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/clap/convert_clap_original_pytorch_to_hf.py,"laion/clap-htsat-unfused,truncation=rand_trunc","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,fec0f8c412e93235fd525d7acd8e240e,yangheng95_PyABSA/PyABSA/pyabsa/tasks/TextAdversarialDefense/prediction/tad_classifier.py,find_cwd_dir;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,b3351874162d4fe4d9961782a681cfe0,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/flax/question-answering/run_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown,seed=unknown,dtype=getattr",Source code for Zalo AI 2021 submission
transformers.WhisperForConditionalGeneration.from_pretrained;transformers.WhisperProcessor.from_pretrained,github.com/sanchit-gandhi/whisper-jax,3647,ca2be3caf30db3a3176d69a6b650cc22,sanchit-gandhi_whisper-jax/whisper-jax/benchmarks/run_pytorch.py,openai/whisper-large-v2;openai/whisper-tiny.en,JAX implementation of OpenAI's Whisper model for up to 70x speed-up on TPU.
transformers.AutoTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,1e765410d5c34e23765ba72cdbe506be,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/git/convert_git_to_pytorch.py,"bert-base-uncased,model_input_names=List","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertTokenizer.from_pretrained;transformers.BertForSequenceClassification.from_pretrained,github.com/Guzpenha/transformer_rankers,155,dd27154a85d9a1ead921903fe6998bad,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/examples/bert_ranker_cross_task_cross_ns_for_crr.py,unknown;unknown,A library to conduct ranking experiments with transformers.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,d624f7fc32d77b3c673ec183fd1467cc,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/eval/eval_mhop_retrieval.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.DetrFeatureExtractor.from_pretrained,github.com/qanastek/HugsVision,185,bd8c1314c2ba2dab1eab38620226a59e,qanastek_HugsVision/HugsVision/hugsvision/nnet/ObjectDetectionTrainer.py,unknown,HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,a26e9dc6a2596c98c732e2d14bbfed51,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/language-modeling/run_clm_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.BertConfig.from_pretrained;transformers.BertConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,1bc8d0c488d9429bafc91cc093369668,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/performer/run_mlm_performer.py,"unknown,cache_dir=unknown;unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.models.auto.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,c932c2166f99e8f2150d6ce64100bdb6,salesforce_CodeRL/CodeRL/transformers/src/transformers/onnx/__main__.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,b4714632e0d06fb8cc6128f5de6103b3,yangheng95_PyABSA/PyABSA/pyabsa/utils/absa_utils/absa_utils.py,microsoft/deberta-v3-base,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.BertTokenizerFast.from_pretrained;transformers.T5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained;transformers.CLIPVisionModel.from_pretrained,github.com/ashawkey/stable-dreamfusion,7150,b2391e69e89ff78089892ab3107794e3,ashawkey_stable-dreamfusion/stable-dreamfusion/ldm/modules/encoders/modules.py,bert-base-uncased;version;version;version;version;version,Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion.
transformers.MBartConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,8d7ecceeb65dce5dfab56e635f002fc3,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py,"hf_config_path,vocab_size=vocab_size",Source code for Zalo AI 2021 submission
transformers.BertConfig.from_pretrained;transformers.BertModel.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,d153b73df9ad3a8e360351e2b0d5b601,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/sentence_transformers/models/Transformer.py,"model_name_or_path,None=model_args,cache_dir=cache_dir;model_name_or_path,config=config,cache_dir=cache_dir;unknown,cache_dir=cache_dir,None=tokenizer_args",NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
transformers.BertTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,245c47356af44773a18573183f511a59,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/bertabs/run_summarization.py,"bert-base-uncased,do_lower_case=True",Source code for Zalo AI 2021 submission
transformers.CLIPConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,94f81c45e6faaf839f3a107143120486,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py,config_path,Source code for Zalo AI 2021 submission
transformers.modeling_bert.BertModel.from_pretrained,github.com/v-mipeng/LexiconAugmentedNER,418,c006c322ba963cbfd19ca3737c603983,v-mipeng_LexiconAugmentedNER/LexiconAugmentedNER/model/gazlstm.py,bert-base-chinese,Reject complicated operations for incorporating lexicon for Chinese NER. 
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,e0ddddc9ecc23a115c90d37086fdccdd,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/flax/text-classification/run_flax_glue.py,"unknown,num_labels=num_labels,finetuning_task=unknown;unknown,use_fast=unknown;unknown,config=config",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/declare-lab/conv-emotion,1210,15df31f6a81cfae6f9c7cad492673453,declare-lab_conv-emotion/conv-emotion/emotion-cause-extraction/RoBERTa Baseline/simpletransformers/seq2seq/seq2seq_model.py,os.path.join;decoder_name,This repo contains implementation of different architectures for emotion recognition in conversations.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForCausalLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,439bd291e6d53eebca9c66d62b08bb76,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/jax-projects/model_parallel/run_clm_mp.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,seed=unknown,dtype=getattr",Source code for Zalo AI 2021 submission
transformers.BertTokenizer.from_pretrained;transformers.BartConfig.from_pretrained;transformers.BartConfig.from_pretrained,github.com/fastnlp/CPT,448,164623cbd41f3193d3379abb2bdf3c65,fastnlp_CPT/CPT/finetune/ner/train_cluener.py,hfl/chinese-bert-wwm;ptm;ptm,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,b024bc5da24b890d88f502ecc2db1e4d,salesforce_CodeRL/CodeRL/transformers/examples/legacy/run_swag.py,"unknown;unknown;unknown,from_tf=bool,config=config;unknown;unknown;checkpoint;checkpoint",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BigBirdTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,caf20c04e210daa2ef60f5d007fd4837,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/jax-projects/big_bird/prepare_natural_questions.py,google/bigbird-roberta-base,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,066d696a955f345aadef3a0415154b34,ylsung_VL_adapter/VL_adapter/VL-T5/src/gqa_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.DPRContextEncoderTokenizerFast.from_pretrained;transformers.DPRContextEncoder.from_pretrained;transformers.RagTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.DPRConfig.from_pretrained;transformers.DPRContextEncoderTokenizerFast.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,13b5529a3f00907427c088f154a44ebb,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag-end2end-retriever/finetune_rag.py,facebook/dpr-ctx_encoder-multiset-base;unknown;unknown;unknown;unknown;unknown,Source code for Zalo AI 2021 submission
transformers.AutoFeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,ed0eb53047b73f6088872faba5fd5ec4,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/cvt/convert_cvt_original_pytorch_checkpoint_to_pytorch.py,facebook/convnext-base-224-22k-1k,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,70c25efef0316ca6a4bca1d7508a9efa,ylsung_VL_adapter/VL_adapter/VL-T5/src/vqa_raw_data.py,"unknown,max_length=unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/zetaalphavector/inPars,138,3beef5eedce7fb624efd4a6904d7c847,zetaalphavector_inPars/inPars/legacy/inpars-v1/train_t5.py,unknown;t5-base,Inquisitive Parrots for Search
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/karthikv792/LLMs-Planning,112,b24745edd584c6be68245079b8bec96a,karthikv792_LLMs-Planning/LLMs-Planning/llm_planning_analysis/back_prompting.py,"bigscience/bloom;bigscience/bloom,cache_dir=/data/karthik/LLM_models/bloom/,local_files_only=False,load_in_8bit=True,device_map=auto,max_memory=max_memory_mapping",An extensible benchmark for evaluating large language models on planning
transformers.T5ForConditionalGeneration.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/facebookresearch/ParlAI,10365,92c51978c3f82be0c8605789ee2896b0,facebookresearch_ParlAI/ParlAI/parlai/agents/hugging_face/t5.py,"opt,dropout_rate=opt,torch_dtype=torch_dtype;opt,dropout_rate=opt",A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/ucinlp/autoprompt,497,a084f2725f527656c85c380caf97df5e,ucinlp_autoprompt/autoprompt/autoprompt/finetune.py,"unknown,num_labels=unknown;unknown;unknown,config=config",AutoPrompt: Automatic Prompt Construction for Masked Language Models.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,0021283936b13b96449541d557e8a5c7,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/seq2seq-distillation/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,bec041ab254bd5ca4919d5fbe38a0c6c,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/jax-projects/hybrid_clip/run_hybrid_clip.py,"unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.ElectraConfig.from_pretrained;transformers.ElectraConfig.from_pretrained;transformers.ElectraConfig.from_pretrained;transformers.ElectraConfig.from_pretrained;transformers.ElectraForMaskedLM.from_pretrained;transformers.ElectraForPreTraining.from_pretrained,github.com/declare-lab/conv-emotion,1210,18f6f119032c978d41ff66292adcc6b8,declare-lab_conv-emotion/conv-emotion/emotion-cause-extraction/RoBERTa Baseline/simpletransformers/language_modeling/language_modeling_model.py,"generator_name;os.path.join,None=kwargs;discriminator_name;os.path.join,None=kwargs;generator_name;discriminator_name",This repo contains implementation of different architectures for emotion recognition in conversations.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,64327a5dc5a1141536b1d53ce892f0e2,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_grapter_512_elu_res.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModelForCausalLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/Vahe1994/SpQR,466,c2968af46835ede7b754caa48026672f,Vahe1994_SpQR/SpQR/lm-evaluation-harness/lm_eval/models/gpt2.py,"unknown;unknown,revision=revision",
transformers.modeling_longformer.LongformerModel.from_pretrained;transformers.modeling_longformer.LongformerForQuestionAnswering.from_pretrained,github.com/songhaoyu/BoB,134,947ee79db73a35e3c3788014c93e42c3,songhaoyu_BoB/BoB/xlibs/convert_longformer_original_pytorch_lightning_to_pytorch.py,longformer_model;longformer_model,The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.DPRContextEncoder.from_pretrained.to;transformers.DPRContextEncoderTokenizerFast.from_pretrained;transformers.RagRetriever.from_pretrained;transformers.RagSequenceForGeneration.from_pretrained;transformers.RagTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,6bb40c8bb512ff7271d9f9e73712b6af,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag/use_own_knowledge_dataset.py,"device=device;unknown;unknown,index_name=custom,indexed_dataset=dataset;unknown,retriever=retriever;unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertModel.from_pretrained,github.com/StanleyLsx/entity_extractor_by_pointer,123,dacef497e382fe91a260e493e3edb844,StanleyLsx_entity_extractor_by_pointer/entity_extractor_by_pointer/engines/models/BinaryPointer.py,bert-base-chinese,使用torch整合两种经典的指针NER抽取范式，分别是SpanBert和苏神的GlobalPointer，简单加了些tricks，配置后一键运行
transformers.AutoModelForCausalLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/kmeng01/memit,325,39003f6b971e4eed3dc02d07b43f1e74,kmeng01_memit/memit/experiments/sweep.py,cuda;unknown,Mass-editing thousands of facts into a transformer memory (ICLR 2023)
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.to,github.com/salesforce/CodeRL,436,6d7d9f40fa086ee58151d0c99298bc2a,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/longform-qa/eli5_utils.py,model_name;device;model_name;device,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModel.from_pretrained,github.com/INK-USC/MHGRN,238,89d94251d4dfc03eb612d19e043bca95,INK-USC_MHGRN/MHGRN/modeling/modeling_encoder.py,"model_name,output_hidden_states=True",Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering (EMNLP 2020)
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/Yui010206/SeViLA,121,3545192aa86f10ade43970de9ba9678a,Yui010206_SeViLA/SeViLA/lavis/processors/gpt_processors.py,gpt2;gpt2,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.WhisperProcessor.from_pretrained;transformers.WhisperConfig.from_pretrained,github.com/sanchit-gandhi/whisper-jax,3647,37b3f9d94fea2a0d90aeb74222f83179,sanchit-gandhi_whisper-jax/whisper-jax/benchmarks/run_pjit.py,openai/whisper-tiny.en;unknown,JAX implementation of OpenAI's Whisper model for up to 70x speed-up on TPU.
transformers.AutoProcessor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained,github.com/sail-sg/EditAnything,2813,9144b2ca4d1e380a58494ee84daec57f,sail-sg_EditAnything/EditAnything/sam2semantic.py,"Salesforce/blip2-opt-2.7b;Salesforce/blip2-opt-2.7b,torch_dtype=data_type","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.AutoTokenizer.from_pretrained;transformers.GPT2Config.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,b9f53241cc77bd27ad7883b0d901a857,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py,tokenizer_name;unknown,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTokenizerFast.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,bbb24d4954724d8ea9b1dc0c4f6fa953,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/x_clip/convert_x_clip_original_pytorch_to_hf.py,openai/clip-vit-base-patch32;openai/clip-vit-base-patch32,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.GPT2LMHeadModel.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/allenai/real-toxicity-prompts,125,a06eb88b03ccb1a21601e713d499249b,allenai_real-toxicity-prompts/real-toxicity-prompts/generation/gpt2_generation.py,"str;tokenizer,pad_token=unknown",
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained.to,github.com/AlibabaResearch/DAMO-ConvAI,788,70c07b41b465e70946616ed4ff91686f,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/PRO/eval/metrics2.py,gpt2;model_name;model_device;model_name;model_device,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,43140e5bf3060e1e83c64c86e314fb91,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/text-classification/run_glue.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;model_path,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.BertModel.from_pretrained;transformers.RobertaModel.from_pretrained;transformers.BartModel.from_pretrained,github.com/zhvng/open-musiclm,398,342d2552fa736ffa9872e445b565ef27,zhvng_open-musiclm/open-musiclm/open_musiclm/laion_clap/clap_module/model.py,bert-base-uncased;roberta-base;facebook/bart-base,"Implementation of MusicLM, a text to music model published by Google Research, with a few modifications."
transformers.AutoConfig.from_pretrained,github.com/yangheng95/PyABSA,779,4afa1a671f242804ca65c1f0e6aed701,yangheng95_PyABSA/PyABSA/pyabsa/tasks/RNARegression/models/__classic__/mhsa.py,bert-base-uncased,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.BertModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f42af0dda0d8c8c89fe0af06011209e8,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py,"pretrained_model_name_or_path=unknown,state_dict=torch.load,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.T5Config.from_pretrained,github.com/awslabs/pptod,148,3b1473553738646188fbdab75d98a43d,awslabs_pptod/pptod/DST/dataclass.py,model_name,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c3f5e84275668dd637bd54b2b016481b,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/gloc/processor/table_truncate.py,pretrained_model_name_or_path=facebook/bart-large,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoProcessor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained.to,github.com/ashawkey/stable-dreamfusion,7150,89afb5b0c23abcdd9d06914d0fdf883e,ashawkey_stable-dreamfusion/stable-dreamfusion/preprocess_image.py,Salesforce/blip2-opt-2.7b;device,Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion.
transformers.SEWDConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,b74c924d3409248669c03d5af459fcd9,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,10eda341b1049b8972637c1046bca1a4,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/dataprocess.py,"THUDM/chatglm-6b,trust_remote_code=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,304212dc5af65ac7c141c4aa7cecf203,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/diana/downstream/dianawotask.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Tokenizer.from_pretrained,github.com/DavidHuji/CapDec,153,238b576cefb93d5df10c0f550b185cf1,DavidHuji_CapDec/CapDec/predictions_runner.py,gpt2,"CapDec: SOTA Zero Shot Image Captioning Using CLIP and GPT2, EMNLP 2022 (findings)"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,4d3eda334430d815ca052c4a544a6a08,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/experiments/clm_behavioral-cloning.py,unknown;unknown,We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,a41a826027da680daa8bbbb7ef37999c,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/text-classification/run_glue.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;model_path,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,197e03c25464057553cd8e4c8750e699,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/slot_based_dialog/cdial_gpt/train.py,unknown;unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.RobertaTokenizerFast.from_pretrained;transformers.RobertaModel.from_pretrained,github.com/ashkamath/mdetr,903,c93d818f46829e74532bba816e952166,ashkamath_mdetr/mdetr/models/transformer.py,text_encoder_type;text_encoder_type,
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,c3efed5845ed00e9000066a674cbecd6,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/codeparrot/scripts/validation_loss.py,unknown;unknown,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSpeechSeq2Seq.from_pretrained;transformers.AutoProcessor.from_pretrained,github.com/salesforce/CodeRL,436,7f072d59c52b75603ec897022ed8b96d,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.OpenAIGPTTokenizer.from_pretrained,github.com/snap-stanford/GreaseLM,200,d8c5d3942b7ebe1d5f03654fc1f26090,snap-stanford_GreaseLM/GreaseLM/utils/data_utils.py,openai-gpt,[ICLR 2022 spotlight]GreaseLM: Graph REASoning Enhanced Language Models for Question Answering
transformers.CLIPVisionModel.from_pretrained.cuda;transformers.CLIPProcessor.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTokenizer.from_pretrained,github.com/johannakarras/DreamPose,748,78effc1d9329e8593235760530927ade,johannakarras_DreamPose/DreamPose/train.py,";openai/clip-vit-base-patch32;unknown,revision=unknown;unknown,subfolder=tokenizer,revision=unknown","Official implementation of ""DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"""
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/loubnabnl/santacoder-finetuning,142,5d6e732bfecee02c5ab00acc0d67fb10,loubnabnl_santacoder-finetuning/santacoder-finetuning/train.py,"unknown,trust_remote_code=True,use_cache=unknown;unknown,use_auth_token=True",Fine-tune SantaCoder for Code/Text Generation.
transformers.AutoTokenizer.from_pretrained,github.com/siddk/voltron-robotics,147,581cbb3f9e541a1e5531aebc7e444309,siddk_voltron-robotics/voltron-robotics/voltron/preprocessing/v1/process.py,"language_model,cache_dir=hf_cache",Voltron: Language-Driven Representation Learning for Robotics
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c39689d4ed5b1f71c3cab1ef4cc8d316,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/LGESQL/cosql/utils/example.py,google/electra-large-discriminator,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.generation.GenerationConfig.from_pretrained,github.com/shibing624/MedicalGPT,1876,06db4dff2c70c0e2852c33ec62e45e68,shibing624_MedicalGPT/MedicalGPT/inference.py,unknown,MedicalGPT: Training Your Own Medical GPT Model with ChatGPT Training Pipeline. 训练医疗大模型，实现了包括增量预训练、有监督微调、RLHF(奖励建模、强化学习训练)和DPO(直接偏好优化)。
transformers.AutoModelForSeq2SeqLM.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,1d6d96aabb59fd5340bd5c67213e692b,salesforce_CodeRL/CodeRL/transformers/examples/legacy/seq2seq/run_eval.py,device;model_name,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.to,github.com/salesforce/CodeRL,436,fe823b2c2fbcb42c0f3fd18a63bc8667,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/longform-qa/eli5_app.py,yjernite/retribert-base-uncased;cuda:0;yjernite/bart_eli5;cuda:0,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,81b287d984f0dd0b95742b1a9a8b496a,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/text2sql/scripts/annotate_sql_program_tabfact.py,pretrained_model_name_or_path=os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizerFast.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/timothybrooks/instruct-pix2pix,5428,39877a60d9530ee719fe5c43c09792b3,timothybrooks_instruct-pix2pix/instruct-pix2pix/stable_diffusion/ldm/modules/encoders/modules.py,bert-base-uncased;version;version,
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/qiuhuachuan/smile,203,1748fffb34d3fc672bdd17caa9980fc9,qiuhuachuan_smile/smile/MeChat_server.py,"THUDM/chatglm-6b,revision=v0.1.0,trust_remote_code=True;THUDM/chatglm-6b,trust_remote_code=True",SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support
transformers.XLNetConfig.from_pretrained;transformers.XLNetTokenizerFast.from_pretrained;transformers.XLNetForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,ba84c036548d839bf52238155e870d98,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/question-answering/run_qa_beam_search.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5Tokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.TFAutoModelForCausalLM.from_pretrained,github.com/rinnakk/japanese-pretrained-models,553,13aeb047dda6db5a14910459ad715da9,rinnakk_japanese-pretrained-models/japanese-pretrained-models/src/task/pretrain_gpt2/check_huggingface.py,"unknown,extra_ids=0;unknown;unknown","Code for producing Japanese pretrained models provided by rinna Co., Ltd."
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,0dccd0dfe93bc33790e5f78405f3a2a8,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/train_momentum.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,428aef909a92162c25cb2b455ef4679f,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/multiple-choice/run_swag.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.TFConvBertModel.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,469a8b81d89a5d559fe5ee92345baef8,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py,"pytorch_dump_path,from_pt=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,bc6081e4cdbd3bb03e6768a1605527cd,yangheng95_PyABSA/PyABSA/pyabsa/framework/tokenizer_class/tokenizer_class.py,"unknown,None=kwargs","Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/ZeroRin/BertGCN,229,9dc33d0adb50e51f1615810fe59faa40,ZeroRin_BertGCN/BertGCN/model/models.py,pretrained_model;pretrained_model;pretrained_model;pretrained_model;pretrained_model;pretrained_model,
transformers.FlavaConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,477373d0ec331c13d7e3fe49a90d1bdf,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/flava/convert_flava_original_pytorch_to_hf.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.XLMRobertaTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,9acf627e09c0459dd87e89435514f6af,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py,"metadata;pytorch_dump_folder_path;pytorch_dump_folder_path,task=entity_classification;pytorch_dump_folder_path","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.SwinConfig.from_pretrained;transformers.SwinConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,701265fdb7a3f1da18a34934fc8a9aef,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/mask2former/convert_mask2former_original_pytorch_checkpoint_to_pytorch.py,"microsoft/swin-tiny-patch4-window7-224,out_features=List;microsoft/swin-large-patch4-window12-384,out_features=List","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2Tokenizer.from_pretrained;transformers.T5TokenizerFast.from_pretrained;transformers.LlamaTokenizerFast.from_pretrained;transformers.LlamaTokenizer.from_pretrained,github.com/facebookresearch/ParlAI,10365,1fb04e8e81d943c3b27d86680cdda44a,facebookresearch_ParlAI/ParlAI/parlai/agents/hugging_face/dict.py,"fle_key;fle_key;opt,truncation=True;_init_llama_path;_init_llama_path",A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.HubertConfig.from_pretrained;transformers.HubertForSequenceClassification.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,6bb220811b55d2b68cd7fec39b4a23e9,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py,"config_path;base_model_name,config=hf_congfig;base_model_name,return_attention_mask=True,do_normalize=False","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.T5ForConditionalGeneration.from_pretrained;transformers.T5Tokenizer.from_pretrained;transformers.BartTokenizer.from_pretrained;transformers.BartForConditionalGeneration.from_pretrained,github.com/ylsung/VL_adapter,187,3a39e588ac61bd64a0b67c730f8f6d08,ylsung_VL_adapter/VL_adapter/download_backbones.py,t5-base;t5-base;facebook/bart-base;facebook/bart-base,"PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,97564a47512c5a9adba5edb2f8c2b384,awslabs_pptod/pptod/DST/inference.py,pretrained_path,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,678c9a69adc9feac89cecae3587eb0c5,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/gen_sel.py,./bert-base-uncased,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.FlaxVisionEncoderDecoderModel.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,b97b41d5c4df30c667c0efb384e255f1,salesforce_CodeRL/CodeRL/transformers/examples/flax/image-captioning/run_image_captioning_flax.py,"unknown,seed=unknown,dtype=getattr;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoProcessor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained,github.com/sail-sg/EditAnything,2813,3741c9ba1389878f1cc6dc9ed8567b24,sail-sg_EditAnything/EditAnything/sam2image.py,"Salesforce/blip2-opt-2.7b;Salesforce/blip2-opt-2.7b,torch_dtype=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,dba2a2f253ca54c094b0555de5059504,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/infer.py,"model_path,trust_remote_code=True,device_map=auto;model_path,trust_remote_code=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.CLIPVisionModel.from_pretrained;transformers.CLIPImageProcessor.from_pretrained;transformers.CLIPVisionModel.from_pretrained,github.com/tabtoyou/KoLLaVA,133,baeeac19c4a852fb1c3c60460655e57b,tabtoyou_KoLLaVA/KoLLaVA/llava/model/llava_mpt.py,unknown;vision_tower;vision_tower,KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.BertTokenizer.from_pretrained;transformers.BertConfig.from_pretrained,github.com/lavis-nlp/spert,624,2256b71618dd6ed92589266e7190dc72,lavis-nlp_spert/spert/spert/spert_trainer.py,"unknown,do_lower_case=unknown,cache_dir=unknown;unknown,cache_dir=unknown",PyTorch code for SpERT: Span-based Entity and Relation Transformer
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c0f4546c0faf1ac78fead6dd5664276e,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/graphixtuning.py,"unknown,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertModel.from_pretrained;transformers.RobertaModel.from_pretrained;transformers.BartModel.from_pretrained,github.com/haoheliu/AudioLDM,2018,2c848807580bf8fb78b7761a6702b094,haoheliu_AudioLDM/AudioLDM/audioldm/clap/open_clip/model.py,bert-base-uncased;roberta-base;facebook/bart-base,"AudioLDM: Generate speech, sound effects, music and beyond, with text."
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/peterwilli/sd-leap-booster,111,f80039040b45b4e7054d95c8fac9c3fc,peterwilli_sd-leap-booster/sd-leap-booster/training/dataset_creator/sd_extractor.py,"model_id_or_path,subfolder=tokenizer;model_id_or_path,subfolder=text_encoder",Fast finetuning using a booster model that puts the initial state to a local minimum
transformers.BertForMaskedLM.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,62cb0a4c99dd5e4f00cf779f1bb0904d,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/bert/convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py,pytorch_dump_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoModelForSeq2SeqLM.from_pretrained.eval;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,adbc66ff3461bf6ea646932de626c265,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/seq2seq-distillation/distillation.py,;unknown,Source code for Zalo AI 2021 submission
transformers.BertModel.from_pretrained;transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained.to,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,6c407a845f843b54a9f32d4f08629d36,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/run_bert_whitening.py,model_path;unknown;device,NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained.to,github.com/the-crypt-keeper/can-ai-code,308,d4eb53cd7d4cd05b9426daaee7b58d44,the-crypt-keeper_can-ai-code/can-ai-code/interview-starcoder.py,checkpoint;device,Self-evaluating interview for AI coders
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,685d3761bfa2c2be726dd9c0022d6e26,yangheng95_PyABSA/PyABSA/pyabsa/utils/text_utils/word2vec.py,pre_tokenizer;rna_bpe_tokenizer,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.BertModel.from_pretrained;transformers.BertModel.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,36c5ede805bef3d4a83c1a453c13bb3e,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/retrieval/models/hop1_retriever.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.to,github.com/CuongNN218/zalo_ltr_2021,138,cadca5133a7ac55be1ab0fbeccf4e59e,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/longform-qa/eli5_utils.py,model_name;device;model_name;device,Source code for Zalo AI 2021 submission
transformers.MarianTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,76b4f8e3c6c92117c3d43e04ba45995f,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/marian/convert_marian_to_pytorch.py,str,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertModel.from_pretrained;transformers.RobertaModel.from_pretrained;transformers.BartModel.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,5f8de7fbcca4a9d8e205dae362c4c16d,AIGC-Audio_AudioGPT/AudioGPT/text_to_audio/Make_An_Audio/ldm/modules/encoders/open_clap/model.py,bert-base-uncased;roberta-base;facebook/bart-base,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,68277c93874f9ee0489d56980df8d381,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/jax-projects/dataset-streaming/run_mlm_flax_stream.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,config=config,seed=unknown,dtype=getattr",Source code for Zalo AI 2021 submission
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained;transformers.RobertaTokenizer.from_pretrained;transformers.RobertaModel.from_pretrained;transformers.BartTokenizer.from_pretrained;transformers.BartModel.from_pretrained,github.com/zhvng/open-musiclm,398,941e6dec635bb8f8e50ea8e29a779e56,zhvng_open-musiclm/open-musiclm/open_musiclm/laion_clap/clap_module/bert.py,bert-base-uncased;bert-base-uncased;roberta-base;roberta-base;facebook/bart-base;facebook/bart-base,"Implementation of MusicLM, a text to music model published by Google Research, with a few modifications."
transformers.BlipConfig.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,eca0e9a3737aa07970909a2abe98ccdd,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/blip/convert_blip_original_pytorch_to_hf.py,config_path;bert-base-uncased,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.UniSpeechSatForSequenceClassification.from_pretrained;transformers.UniSpeechSatForAudioFrameClassification.from_pretrained;transformers.UniSpeechSatForXVector.from_pretrained;transformers.UniSpeechSatConfig.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,34b19cbf0db1eea8a30aabf9a3f38339,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/unispeech_sat/convert_unispeech_original_s3prl_checkpoint_to_pytorch.py,"base_model_name,config=hf_config;base_model_name,config=hf_config;base_model_name,config=hf_config;config_path;base_model_name,return_attention_mask=True,do_normalize=False","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.RobertaTokenizerFast.from_pretrained,github.com/memray/OpenNMT-kpg-release,210,caac3cbb71b391319974b0adb9139cd9,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/inputters/inputter.py,"roberta-base,__slow_tokenizer=roberta_kp_tokenizer,tokenizer_file=None,vocab_file=bpe_vocab,merges_file=bpe_merges",Keyphrase Generation
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/yuchenlin/SwiftSage,164,1fcf2bfd22fa942797ee49899fafd653,yuchenlin_SwiftSage/SwiftSage/fast_agent/ds_train.py,"unknown,cache_dir=unknown;unknown,use_fast=True,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks 
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,c82af9d31a2e7f58d5ba4ef177258fbb,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/flax/language-modeling/run_mlm_flax.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,config=config,seed=unknown,dtype=getattr",Source code for Zalo AI 2021 submission
transformers.BartTokenizer.from_pretrained;transformers.BartForConditionalGeneration.from_pretrained.cuda,github.com/stanfordnlp/chirpycardinal,125,2ab52cbc127af32ec044e8858d3d636d,stanfordnlp_chirpycardinal/chirpycardinal/docker/colbertinfiller/app/remote_module.py,facebook/bart-large;,Stanford's Alexa Prize socialbot
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,5d77c0907188784f805fcea65c1bfb17,ylsung_VL_adapter/VL_adapter/VL-T5/src/pretrain_vcr_data.py,"unknown,max_length=unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/wangyuxinwhy/uniem,572,257ce38f04f21ac2680125d7e368657d,wangyuxinwhy_uniem/uniem/uniem/model.py,str;model_name_or_path,unified embedding model
transformers.BertModel.from_pretrained,github.com/LearnedVector/A-Hackers-AI-Voice-Assistant,886,ec57ea1f3d55daa66a74389d0a4fbaad,LearnedVector_A-Hackers-AI-Voice-Assistant/A-Hackers-AI-Voice-Assistant/VoiceAssistant/nlu/neuralnet/model.py,unknown,"A hackers AI voice assistant, built using Python and PyTorch."
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.CLIPImageProcessor.from_pretrained;transformers.CLIPVisionModel.from_pretrained.cuda;transformers.CLIPImageProcessor.from_pretrained,github.com/tabtoyou/KoLLaVA,133,bb539f3464a45cb87715d47b869c5ecf,tabtoyou_KoLLaVA/KoLLaVA/llava/eval/model_vqa_science.py,"config;model_name;unknown,torch_dtype=unknown;;unknown,torch_dtype=unknown",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.BertTokenizer.from_pretrained,github.com/fastnlp/CPT,448,2291964fb2250430c97ec94321ea2889,fastnlp_CPT/CPT/pretrain/megatron/tokenizer/tokenizer.py,from_pretrained_path,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,e3e26a294528ca3c2a49b97a4c436985,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/src/accelerate/test_utils/scripts/external_deps/test_checkpointing.py,"model_name;model_name,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,8377a44720c38e7411390446327c1b72,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/pytorch-lightning/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",Source code for Zalo AI 2021 submission
transformers.FSMTTokenizer.from_pretrained;transformers.FSMTForConditionalGeneration.from_pretrained.to,github.com/CuongNN218/zalo_ltr_2021,138,fd2a36f600dc01e667d7817a13d640c8,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/seq2seq/old_test_fsmt_bleu_score.py,mname;torch_device,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,b80d6680c1fa6c277277a61007dc6990,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/fsdp_with_peak_mem_tracking.py,"unknown;unknown,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/shmsw25/FActScore,119,084b7b8d6aeb1bab0a6546f3d1c99124,shmsw25_FActScore/FActScore/factscore/download_data.py,"path_raw,device_map=Dict,torch_dtype=unknown,low_cpu_mem_usage=True;kalpeshk2011/instruct-llama-7b-wdiff,device_map=Dict,torch_dtype=unknown,low_cpu_mem_usage=True;path_raw;kalpeshk2011/instruct-llama-7b-wdiff","A package to evaluate factuality of long-form generation. Original implementation of our EMNLP 2023 paper ""FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"""
transformers.Blip2Processor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained.to,github.com/junshutang/Make-It-3D,1517,a3095c4ae3fc0db397c557b1254127e1,junshutang_Make-It-3D/Make-It-3D/main.py,Salesforce/blip2-opt-2.7b;cuda,[ICCV 2023] Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ab70af131ef1cd3142d0321b7a3db175,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/gen_seltest.py,./bert-base-uncased,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,a025872b7568f79c1d062c345b5b6bae,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/bertabs/modeling_bertabs.py,bert-base-uncased,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,8694fa5bf2cabe753d47ac33c8a53576,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/text-classification/run_glue_no_trainer.py,"unknown,num_labels=num_labels,finetuning_task=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoFeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,107ada2b8897bd97209247a8cc037b28,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py,"facebook/convnext-base-224-22k-1k,size=size","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.RobertaForMaskedLM.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,b36930e6b3181465453e1ce1c10797d5,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/distillation/scripts/extract.py,unknown;unknown,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,1986096d14e12108aa8a0c2476a9c73e,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/fsner/src/fsner/tokenizer_utils.py,pretrained_model_name_or_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/andreamad8/FSB,119,cfcd58acc0ba5382d6e3ce769aa48357,andreamad8_FSB/FSB/classifier/run_glue.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,f866c0558b2da57b6994ba170c743160,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/babyai-text/babyai/scripts/trace_agent_traj.py,"storage/models/GPTJ,padding_side=left;EleutherAI/gpt-j-6B,padding_side=left",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,6381f1f2a91102ddee0575f1fa617025,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/summarization/run_summarization.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.LongformerModel.from_pretrained;transformers.LongformerForQuestionAnswering.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,ed7cdb92db809fd3a7ba69fb04fb7381,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py,longformer_model;longformer_model,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.SwitchTransformersConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,a2244fed223b7ada13bbb917f39381b7,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/switch_transformers/convert_switch_transformers_original_flax_checkpoint_to_pytorch.py,config_file,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,b458ab18c0513641387391304b6e4f7f,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_grapter_512_elu.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.MarianMTModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,1486cddcba185ad8aaecdbfcc7c1c9c1,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/seq2seq-distillation/_test_bash_script.py,MARIAN_MODEL,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/twistedcubic/attention-rank-collapse,141,6270a6b275fea5a66dd3ffdb06fd825d,twistedcubic_attention-rank-collapse/attention-rank-collapse/run_memorization.py,"unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown","[ICML 2021 Oral] We show pure attention suffers rank collapse, and how different mechanisms combat it."
transformers.BertTokenizerFast.from_pretrained,github.com/Guzpenha/transformer_rankers,155,68dcb2a2a125ad344efeaaa382c76ee0,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/examples/analyze_ws.py,unknown,A library to conduct ranking experiments with transformers.
transformers.T5Config.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,d31160a450d2f451c72f23c2e62426a9,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/t5/convert_t5x_checkpoint_to_flax.py,config_name,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/princeton-nlp/MeZO,883,743e468732ac0d15f754a20291664ffd,princeton-nlp_MeZO/MeZO/medium_models/run.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown;unknown,additional_special_tokens=special_tokens,cache_dir=unknown",[NeurIPS 2023] MeZO: Fine-Tuning Language Models with Just Forward Passes. https://arxiv.org/abs/2305.17333
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,0159923d0d53484c6b3c4bb7c623e59b,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/data_all_in/map_subword.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/guifaChild/text_to_vedio,158,432498e8a8ee70fe5c789ee933117661,guifaChild_text_to_vedio/text_to_vedio/ChatGLM-6B-main/ptuning/main.py,"unknown,trust_remote_code=True;unknown,trust_remote_code=True;unknown,config=config,trust_remote_code=True;unknown,config=config,trust_remote_code=True",这是一个由文本直接生成视频的项目
transformers.AutoModelForSequenceClassification.from_pretrained,github.com/KRR-Oxford/DeepOnto,119,1849a12171e4c5a6aaca3de0e9a81ba2,KRR-Oxford_DeepOnto/DeepOnto/src/deeponto/align/bertmap/bert_classifier.py,"unknown,output_hidden_states=eval_mode",A package for ontology engineering with deep learning and language models.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/bofenghuang/vigogne,448,e58b5f12ee6fb3c96b13616d5859712d,bofenghuang_vigogne/vigogne/scripts/merge_peft_adapters.py,"base_model_name_or_path,padding_side=right,use_fast=False;base_model_name_or_path,torch_dtype=unknown,low_cpu_mem_usage=True,device_map=Dict",French instruction-following and chat models
transformers.AutoTokenizer.from_pretrained,github.com/airsplay/vokenization,187,299525854689622228de1b108bedecb4,airsplay_vokenization/vokenization/tokenization/tokenize_dataset.py,"tokenizer_name,use_fast=True","PyTorch code for EMNLP 2020 Paper ""Vokenization: Improving Language Understanding with Visual Supervision"""
transformers.LongformerModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,00a18ea24dfdd65d680c6eee80785ea0,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dstc11-simmc/task2/model/backbone.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,abdad32f902a2cead22689d4704b2738,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/codeparrot/scripts/initialize_model.py,"unknown;unknown,None=config_kwargs",Source code for Zalo AI 2021 submission
transformers.TransfoXLCorpus.from_pretrained;transformers.TransfoXLLMHeadModel.from_pretrained,github.com/salesforce/CodeRL,436,50c2200f184308d748c814922a381332,salesforce_CodeRL/CodeRL/transformers/examples/legacy/run_transfo_xl.py,unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.Wav2Vec2Config.from_pretrained,github.com/salesforce/CodeRL,436,bac73d3b9eb312f90e9ca05398088b27,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py,"unknown,cache_dir=unknown,do_normalize=True;unknown,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizerFast.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/Lipurple/Grounded-Diffusion,137,4a65186acd3e2d8b6c0f810bec29cf6b,Lipurple_Grounded-Diffusion/Grounded-Diffusion/ldm/modules/encoders/modules.py,bert-base-uncased;version;version,Open-vocabulary Object Segmentation with Diffusion Models
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/michiyasunaga/LinkBERT,369,48b55b1447c15e975c53a685dfc139c9,michiyasunaga_LinkBERT/LinkBERT/src/tokcls/run_ner.py,"unknown,num_labels=num_labels,label2id=label_to_id,id2label=Dict,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;tokenizer_name_or_path,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown,add_prefix_space=True;tokenizer_name_or_path,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown",[ACL 2022] LinkBERT: A Knowledgeable Language Model 😎 Pretrained with Document Links
transformers.BertTokenizer.from_pretrained,github.com/yahshibu/nested-ner-tacl2020-transformers,138,f08112ca7cad0dc8194a2f2e1f18985b,yahshibu_nested-ner-tacl2020-transformers/nested-ner-tacl2020-transformers/reader/reader.py,"bert_model,do_lower_case=unknown",Implementation of Nested Named Entity Recognition using BERT
transformers.AutoTokenizer.from_pretrained,github.com/wangyuxinwhy/uniem,572,efad369e5ef63c07ab474259de11de5a,wangyuxinwhy_uniem/uniem/scripts/train_medi.py,model_name_or_path,unified embedding model
transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/NyanNyanovich/nyan,131,e180f9330e9d0918880a0d8399ad65cc,NyanNyanovich_nyan/nyan/nyan/labse.py,device;model_name,Automatic news aggregator in Telegram / Автоматический агрегатор новостей в Телеграме
transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,73afea37f3a698a5ebc1c7313c7d91cc,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py,tokenizer_model_name,Source code for Zalo AI 2021 submission
transformers.BertForMaskedLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,485bdf29f432090c95b00b4c29c8b69c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/space-2/trippy/bert_models.py,model_name_or_path,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/bofenghuang/vigogne,448,dcac026724c1f332300b7d2da9907f97,bofenghuang_vigogne/vigogne/vigogne/demo/demo_chat.py,"base_model_name_or_path,padding_side=right,use_fast=False;base_model_name_or_path,load_in_8bit=load_8bit,torch_dtype=unknown,device_map=auto;base_model_name_or_path,device_map=Dict,torch_dtype=unknown;base_model_name_or_path,device_map=Dict,low_cpu_mem_usage=True",French instruction-following and chat models
transformers.Wav2Vec2ConformerConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,6b055ea527725ca107c80ef5e3503c7a,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/wav2vec2_conformer/convert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py,"config_path,hidden_act=swish","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForTokenClassification.from_pretrained,github.com/salesforce/CodeRL,436,968349202bf3348a2bfb0c64258e9ff1,salesforce_CodeRL/CodeRL/transformers/examples/legacy/token-classification/run_tf_ner.py,"unknown,num_labels=num_labels,id2label=label_map,label2id=Dict,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,from_pt=bool,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,695657fc7e431d6920fc54cdaab17696,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/run_mlm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,2bf2a2e1e94e47d6b1b36a79d7dfbe8f,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/slot_based_dialog/cdial_gpt/reader.py,./gpt_chinese;./gpt_chinese,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoProcessor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained,github.com/sail-sg/EditAnything,2813,aabe09fbe87820072a3daddfeeded73a,sail-sg_EditAnything/EditAnything/editany_lora.py,"Salesforce/blip2-opt-2.7b;Salesforce/blip2-opt-2.7b,torch_dtype=unknown,device_map=auto","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ypwhs/CreativeChatGLM,203,21faa047b03dfc13872ce468923397ae,ypwhs_CreativeChatGLM/CreativeChatGLM/predictors/chatglm2_predictor.py,"model_name,trust_remote_code=True,resume_download=True;model_name,trust_remote_code=True,resume_download=True;model_name,trust_remote_code=True,resume_download=True;model_name,trust_remote_code=True,resume_download=True,low_cpu_mem_usage=True,torch_dtype=unknown,device_map=Dict;model_name,trust_remote_code=True,resume_download=True",👋 欢迎来到 ChatGLM 创意世界！你可以使用修订和续写的功能来生成创意内容！
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,1940631b842d5ac154266ae826d7f9c7,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/relation_emb_init.py,"tokenizer_path,use_fast=True;pretrain_model_path",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.Wav2Vec2ForCTC.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,cd3701cc30e8623614b3b563c1da3939,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/wav2vec2/run_common_voice.py,"unknown,cache_dir=unknown,activation_dropout=unknown,attention_dropout=unknown,hidden_dropout=unknown,feat_proj_dropout=unknown,mask_time_prob=unknown,gradient_checkpointing=unknown,layerdrop=unknown,ctc_loss_reduction=mean,pad_token_id=unknown,vocab_size=len",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d60ab1b47564ecfd6f15ea8de4317497,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/finetune.py,"unknown,use_fast=False;unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,7337eacf35ac621ac7275f01356d2892,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/scripts/tabfact/run_col.py,pretrained_model_name_or_path=../../utils_file/gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.LlamaForCausalLM.from_pretrained,github.com/bofenghuang/vigogne,448,02393f3dade3067845de1b94b7a53251,bofenghuang_vigogne/vigogne/scripts/convert_llama_weights_to_hf.py,"tmp_model_path,torch_dtype=unknown,low_cpu_mem_usage=True",French instruction-following and chat models
transformers.Blip2Processor.from_pretrained;transformers.Blip2Model.from_pretrained,github.com/Xpitfire/symbolicai,720,c9c6d68fff3950deb79349100472767d,Xpitfire_symbolicai/symbolicai/symai/backend/engine_blip2.py,"Salesforce/blip2-opt-2.7b;Salesforce/blip2-opt-2.7b,torch_dtype=unknown",Compositional Differentiable Programming Library
transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,162217c2493e5ce50eb27639e18a06ce,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/Condenser/helper/create_train_co.py,"unknown,use_fast=True",Source code for Zalo AI 2021 submission
transformers.RobertaTokenizer.from_pretrained,github.com/haoheliu/AudioLDM,2018,ffb2f04e5999168477ece382a669b6bb,haoheliu_AudioLDM/AudioLDM/audioldm/clap/encoders.py,roberta-base,"AudioLDM: Generate speech, sound effects, music and beyond, with text."
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,229d3df8dc3687a7deda077be305695d,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/scripts/wtq/run_col.py,pretrained_model_name_or_path=../../utils_file/gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,b6f4dd488788a4605ae5c7ae11861321,naver_gdc/gdc/dpg/examples/run-distributional.py,config,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.AutoTokenizer.from_pretrained,github.com/ai-forever/Kandinsky-2,2534,a19279f7c172d710bf6c9cb3e716d20d,ai-forever_Kandinsky-2/Kandinsky-2/kandinsky2/train_utils/data/dataset_unclip_2_1.py,tokenizer_name,Kandinsky 2 — multilingual text2image latent diffusion model
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,431fc971fdbc68c3ad9f9113678198d1,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/adversarial/run_hans.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.RobertaTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,71983600b05481e2f8671423e1c25e21,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/r2sql/sparc/reranker/dataset.py,"roberta-base,max_len=128",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BioGptConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,411ba73d09f3d27b1c9a9585bbd981c4,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/biogpt/convert_biogpt_original_pytorch_checkpoint_to_pytorch.py,pytorch_dump_folder_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/tcapelle/apple_m1_pro_python,160,f7be8f3f5eb6f86dcf69f76158432db6,tcapelle_apple_m1_pro_python/apple_m1_pro_python/pytorch/train_bert.py,"model_name;model_name,num_labels=num_labels",A collection of ML scripts to test the M1 Pro MacBook Pro 
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,fb4499482bd30bf51674f5f1bd1e9c7d,awslabs_pptod/pptod/data/pre-training_corpora/utlis/tokenize_intent_classification_dataset.py,tokenizer_path,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.UniSpeechSatConfig.from_pretrained,github.com/salesforce/CodeRL,436,882073dcc11890c038af0f099a5a8c04,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/unispeech_sat/convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,59fae34216a076dc5ae84d908e3e11d4,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/fsner/src/fsner/model.py,"pretrained_model_name_or_path,return_dict=True",Source code for Zalo AI 2021 submission
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.Wav2Vec2Config.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f19569c0ffc5447fd7d267408f524883,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/wav2vec2/run_pretrain.py,"unknown,cache_dir=unknown,do_normalize=True;unknown,cache_dir=unknown,gradient_checkpointing=unknown",Source code for Zalo AI 2021 submission
transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d8cec5ccac9dcdb3a91d028721b2b7b5,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/proton/preprocess/test_grappa_prob_cmp.py,device;os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.T5Config.from_pretrained;transformers.T5Config.from_pretrained;transformers.FlaxT5ForConditionalGeneration.from_pretrained,github.com/salesforce/CodeRL,436,cc552629790d200f14aa419f8bf83031,salesforce_CodeRL/CodeRL/transformers/examples/flax/language-modeling/run_t5_mlm_flax.py,"unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,cache_dir=unknown,vocab_size=len;unknown,cache_dir=unknown;unknown,config=config,seed=unknown,dtype=getattr",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ae193f762f7e81e392b0c47bc9e5e91b,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/diana/downstreamdeca/dianawometa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.half.cuda;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoConfig.from_pretrained,github.com/RUC-GSAI/YuLan-IR,181,c223c26dfd804a0dbfd84a6f17beb87c,RUC-GSAI_YuLan-IR/YuLan-IR/RETA-LLM/system/load_model.py,"model_path,trust_remote_code=True;;model_path,None=tokenizer_init_kwargs;model_path,None=model_init_kwargs;model_path,None=model_init_kwargs;model_path,None=model_init_kwargs",YuLan-IR: Information Retrieval Boosted LMs
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForMaskedLM.from_pretrained;transformers.TFAutoModelForMaskedLM.from_pretrained,github.com/salesforce/CodeRL,436,ba4f06d9e30812e2b82884ea1ae69bce,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/language-modeling/run_mlm.py,"checkpoint;unknown;unknown;unknown;unknown;checkpoint,config=config;unknown,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,040cf9d1b57d53ddec3bd7124a86c457,ashkamath_mdetr/mdetr/datasets/clevr.py,unknown,
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelWithLMHead.from_pretrained,github.com/allenai/real-toxicity-prompts,125,0580aeeae4e5ee1bb15dfa22f50e3760,allenai_real-toxicity-prompts/real-toxicity-prompts/scripts/finetuning/finetune_gpt2_ctrl.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",
transformers.T5Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/ssymmetry/BBT-FinCUGE-Applications,180,eac05163b1150e35bf6c33769cd88a6f,ssymmetry_BBT-FinCUGE-Applications/BBT-FinCUGE-Applications/genarate_demo.py,model_name;model_name,
transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,f5888bd573ea3f88a68ea8837dfee32c,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/seq2seq-distillation/convert_pl_checkpoint_to_hf.py,hf_src_model_dir;hf_src_model_dir,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.FlaxAutoModelForImageClassification.from_pretrained,github.com/salesforce/CodeRL,436,605de38b56b34c91baf28d26aca6a2f7,salesforce_CodeRL/CodeRL/transformers/examples/flax/vision/run_image_classification.py,"unknown,num_labels=len,image_size=unknown,cache_dir=unknown;unknown,num_labels=len,image_size=unknown,cache_dir=unknown;unknown,config=config,seed=unknown,dtype=getattr",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained.cuda,github.com/Neutralzz/BiLLa,414,896c18d1841d9147f095d7a2ac8fe1cb,Neutralzz_BiLLa/BiLLa/eval_codes/get_model_answer.py,"model_path,use_fast=False;",BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability
transformers.BertConfig.from_pretrained;transformers.BertTokenizer.from_pretrained;transformers.BertForQuestionAnswering.from_pretrained,github.com/fastnlp/CPT,448,a8bdab0f5e9889eb33be127d12ab8734,fastnlp_CPT/CPT/finetune/mrc/run_mrc.py,"config_path;config_path;model_path,config=bert_config",CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.XLMRobertaTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,2cded9b6c19e91497b9341aeab77980c,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py,"metadata;pytorch_dump_folder_path;pytorch_dump_folder_path,task=entity_classification;pytorch_dump_folder_path",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/RUC-GSAI/YuLan-IR,181,a104235bd40ca89df01146354b90cab1,RUC-GSAI_YuLan-IR/YuLan-IR/RETA-LLM/system/searcher.py,"DAM_NAME,config=config",YuLan-IR: Information Retrieval Boosted LMs
transformers.AutoProcessor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained,github.com/sail-sg/EditAnything,2813,c9f734e9978176a43f717732ea632890,sail-sg_EditAnything/EditAnything/dataset_build.py,"Salesforce/blip2-opt-2.7b;Salesforce/blip2-opt-2.7b,torch_dtype=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.Wav2Vec2Processor.from_pretrained;transformers.HubertForCTC.from_pretrained.to,github.com/YuanGongND/whisper-at,199,57dc77dda9afb38c6d2d214090c61b17,YuanGongND_whisper-at/whisper-at/src/noise_robust_asr/asr_experiments/transcribe_esc_hubert_xl.py,facebook/hubert-xlarge-ls960-ft;device,"Code and Pretrained Models for Interspeech 2023 Paper ""Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong Audio Event Taggers"""
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/demi6od/ChatBot,173,7c0c4b8c4a13e201ee34057edfce96d6,demi6od_ChatBot/ChatBot/ChatBotBertTransformer/model.py,bert-base-uncased;bert-base-uncased,"Pytorch Generative ChatBot (Dialog System) based on RNN, Transformer, Bert and GPT2"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,2a9e76fdd0db4a98237a5c73945ca1a1,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_grapter_256.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoModelForAudioClassification.from_pretrained,github.com/salesforce/CodeRL,436,6b01641283051ec91fe72cba1fd9729e,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/audio-classification/run_audio_classification.py,"unknown,return_attention_mask=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,num_labels=len,label2id=label2id,id2label=id2label,finetuning_task=audio-classification,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,5d9b3c70b886cea03aa09f61934b9733,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/longt5/convert_longt5x_checkpoint_to_flax.py,config_name,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/thu-ml/controlvideo,183,4c0b93e9c162574bbf17bbd74ce5c8ed,thu-ml_controlvideo/controlvideo/train.py,"pretrained_model_path,subfolder=tokenizer;pretrained_model_path,subfolder=text_encoder","Official implementation for ""ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing"" "
transformers.AutoConfig.from_pretrained;transformers.AutoModelWithLMHead.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ucinlp/autoprompt,497,a2d9225987fb19b488e3f10a91a1e768,ucinlp_autoprompt/autoprompt/autoprompt/create_trigger.py,"model_name;model_name;model_name,add_prefix_space=True",AutoPrompt: Automatic Prompt Construction for Masked Language Models.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,fa6402a42971672627b1dc0f98411b8a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/multiple_choice/run_multiple_choice.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.GPT2TokenizerFast.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/thunlp/OpenBackdoor,114,1dd381f3d31733bfa334759042c06e6e,thunlp_OpenBackdoor/OpenBackdoor/openbackdoor/utils/evaluator.py,gpt2-large;gpt2-large,"An open-source toolkit for textual backdoor attack and defense (NeurIPS 2022 D&B, Spotlight)"
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,c9560829f8e2bd4aebc7e2a526cd5aec,ylsung_VL_adapter/VL_adapter/VL-T5/src/nlvr_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForMultipleChoice.from_pretrained,github.com/salesforce/CodeRL,436,45c6569131f0837cd68290b0a241b32e,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/multiple-choice/run_swag.py,"config_path,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;model_path,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,2674f754698692ed9b3b0ae27aa835ee,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/LGESQL/cosql/model/encoder/graph_input.py,os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.T5ForConditionalGeneration.from_pretrained,github.com/salesforce/CodeRL,436,3d959852da2cd9765b725824ae304454,salesforce_CodeRL/CodeRL/train.py,"model_path,tuning_mode=unknown,clone_rl_head=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForTokenClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,2b1c3f038fe1b27c39612efe23c6e339,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/flax/token-classification/run_flax_ner.py,"unknown,num_labels=num_labels,label2id=label_to_id,id2label=Dict,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;tokenizer_name_or_path,cache_dir=unknown,revision=unknown,use_auth_token=unknown,add_prefix_space=True;tokenizer_name_or_path,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,ffb52dc2b64249d0ad0a538cc1c9773d,salesforce_CodeRL/CodeRL/transformers/examples/legacy/seq2seq/pack_dataset.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BartConfig.from_pretrained,github.com/fastnlp/CPT,448,a5c75969c87624b57b83d563ccd7e34c,fastnlp_CPT/CPT/pretrain/megatron/model/cpt_model.py,unknown,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoFeatureExtractor.from_pretrained,github.com/kangyeolk/Paint-by-Sketch,156,deac862649954b8c7cf35438d8c0ab92,kangyeolk_Paint-by-Sketch/Paint-by-Sketch/scripts/eval_dataloader.py,safety_model_id,Stable Diffusion-based image manipulation method with a sketch and reference image
transformers.RobertaTokenizerFast.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,1120ebabc5e4dbe0ada93fa54bcaffaf,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/spectra/main.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModel.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,b1ecc873c371c1f446bc76989fbb61d1,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/qa/qa_model.py,unknown,Multi-hop dense retrieval for question answering
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,779d5c5a2d7601dfe8ec29f4f16d291d,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/mm-imdb/run_mmimdb.py,"unknown;unknown,do_lower_case=unknown,cache_dir=unknown;unknown,config=transformer_config,cache_dir=unknown;unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizer.from_pretrained,github.com/memray/OpenNMT-kpg-release,210,1e7678c24315363fdc959a6a6039b4e0,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/keyphrase/json_plus_process.py,"bert-base-uncased,do_lower_case=True",Keyphrase Generation
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,38049d28cccb055c47f759f930f9b568,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/bertology/run_bertology.py,"unknown,num_labels=num_labels,finetuning_task=unknown,output_attentions=True,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,71a2ede71f18988bde5213b2cff4615f,awslabs_pptod/pptod/Pretraining/pretrain.py,preprocessed_tokenizer_path,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.models.auto.configuration_auto.AutoConfig.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,9a2f15399ce7e5b5e05542562d3e37d9,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/seq2seq/utils/picard_model_wrapper.py,"pretrained_model_name_or_path,return_unused_kwargs=True,None=kwargs",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/yangheng95/PyABSA,779,0b7532c6d3ff2dd1d45e468fab30022f,yangheng95_PyABSA/PyABSA/pyabsa/tasks/_Archive/ProteinRegression/instructor/proteinr_instructor.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.BertTokenizerFast.from_pretrained,github.com/IBM/zshot,285,3a38f2eb9d42f075ec42c4e3ccced435,IBM_zshot/zshot/zshot/linker/linker_smxm.py,"bert-large-cased,truncation_side=left,cache_dir=MODELS_CACHE_PATH",Zero and Few shot named entity & relationships recognition
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,52ab32b2740d4aeb7b8ee84ea35ba68c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/flow_based_dialog/data_cls.py,directory,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained.to;transformers.LlamaTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,edd96274b74736665d5d73ffb15a97dc,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/PRO/eval/infer_and_eval_main_generate.py,model_name_or_path;model_device;model_name_or_path;model_name_or_path,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,cdbb7acc8693e80e25bcabbb31b63c91,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/question-answering/run_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;model_path,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.RobertaTokenizer.from_pretrained,github.com/snap-stanford/GreaseLM,200,f49b87aaca0104995087f5fd0b821cfa,snap-stanford_GreaseLM/GreaseLM/preprocess_utils/graph.py,roberta-large,[ICLR 2022 spotlight]GreaseLM: Graph REASoning Enhanced Language Models for Question Answering
transformers.BartTokenizer.from_pretrained;transformers.BartForConditionalGeneration.from_pretrained.cuda,github.com/stanfordnlp/chirpycardinal,125,18e9274644bb0d475be2b1ac89874009,stanfordnlp_chirpycardinal/chirpycardinal/docker/infiller/app/remote_module.py,facebook/bart-large;,Stanford's Alexa Prize socialbot
transformers.DPRContextEncoderTokenizerFast.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,82a0a5bd530094646bdfa3827ca48cc2,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag-end2end-retriever/kb_encode_utils.py,facebook/dpr-ctx_encoder-multiset-base,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/airsplay/vokenization,187,3007e78c8ab5faae858125da4232d6be,airsplay_vokenization/vokenization/tokenization/to_hdf5.py,tokenizer_name,"PyTorch code for EMNLP 2020 Paper ""Vokenization: Improving Language Understanding with Visual Supervision"""
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,35af1c0944fd808b2fea6a20327a8b45,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/pace/pace/utils/write_simmc2_rg.py,"bert-base-uncased,do_lower_case=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.PerceiverTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,1f92ea94d3497830eee32c402c45237b,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py,/Users/NielsRogge/Documents/Perceiver/Tokenizer files,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.SwinConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,8f198e590f16242b8a0607e5d531f224,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/maskformer/convert_maskformer_swin_to_pytorch.py,"microsoft/swin-tiny-patch4-window7-224,out_features=List","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.T5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/Mikubill/naifu-diffusion,222,d350644a64034d1507fc055394f8189b,Mikubill_naifu-diffusion/naifu-diffusion/experiment/encoder.py,version;version;version;version,Train stable diffusion model with Diffusers and Pytorch Lightning
transformers.AutoConfig.from_pretrained;transformers.AutoModel.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/amazon-science/sccl,276,9f0c441827859e12e8cd0f093714291f,amazon-science_sccl/sccl/utils/optimizer.py,"BERT_CLASS;BERT_CLASS,config=config;BERT_CLASS","Pytorch implementation of Supporting Clustering with Contrastive Learning, NAACL 2021"
transformers.LongformerTokenizerFast.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,4abe7a9deacf5866d1f9ca236b1b1a62,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dstc11-simmc/task3/eval_dstc11_task3.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoFeatureExtractor.from_pretrained,github.com/cross-domain-compositing/cross-domain-compositing,167,25e2e3fe95d7c601c5a1dd8b881ab978,cross-domain-compositing_cross-domain-compositing/cross-domain-compositing/scripts/txt2img.py,safety_model_id,
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/yangheng95/PyABSA,779,4e18f91d142042f90ad461e529034209,yangheng95_PyABSA/PyABSA/pyabsa/tasks/ABSAInstruction/model.py,"checkpoint;checkpoint;model_checkpoint,force_download=True;model_checkpoint,force_download=True","Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,af7620ce910188958f5987cfb869d17b,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/LGESQL/sparc/utils/example.py,google/electra-large-discriminator,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained,github.com/YuxinWenRick/tree-ring-watermark,145,eaf4bd3423795ddc9830d785a2ef7bdd,YuxinWenRick_tree-ring-watermark/tree-ring-watermark/open_clip/hf_model.py,model_name_or_path,
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,f6163cb0a2a4ba7c89c93ad3a3c69a5d,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/question-answering/run_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.GPT2LMHeadModel.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/Vahe1994/SpQR,466,50e11ac8ded56d634233cf099dd87660,Vahe1994_SpQR/SpQR/lm-evaluation-harness/scripts/make_gpt2_test_cases.py,gpt2;gpt2,
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForTokenClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,9744159031240e1a4975e928a7ba75fc,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/token-classification/run_ner.py,"unknown,num_labels=num_labels,label2id=label_to_id,id2label=Dict,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;tokenizer_name_or_path,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown,add_prefix_space=True;tokenizer_name_or_path,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,f4646fcbd76ab973f92b5aa2d3409446,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/combined_prefixtuning.py,"unknown,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.CLIPConfig.from_pretrained,github.com/salesforce/CodeRL,436,8ffa3f19bdc398cfeb461ca389e09698,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/clip/convert_clip_original_pytorch_to_hf.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/VinAIResearch/XPhoneBERT,237,aa1f90bd79e057a060eca44f9759df80,VinAIResearch_XPhoneBERT/XPhoneBERT/VITS_with_XPhoneBERT/inference.py,unknown,XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech (INTERSPEECH 2023)
transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,23f54dc8d3df71f4e99f897500356fde,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/proton/preprocess/common_utils.py,unknown;os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.XLNetLMHeadModel.from_pretrained,github.com/salesforce/CodeRL,436,27f10781ed3c4cf7c1360b3fa2ad1990,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/language-modeling/run_plm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMultipleChoice.from_pretrained,github.com/salesforce/CodeRL,436,def8c4216de49349c39868d63d3b4a6c,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/multiple-choice/run_swag_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.WhisperProcessor.from_pretrained,github.com/sanchit-gandhi/whisper-jax,3647,6faf80fb7ee41526b488b1eec993ed73,sanchit-gandhi_whisper-jax/whisper-jax/whisper_jax/pipeline.py,unknown,JAX implementation of OpenAI's Whisper model for up to 70x speed-up on TPU.
transformers.CamembertTokenizer.from_pretrained;transformers.CamembertForMaskedLM.from_pretrained,github.com/salesforce/CodeRL,436,0144463dc7502c645cb1e535a96e9f52,salesforce_CodeRL/CodeRL/transformers/examples/legacy/run_camembert.py,camembert-base;camembert-base,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,ecb4131910f78ebc6ce2473a4dfb7082,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/summarization/run_summarization_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.ElectraConfig.from_pretrained;transformers.ElectraTokenizerFast.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,a99ce76d14ab6308cd344673943cbc39,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/pretrain/save_model.py,unknown;unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.RagConfig.from_pretrained;transformers.RagTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,f2faa2f5146f34e8260af5f9ca0230cd,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag-end2end-retriever/distributed_ray_retriever.py,"retriever_name_or_path,None=kwargs;retriever_name_or_path,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,ccdf0b48cda3fb16ff134123c97ce55e,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/eval/eval_single_fever.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,f13e1db03789c99ef6242ffd22c95cc6,salesforce_CodeRL/CodeRL/transformers/examples/legacy/seq2seq/finetune_trainer.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=unknown,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/Yui010206/SeViLA,121,4cf8fd6f0e218f93f06a8b05ba6dfe9b,Yui010206_SeViLA/SeViLA/lavis/models/blip2_models/blip2_opt.py,"opt_model,use_fast=False",[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,33598235beb31f27e704edd965ed293a,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/encode_corpus.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.models.auto.AutoConfig.from_pretrained;transformers.models.auto.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,a777af91e4f8f3025e650269d20cf27c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/graphix/seq2seq/run_peteshaw_train.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown,max_length=unknown,num_beams=unknown,num_beam_groups=unknown,diversity_penalty=unknown,gradient_checkpointing=unknown,use_cache=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained,github.com/SCUTlihaoyu/open-chat-video-editor,2354,73265c559329faefd31977260a2b135b,SCUTlihaoyu_open-chat-video-editor/open-chat-video-editor/generator/video/retrieval/models/clip_model.py,model_name,Open source short video automatic generation tool
transformers.BertTokenizer.from_pretrained;transformers.BertForMaskedLM.from_pretrained.to.eval;transformers.GPT2LMHeadModel.from_pretrained.to;transformers.GPT2Tokenizer.from_pretrained,github.com/HazyResearch/domino,129,6338be627afa3268f53e1e4affc46134,HazyResearch_domino/domino/domino/_describe/generate.py,unknown;;device;gpt2,
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/salesforce/CodeRL,436,e218446ce1825efe89c1141227913ab4,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/mlm_wwm/run_mlm_wwm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoFeatureExtractor.from_pretrained,github.com/kangyeolk/Paint-by-Sketch,156,683567a794db631837edae76a8d4b11d,kangyeolk_Paint-by-Sketch/Paint-by-Sketch/scripts/inference.py,safety_model_id,Stable Diffusion-based image manipulation method with a sketch and reference image
transformers.WavLMConfig.from_pretrained,github.com/salesforce/CodeRL,436,7ee0858cccee1530be219473ba65a166,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/wavlm/convert_wavlm_original_pytorch_checkpoint_to_pytorch.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained;transformers.RobertaTokenizer.from_pretrained;transformers.RobertaModel.from_pretrained;transformers.BartTokenizer.from_pretrained;transformers.BartModel.from_pretrained,github.com/haoheliu/AudioLDM,2018,722b3c3fc475eee18c35537f8c6800ec,haoheliu_AudioLDM/AudioLDM/audioldm/clap/open_clip/bert.py,bert-base-uncased;bert-base-uncased;roberta-base;roberta-base;facebook/bart-base;facebook/bart-base,"AudioLDM: Generate speech, sound effects, music and beyond, with text."
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/tabtoyou/KoLLaVA,133,a00be7b8c03bd822de5a1111daf2c66f,tabtoyou_KoLLaVA/KoLLaVA/llava/model/consolidate.py,"src_path,torch_dtype=unknown,low_cpu_mem_usage=True;src_path",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,a92f001d03b93f1fed678b8479ac8d98,awslabs_pptod/pptod/E2E_TOD/learn.py,unknown,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/Guzpenha/transformer_rankers,155,6368c9bce6da77dcb34bf301d6b9886b,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/scripts/train_response2context.py,model_checkpoint;model_checkpoint,A library to conduct ranking experiments with transformers.
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,f85f35e8a794fdae23e6083fa926597b,yangheng95_PyABSA/PyABSA/pyabsa/tasks/TextClassification/prediction/text_classifier.py,find_cwd_dir;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoTokenizer.from_pretrained,github.com/siddk/voltron-robotics,147,388f4de6262eaa000b3da69a19316243,siddk_voltron-robotics/voltron-robotics/voltron/preprocessing/process.py,"language_model,cache_dir=hf_cache",Voltron: Language-Driven Representation Learning for Robotics
transformers.BertTokenizer.from_pretrained,github.com/Yui010206/SeViLA,121,11d4ae4401e1abf54fd588d796e96a2c,Yui010206_SeViLA/SeViLA/lavis/models/alpro_models/__init__.py,bert-base-uncased,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,a444d9bf7717877bd7442780f95f3feb,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dial-start/data_preprocess.py,"bert-base-uncased,do_lower_case=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained,github.com/farizrahman4u/loopgpt,1317,8682e41a42f333996a3c2cefc1b06028,farizrahman4u_loopgpt/loopgpt/loopgpt/models/llama_cpp.py,huggyllama/llama-7b,Modular Auto-GPT Framework
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/AdvancedLiterateMachinery,388,96066e5864772cb08f0764f85b0a4b33,AlibabaResearch_AdvancedLiterateMachinery/AdvancedLiterateMachinery/DocumentUnderstanding/GeoLayoutLM/lightning_modules/geolayoutlm_vie_module.py,"bert-base-uncased,do_lower_case=True","A collection of original, innovative ideas and algorithms towards Advanced Literate Machinery. This project is maintained by the OCR Team in the Language Technology Lab, Alibaba DAMO Academy."
transformers.models.layoutlmv2.feature_extraction_layoutlmv2.LayoutLMv2FeatureExtractor.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,d67479da82bb65c86173baf5c82c1a53,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/layoutxlm/processing_layoutxlm.py,"pretrained_model_name_or_path,None=kwargs",Source code for Zalo AI 2021 submission
transformers.HubertConfig.from_pretrained,github.com/salesforce/CodeRL,436,57a5bd52501288fc4e139dc41597229b,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/hubert/convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,e80bfb074f8289b738ac60edc9a5c9b9,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/benchmarks/big_model_inference.py,unknown;unknown,We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,403ed8c880c80655f4e2432586aa63ee,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/question-answering/run_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;model_path,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.OpenAIGPTTokenizer.from_pretrained;transformers.OpenAIGPTTokenizer.from_pretrained,github.com/INK-USC/MHGRN,238,334263fa3817e90885205d487a59069a,INK-USC_MHGRN/MHGRN/utils/data_utils.py,openai-gpt;openai-gpt,Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering (EMNLP 2020)
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d65be55885a069d622ea11088ae5d9de,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/diana/downstreamdeca/diana.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,e27eef000f9e08baa29d513780443daa,ylsung_VL_adapter/VL_adapter/VL-T5/src/vcr_data.py,"unknown,max_length=unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,0060d88ca33880dc2ae9ef1d12b50fd0,ylsung_VL_adapter/VL_adapter/VL-T5/src/activitynet_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.eval.cuda,github.com/bofenghuang/vigogne,448,e239ca1245d6f3e7d695880d56342215,bofenghuang_vigogne/vigogne/vigogne/data/translate_dataset.py,model_name_or_path;,French instruction-following and chat models
transformers.Wav2Vec2Config.from_pretrained;transformers.MBartConfig.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,0bde076bf6c4af572638838dd7110a47,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/speech_encoder_decoder/convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py,"encoder_config_path,add_adapter=True,adapter_stride=adapter_stride,adapter_kernel_size=adapter_kernel_size,use_auth_token=True,output_hidden_size=encoder_output_dim;decoder_config_path;encoder_config_path,use_auth_token=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,1ac14c15454ebc1b79e90b7d65a3e9be,ylsung_VL_adapter/VL_adapter/VL-T5/src/pretrain_data.py,"unknown,do_lower_case=unknown;unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.to,github.com/CuongNN218/zalo_ltr_2021,138,0ae7111dd1be1c89de23ff3e3a375d88,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/longform-qa/eli5_app.py,yjernite/retribert-base-uncased;cuda:0;yjernite/bart_eli5;cuda:0,Source code for Zalo AI 2021 submission
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,fd6cb334cfbdded79e1cec869fac8bd8,ylsung_VL_adapter/VL_adapter/VL-T5/src/gqa_clip_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.T5TokenizerFast.from_pretrained,github.com/Yui010206/SeViLA,121,1fbddf58641ea256023bcc6bf56d86b7,Yui010206_SeViLA/SeViLA/lavis/models/blip2_models/blip2_fmr.py,t5_model,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.CLIPModel.from_pretrained;transformers.CLIPProcessor.from_pretrained,github.com/Xpitfire/symbolicai,720,8c15226b4f6a1a836339ae3d249e4f9c,Xpitfire_symbolicai/symbolicai/symai/backend/engine_clip.py,unknown;unknown,Compositional Differentiable Programming Library
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,9cef40865be4a5dba60c5d8af3517c4c,ylsung_VL_adapter/VL_adapter/VL-T5/src/modeling_prefix_bart.py,facebook/bart-base;facebook/bart-base,"PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,f357e5a048a0a8773725fa0571954805,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/eval/eval_retrieval.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/skeskinen/bert.cpp,346,7b3424177a365d06a3f7e62056ac1712,skeskinen_bert.cpp/bert.cpp/models/convert-to-ggml.py,"dir_model;dir_model,low_cpu_mem_usage=True",ggml implementation of BERT
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,b53aead11e6d64bec739d9bc50346873,ylsung_VL_adapter/VL_adapter/VL-T5/src/modeling_bart.py,facebook/bart-base;facebook/bart-base,"PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,98f85a1fd54d8a069f33bd8923f45106,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/text2sql/scripts/w_ic_examples_retrieval/annotate_binder_program.py,pretrained_model_name_or_path=os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,412133e0f93ab8556d03562d4d21b76c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/pace/pace/modules/gpt2_module.py,"config,cache_dir=config;config,from_tf=bool,config=unknown,cache_dir=config",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertConfig.from_pretrained;transformers.BertConfig.from_pretrained;transformers.BertConfig.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/facebookresearch/ParlAI,10365,36f6af865ce31dc257d64985ae485363,facebookresearch_ParlAI/ParlAI/parlai/agents/rag/dpr.py,bert-base-uncased;bert-base-uncased;config_path;bert-base-uncased,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.RobertaTokenizerFast.from_pretrained;transformers.Wav2Vec2Processor.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,9131a83af4f89792a445c958790cce0c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/spectra/spectra-trippy/run_dst.py,roberta-base;facebook/wav2vec2-base-960h,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,8c364140d29d61b78d82d3ca56305e12,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/src/accelerate/test_utils/scripts/external_deps/test_peak_memory_usage.py,"model_name;model_name,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.RobertaTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,bfd70d9b1b95d4dfab7cee27ba5aceb4,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py,roberta-base;pytorch_dump_folder_path,Source code for Zalo AI 2021 submission
transformers.CLIPModel.from_pretrained;transformers.CLIPProcessor.from_pretrained,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,09b2b8c625151fe17bba83f775587338,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/sentence_transformers/models/CLIPModel.py,model_name;processor_name,NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/lensterxyz/lenster,21455,ad18b4dc902a3fd684aff6c6303c215c,lensterxyz_lenster/lenster/packages/ai/tagger_route.py,topic_hf;topic_hf,Hey is a decentralized and permissionless social media app built with Lens Protocol 🌿
transformers.AutoTokenizer.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,54ab4d9ab2f52dff5ee34ede74c98b09,AIGC-Audio_AudioGPT/AudioGPT/text_to_audio/Make_An_Audio/ldm/modules/encoders/CLAP/CLAPWrapper.py,unknown,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.BertTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,1cbfb47a68cbbce4afcd718750078517,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/bertabs/run_summarization.py,"bert-base-uncased,do_lower_case=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,a25837b3fef1460b7223659a0ff22018,salesforce_CodeRL/CodeRL/transformers/examples/legacy/text-classification/run_tf_text_classification.py,"unknown,cache_dir=unknown;unknown,num_labels=len,label2id=label2id,id2label=Dict,finetuning_task=text-classification,cache_dir=unknown;unknown,from_pt=bool,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizerFast.from_pretrained;transformers.T5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.RobertaTokenizer.from_pretrained;transformers.T5Tokenizer.from_pretrained;transformers.T5EncoderModel.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,9a845b86ed82ad97cf0ccb01e4527e27,AIGC-Audio_AudioGPT/AudioGPT/text_to_audio/Make_An_Audio/ldm/modules/encoders/modules.py,bert-base-uncased;version;version;unknown;unknown;roberta-base;version;version,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.Wav2Vec2Config.from_pretrained;transformers.Wav2Vec2ForSequenceClassification.from_pretrained;transformers.Wav2Vec2FeatureExtractor.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,39be9a12fdd6f89ae583eb7a47bd4f61,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py,"config_path;base_model_name,config=hf_congfig;base_model_name,return_attention_mask=True,do_normalize=False",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,27b3ddf57f9b565deafbb6a64ba75e98,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/token-classification/scripts/preprocess.py,model_name_or_path,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,224a1f51d02f3820bf4c156f84fff36a,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectTermExtraction/instructor/atepc_instructor.py,"unknown,do_lower_case=unknown;unknown","Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/siddk/voltron-robotics,147,cfe7ac5a1b06f7251cda32362ba38e28,siddk_voltron-robotics/voltron-robotics/voltron/models/reproductions/vrn3m.py,"language_model,cache_dir=hf_cache;language_model,cache_dir=hf_cache",Voltron: Language-Driven Representation Learning for Robotics
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForTokenClassification.from_pretrained,github.com/salesforce/CodeRL,436,9d3e57c83b2f9f4e4c7ec05254cd90f9,salesforce_CodeRL/CodeRL/transformers/examples/flax/token-classification/run_flax_ner.py,"unknown,num_labels=num_labels,label2id=label_to_id,id2label=Dict,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;tokenizer_name_or_path,cache_dir=unknown,revision=unknown,use_auth_token=unknown,add_prefix_space=True;tokenizer_name_or_path,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/DavidHuji/CapDec,153,55c31d16eda07a01997b7a3a8467ee2d,DavidHuji_CapDec/CapDec/gpt2_prefix.py,gpt2_type;gpt2,"CapDec: SOTA Zero Shot Image Captioning Using CLIP and GPT2, EMNLP 2022 (findings)"
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,9409e1aa837e34b718a6b6bf28f0dfb6,ashkamath_mdetr/mdetr/datasets/lvis_modulation.py,unknown,
transformers.BertTokenizerFast.from_pretrained,github.com/Guzpenha/transformer_rankers,155,87bdd5e26426f0b4f86c521d87ee0066,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/examples/minimal_example.py,bert-base-cased,A library to conduct ranking experiments with transformers.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForTokenClassification.from_pretrained,github.com/salesforce/CodeRL,436,aacf21f24b1f4aaaf55312e8ddddb640,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/token-classification/run_ner.py,"unknown,num_labels=num_labels;unknown,num_labels=num_labels;tokenizer_name_or_path,use_fast=True,add_prefix_space=True;tokenizer_name_or_path,use_fast=True;unknown,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,b4870e6ec80ad6008c6ac3fb3cc92a85,salesforce_CodeRL/CodeRL/transformers/examples/legacy/run_chinese_ref.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,5eb956ac0815b7aa1f82d3cabf0c1218,awslabs_pptod/pptod/data/pre-training_corpora/utlis/build_tokenizer.py,model_name,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.AutoModelForMaskedLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/wangyuxinwhy/uniem,572,f3140b69b321fe96ed8fd7dec0d6e7aa,wangyuxinwhy_uniem/uniem/mteb-zh/mteb_zh/models.py,"model_name;model_name;model_name;model_name,trust_remote_code=True,model_args=model_args",unified embedding model
transformers.BertTokenizerFast.from_pretrained,github.com/ChenWu98/cycle-diffusion,440,c276793f1f38c9f202d85385cb1a0580,ChenWu98_cycle-diffusion/cycle-diffusion/model/lib/latentdiff/ldm/modules/encoders/modules.py,bert-base-uncased,[ICCV 2023] Zero-shot image editing with stochastic diffusion models
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,01b183d2a6e486f26247f97670abbcc9,salesforce_CodeRL/CodeRL/transformers/examples/legacy/question-answering/run_squad_trainer.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=False;unknown,from_tf=bool,config=config,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/karthikv792/LLMs-Planning,112,714ae34ae3c4dec534fb1aeb025ab3d4,karthikv792_LLMs-Planning/LLMs-Planning/llm_planning_analysis/response_generation.py,"bigscience/bloom;bigscience/bloom,cache_dir=cache_dir,local_files_only=False,load_in_8bit=True,device_map=auto,max_memory=max_memory_mapping",An extensible benchmark for evaluating large language models on planning
transformers.GPT2TokenizerFast.from_pretrained,github.com/Vahe1994/SpQR,466,673b697d6f323fa1044b78a5af1cc234,Vahe1994_SpQR/SpQR/lm-evaluation-harness/scripts/cost_estimate.py,gpt2,
transformers.XLMProphetNetForConditionalGeneration.from_pretrained;transformers.ProphetNetForConditionalGeneration.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,ed9fdc0a010e7d426519d48de8db789f,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py,"prophetnet_checkpoint_path,output_loading_info=True;prophetnet_checkpoint_path,output_loading_info=True",Source code for Zalo AI 2021 submission
transformers.PerceiverTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,7ffdeb3a4424e4188c6a0089ea22e2f5,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py,/Users/NielsRogge/Documents/Perceiver/Tokenizer files,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,7cb84e29b5347ea68c6691bc69b2c65f,naver_gdc/gdc/dpg/gdc/examples/run.py,config,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.Wav2Vec2Config.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,7dfab839f0f7c41c2c42f0474b2518d3,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/jax-projects/wav2vec2/run_wav2vec2_pretrain_flax.py,"unknown,cache_dir=unknown,do_normalize=True;unknown,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,f22ad25e0d90418fb10c9bab029c3b6b,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/question-answering/run_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.RobertaTokenizer.from_pretrained,github.com/haoheliu/AudioLDM,2018,34cc90a554cd4fa6535f010de9cae6da,haoheliu_AudioLDM/AudioLDM/audioldm/clap/training/data.py,roberta-base,"AudioLDM: Generate speech, sound effects, music and beyond, with text."
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/andreamad8/FSB,119,dd631983f1745a9aa003b38845162371,andreamad8_FSB/FSB/demo/app.py,"model_checkpoint,low_cpu_mem_usage=True;gpt2;model_checkpoint;model_checkpoint",The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
transformers.T5Config.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/awslabs/pptod,148,42a2771343c6ea40e6e021efd5f131ba,awslabs_pptod/pptod/E2E_TOD/modelling/T5Model.py,"model_path;model_path,config=t5_config,resume_download=True;model_path",Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.RagRetriever.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,19d3d84cb22971048b0ed15b8b57a0a7,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag/eval_rag.py,"checkpoint,None=model_kwargs",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/kmeng01/memit,325,0cdeac09c9618f39020a9aad1025a0b8,kmeng01_memit/memit/experiments/causal_trace.py,"model_name;model_name,low_cpu_mem_usage=low_cpu_mem_usage,torch_dtype=torch_dtype",Mass-editing thousands of facts into a transformer memory (ICLR 2023)
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,3d9288e57894c7457e4ed7ba72f2e5e7,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_rel.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForMaskedLM.from_pretrained;transformers.TFAutoModelForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,17efd78992faebac3449904ff787a5f5,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/language-modeling/run_mlm.py,"checkpoint;unknown;unknown;unknown;unknown;checkpoint,config=config;unknown,config=config",Source code for Zalo AI 2021 submission
transformers.BertConfig.from_pretrained,github.com/salesforce/CodeRL,436,a1cc57281781d34afe523da3b4a1de45,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/bertabs/modeling_bertabs.py,bert-base-uncased,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c061a0897620b3c8dd2cc511796c8ab2,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_ham_ft.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,efc4d6e372a80e4325b78e09ca8712de,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/graphixv1.py,"unknown,use_fast=False",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.T5Tokenizer.from_pretrained,github.com/awslabs/pptod,148,9ba6c7d183fa87f1bef9b1697b3a0c3b,awslabs_pptod/pptod/IC/inference.py,pretrained_path,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
transformers.ClapModel.from_pretrained.to;transformers.AutoProcessor.from_pretrained,github.com/gitmylo/audio-webui,657,abd9f87b226350b2c0f3dfdcc1aca8f6,gitmylo_audio-webui/audio-webui/webui/modules/implementations/audioldm.py,"map_device;sanchit-gandhi/clap-htsat-unfused-m-full,cache_dir=cache_dir",A webui for different audio related Neural Networks
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,700d3a2a55be516fb71627a8f79cfe5a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/translation/run_translation.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelWithLMHead.from_pretrained,github.com/facebookresearch/KILT,859,071e0a9403bcdbf0e20c62868c0d95a0,facebookresearch_KILT/KILT/kilt/readers/t5/base_transformer.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir;unknown,config=unknown,cache_dir=cache_dir",Library for Knowledge Intensive Language Tasks
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,b8a609710656b762c3559598bfcd2618,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/scripts/demo.py,unknown;unknown;google/electra-large-discriminator;google/electra-large-discriminator,Multi-hop dense retrieval for question answering
transformers.CLIPVisionModel.from_pretrained;transformers.CLIPImageProcessor.from_pretrained;transformers.CLIPVisionModel.from_pretrained,github.com/tabtoyou/KoLLaVA,133,5cbbca796246527ccbe0e78329b3d405,tabtoyou_KoLLaVA/KoLLaVA/llava/model/llava.py,unknown;vision_tower;vision_tower,KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,8f5672b92f0738905cdfa865cc54b262,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/quantization-qdqbert/evaluate-hf-trt-qa.py,"unknown,use_fast=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,519c30d485a56198ad7d2c93a0bead4a,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/data_systhesis/snowball/run_refer.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.TFConvBertModel.from_pretrained,github.com/salesforce/CodeRL,436,e797e9a17cff97fd2c6c2caa7d194a31,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py,"pytorch_dump_path,from_pt=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoModelForImageClassification.from_pretrained;transformers.AutoFeatureExtractor.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,4223ca3a0f88e940c2cc2e051e7500a6,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/image-classification/run_image_classification.py,"unknown,num_labels=len,label2id=label2id,id2label=id2label,finetuning_task=image-classification,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/salesforce/CodeRL,436,82efd792e7fdd888290135337dc5829e,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/language-modeling/run_mlm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BartForConditionalGeneration.from_pretrained.eval;transformers.T5ForConditionalGeneration.from_pretrained,github.com/allenai/comet-atomic-2020,201,55fb384ebfebfb861fcf674cb4ce5c30,allenai_comet-atomic-2020/comet-atomic-2020/models/comet_atomic2020_bart/distillation.py,;unknown,
transformers.BigBirdTokenizerFast.from_pretrained,github.com/salesforce/CodeRL,436,c1cc1e2cd39804e6caeb0ecf3810e417,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/jax-projects/big_bird/train.py,unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizerFast.from_pretrained,github.com/zjunlp/OntoProtein,122,5c70338ea0d75b86f6530d5c428b2f00,zjunlp_OntoProtein/OntoProtein/run_downstream.py,"unknown,do_lower_case=False","Code and datasets for the ICLR2022 paper ""OntoProtein: Protein Pretraining With Gene Ontology Embedding"""
transformers.DPRContextEncoderTokenizerFast.from_pretrained,github.com/salesforce/CodeRL,436,004ba273c403115feafa88bd5bfa71a1,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag-end2end-retriever/kb_encode_utils.py,facebook/dpr-ctx_encoder-multiset-base,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizer.from_pretrained,github.com/Yui010206/SeViLA,121,d0ea3cfaec6c61d23d2669314f2b075e,Yui010206_SeViLA/SeViLA/lavis/models/blip_models/blip.py,bert-base-uncased,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.T5Config.from_pretrained;transformers.T5Config.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,09ac6f2a8cf43e0e16c7cb42f1a0c7d3,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/ssll/pretrain.py,"unknown,cache_dir=unknown,use_fast=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,vocab_size=len,use_auth_token=unknown;unknown,cache_dir=unknown,use_auth_token=unknown;t5-base,cache_dir=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ba1612a5694d3a028b1a4d9d11abf87a,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/testseen.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Tokenizer.from_pretrained,github.com/devjwsong/gpt2-dialogue-generation-pytorch,163,330526424f61408d614a86dd1ca9caa0,devjwsong_gpt2-dialogue-generation-pytorch/gpt2-dialogue-generation-pytorch/src/load_data.py,unknown,The PyTorch implementation of fine-tuning the GPT-2(Generative Pre-Training 2) for dialogue generation.
transformers.Wav2Vec2Config.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,901d07995299a1055d4370809377ba72,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.tokenization_bert.BertTokenizer.from_pretrained,github.com/v-mipeng/LexiconAugmentedNER,418,a9f8f070b07fbf0a558c509a0b8adcb7,v-mipeng_LexiconAugmentedNER/LexiconAugmentedNER/utils/functions.py,"bert-base-chinese,do_lower_case=True",Reject complicated operations for incorporating lexicon for Chinese NER. 
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,6c1e327979f2b1cc3cd058e1aec42968,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.FlaxAutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,d1640f21044c4f7a2e01d2ad9b9f3be5,salesforce_CodeRL/CodeRL/transformers/examples/flax/text-classification/run_flax_glue.py,"unknown,num_labels=num_labels,finetuning_task=unknown;unknown,use_fast=unknown;unknown,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,22a0ec8b82c47b766427fd0cfaa16f82,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/zero-shot-distillation/distill_classifier.py,"model_path;model_path,use_fast=use_fast_tokenizer;unknown,num_labels=len;unknown,use_fast=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,c4142da3338ab82deecde81be77cdebf,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/utils/processor/table_truncate.py,pretrained_model_name_or_path=facebook/bart-large,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,41e1a69649bd632d8961e89efd9b8b7b,ashkamath_mdetr/mdetr/datasets/phrasecut.py,unknown,
transformers.Wav2Vec2Processor.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained,github.com/jonatasgrosman/wav2vec2-sprint,143,ed22202ef9c0b348368588d09e8fa3cb,jonatasgrosman_wav2vec2-sprint/wav2vec2-sprint/common_voice_eval.py,MODEL_ID;MODEL_ID,
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForTokenClassification.from_pretrained,github.com/salesforce/CodeRL,436,50a985418e09ab49c9e77467cbd501c9,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/token-classification/run_ner.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;tokenizer_name_or_path,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown,add_prefix_space=True;tokenizer_name_or_path,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertConfig.from_pretrained,github.com/IBM/zshot,285,27ad1de223f779242ee83b7b3dae9ab8,IBM_zshot/zshot/zshot/relation_extractor/zsrc/zero_shot_rel_class.py,"bert-large-cased,num_labels=2,finetuning_task=fewrel-zero-shot,device=device",Zero and Few shot named entity & relationships recognition
transformers.BertTokenizer.from_pretrained;transformers.BertForSequenceClassification.from_pretrained,github.com/NorskRegnesentral/skweak,899,558331e0221bb38e60ff0c4dbe54760e,NorskRegnesentral_skweak/skweak/examples/sentiment/transformer_model.py,"ltgoslo/norbert;unknown,num_labels=3",skweak: A software toolkit for weak supervision applied to NLP tasks
transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,707e36c714f133d5438c770b77b98a68,yangheng95_PyABSA/PyABSA/pyabsa/tasks/TextClassification/instructor/classifier_instructor.py,unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained,github.com/allenai/comet-atomic-2020,201,e0aa213671ecfea27ccd06dc158b9f16,allenai_comet-atomic-2020/comet-atomic-2020/models/comet_atomic2020_gpt2/comet_gpt2.py,"model_name;unknown;model_name,use_cdn=False",
transformers.GPT2TokenizerFast.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained.to,github.com/thunlp/OpenBackdoor,114,799e136bdbc48a46d116f59f6c1c50fa,thunlp_OpenBackdoor/OpenBackdoor/openbackdoor/defenders/onion_defender.py,gpt2;unknown,"An open-source toolkit for textual backdoor attack and defense (NeurIPS 2022 D&B, Spotlight)"
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,b723ef2b2af3ae149ead66f31d19259b,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/translation/run_translation_no_trainer.py,"unknown;unknown;unknown,use_fast=unknown;unknown,use_fast=unknown;unknown,from_tf=bool,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,f96f39ceb1e4af117e3d80a1b656e4c2,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/cgodial/slot_based_dialog/chinese_t5/utils.py,./t5_chinese_small,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.CLIPTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,e6c323af907383548538cf2ec6e75736,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py,openai/clip-vit-base-patch32,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,6e3de9e37f27c69646e4004c8051655b,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/scripts/tabfact/run_end2end.py,pretrained_model_name_or_path=../../utils_file/gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/HazyResearch/domino,129,1bed3aaf71fa767614582aa7ff66bfec,HazyResearch_domino/domino/domino/_embed/transformers.py,variant;variant,
transformers.AutoConfig.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/princeton-nlp/MeZO,883,d9bf4b2c659e607f90b322670e7e790e,princeton-nlp_MeZO/MeZO/large_models/run.py,"unknown;unknown,config=config;unknown,config=config,device_map=auto,torch_dtype=torch_dtype,max_memory=Dict,load_in_8bit=unknown;unknown,use_fast=False",[NeurIPS 2023] MeZO: Fine-Tuning Language Models with Just Forward Passes. https://arxiv.org/abs/2305.17333
transformers.BertTokenizer.from_pretrained;transformers.BartConfig.from_pretrained;transformers.BartConfig.from_pretrained,github.com/fastnlp/CPT,448,9029f2c6c4f6c7b72915025b5f82c16d,fastnlp_CPT/CPT/finetune/ner/train_onto.py,hfl/chinese-bert-wwm;ptm;ptm,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.Wav2Vec2Config.from_pretrained;transformers.Speech2Text2Config.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,65c6ec9949999c0282b7c2c4b077016d,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/speech_encoder_decoder/convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py,"encoder_config_path;decoder_config_path,vocab_size=vocab_size,decoder_layers=num_decoder_layers,do_stable_layer_norm=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained,github.com/princeton-nlp/MeZO,883,1d0f6d92627f20f469fc0acc3c794858,princeton-nlp_MeZO/MeZO/medium_models/src/prefix.py,roberta-base,[NeurIPS 2023] MeZO: Fine-Tuning Language Models with Just Forward Passes. https://arxiv.org/abs/2305.17333
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,17bd0718c03209cf7d5b08a9f7bb568a,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dial-start/test.py,"bert-base-uncased,do_lower_case=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.XLNetConfig.from_pretrained;transformers.XLNetTokenizerFast.from_pretrained;transformers.XLNetForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,32389ffe61c7831265d0421409284800,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/question-answering/run_qa_beam_search.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,330738ef43a0ae936f21807cbe52bb31,ylsung_VL_adapter/VL_adapter/VL-T5/src/pretrain_raw_data.py,"unknown,do_lower_case=unknown;unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/RUC-GSAI/YuLan-IR,181,35d7276b0df29e0cb70901a7cfbbb55b,RUC-GSAI_YuLan-IR/YuLan-IR/RETA-LLM/indexer/train_dam_module.py,"unknown;unknown,config=config,use_fast=False;unknown,config=config",YuLan-IR: Information Retrieval Boosted LMs
transformers.RobertaTokenizer.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/salesforce/CodeRL,436,ab4ea1750b58dd750b912d443e42d1a5,salesforce_CodeRL/CodeRL/generate.py,"Salesforce/codet5-base,cache_dir=unknown;unknown,tuning_mode=critic;unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.RobertaPreLayerNormConfig.from_pretrained;transformers.RobertaPreLayerNormForMaskedLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,3c29709d0be2898c98f4fd0c8e3543e1,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/roberta_prelayernorm/convert_roberta_prelayernorm_original_pytorch_checkpoint_to_pytorch.py,"checkpoint_repo,architectures=List;pretrained_model_name_or_path=None,config=config,state_dict=state_dict;checkpoint_repo","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,e42201c38d74902709a39e1117ae607a,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_gease.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,fc974f0bdb6113f38aeb5cd1dc01a9b2,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag/lightning_base.py,"unknown,None=unknown,cache_dir=cache_dir,None=config_kwargs;unknown,cache_dir=cache_dir",Source code for Zalo AI 2021 submission
transformers.AutoModelForSeq2SeqLM.from_pretrained.eval;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,e6a32eacdedd88f16a2f770a1b366512,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/seq2seq-distillation/distillation.py,;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,a73a5e9c5f72c9a00d6acce29f519aa4,ylsung_VL_adapter/VL_adapter/VL-T5/src/nlvr_clip_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,bc3985b7cf3d5756fdb77a5974c10b84,naver_gdc/gdc/rm_vs_dm/gdc/examples/run-distributional.py,config,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/csinva/gpt-paper-title-generator,132,be8ac69453bcea443373096c3f803704,csinva_gpt-paper-title-generator/gpt-paper-title-generator/gptneo/02_finetune_hf.py,checkpoint;checkpoint,Generating paper titles (and more!) with GPT trained on data scraped from arXiv.
transformers.DetrForObjectDetection.from_pretrained,github.com/qanastek/HugsVision,185,ca6dccd751f3e850ccec8f6dad7bab11,qanastek_HugsVision/HugsVision/build/lib/hugsvision/models/Detr.py,model_path,HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.RobertaTokenizerFast.from_pretrained;transformers.RobertaModel.from_pretrained,github.com/antoyang/TubeDETR,140,852080f63f7d952401f12ac928da48f5,antoyang_TubeDETR/TubeDETR/models/transformer.py,"text_encoder_type,local_files_only=True;text_encoder_type,local_files_only=True",[CVPR 2022 Oral] TubeDETR: Spatio-Temporal Video Grounding with Transformers
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,54c6770b3e9203e6f350a03483d9485f,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/memory.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.AutoConfig.from_pretrained,github.com/yangheng95/PyABSA,779,9001ba7444035834003e7a9cf7936ba3,yangheng95_PyABSA/PyABSA/pyabsa/tasks/_Archive/RNAClassification/models/__classic__/mhsa.py,bert-base-uncased,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,346b3fa607b75ab112e8fe28f7cdb1f0,yangheng95_PyABSA/PyABSA/pyabsa/tasks/RNARegression/prediction/rna_regressor.py,find_cwd_dir;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.BigBirdTokenizerFast.from_pretrained,github.com/salesforce/CodeRL,436,5c1e3b28bb8756c404764a135820f406,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/jax-projects/big_bird/evaluate.py,model_id,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.BertTokenizer.from_pretrained,github.com/airsplay/vokenization,187,b82ad865f496878e0fd95074c3480103,airsplay_vokenization/vokenization/vokenization/vokenization.py,bert-base-uncased,"PyTorch code for EMNLP 2020 Paper ""Vokenization: Improving Language Understanding with Visual Supervision"""
transformers.RobertaTokenizer.from_pretrained;transformers.RobertaTokenizer.from_pretrained;transformers.RobertaTokenizerFast.from_pretrained,github.com/memray/OpenNMT-kpg-release,210,071c5aead899766b620f3b41f9bba12f,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/tokenizer_test.py,"roberta-base,dropout=0.5;roberta-base;roberta-base,__slow_tokenizer=tokenizer,tokenizer_file=None,vocab_file=unknown,merges_file=unknown",Keyphrase Generation
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/tabtoyou/KoLLaVA,133,b5fb5b0ec57652a00938b3fa36ece2a5,tabtoyou_KoLLaVA/KoLLaVA/llava/model/make_delta.py,"base_model_path,torch_dtype=unknown,low_cpu_mem_usage=True;target_model_path,torch_dtype=unknown,low_cpu_mem_usage=True;target_model_path",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,12956616b13f267014f606d2ddf450ff,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/fsner/src/fsner/tokenizer_utils.py,pretrained_model_name_or_path,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,e187577a651b7a92145c18374bbd4d35,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/seq2seq/old_test_datasets.py,"tok_name;tok;facebook/mbart-large-cc25;MARIAN_TINY;tok_name,use_fast=False",Source code for Zalo AI 2021 submission
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,c26e15911de661be81a70010d13637d1,ylsung_VL_adapter/VL_adapter/VL-T5/src/mmt_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/IBM/zshot,285,2919f8d6afa0d0f8a12744a69df771ef,IBM_zshot/zshot/zshot/linker/linker_regen/linker_regen.py,"MODEL_NAME,cache_dir=MODELS_CACHE_PATH;MODEL_NAME,model_max_length=1024,cache_dir=MODELS_CACHE_PATH",Zero and Few shot named entity & relationships recognition
transformers.FSMTTokenizer.from_pretrained;transformers.FSMTForConditionalGeneration.from_pretrained.to,github.com/salesforce/CodeRL,436,c2e9d4272f34cb713f8c51664a73615e,salesforce_CodeRL/CodeRL/transformers/examples/legacy/seq2seq/old_test_fsmt_bleu_score.py,mname;torch_device,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForTokenClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,af7c028c94f9f9e4f9f2d0dac93e858f,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/token-classification/run_ner.py,"unknown,num_labels=num_labels,id2label=label_map,label2id=Dict,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,0968a7fd754cd4d8cd81c62e0eba435e,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/text-classification/run_xnli.py,"unknown,num_labels=num_labels,finetuning_task=xnli,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,do_lower_case=unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.modeling_bart.BartForConditionalGeneration.from_pretrained;transformers.tokenization_bart.BartTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,00e2f388f3899f83d33baa7876750eb5,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/star/data_systhesis/snowball/generator/models/relogic.py,pretrain_config;pretrain_config,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,13b2384976533dd4311297e44a9389d0,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/src/accelerate/test_utils/scripts/external_deps/test_metrics.py,"hf-internal-testing/mrpc-bert-base-cased;hf-internal-testing/mrpc-bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.DPRQuestionEncoderTokenizer.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,6603bc89735e5506a0491991a7732703,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag/test_distributed_retriever.py,os.path.join;os.path.join,Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelWithLMHead.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,c7b7057a3b5318baed183e0ece72e151,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/run_language_modeling.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown",Source code for Zalo AI 2021 submission
transformers.AutoModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,79e5b7e0f199aaf60b83702d88456f0a,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/sunsql/model/encoder/graph_input.py,os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained;transformers.AutoModel.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained;transformers.AutoModel.from_pretrained,github.com/labteral/ernie,197,fb37933a0aba899f523322f861f912fe,labteral_ernie/ernie/ernie/ernie.py,"unknown;unknown;model_path;model_path;model_path,from_pt=False;model_name,None=tokenizer_kwargs;model_name;model_name,from_pt=False;model_name,from_pt=True;model_name;temporary_path,from_pt=True;model_name",Simple State-of-the-Art BERT-Based Sentence Classification with Keras / TensorFlow 2. Built with HuggingFace's Transformers.
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/neulab/knn-transformers,247,5bed954c1f392f200d723b2ca1d7b07a,neulab_knn-transformers/knn-transformers/run_clm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown","PyTorch + HuggingFace code for RetoMaton: ""Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval"" (ICML 2022), including an implementation of kNN-LM and kNN-MT"
transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,f61faa54a5e25b1d952b6177b980baec,salesforce_CodeRL/CodeRL/transformers/examples/legacy/token-classification/scripts/preprocess.py,model_name_or_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForTokenClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForTokenClassification.from_pretrained,github.com/ashawkey/stable-dreamfusion,7150,bbd08a49f093a66b87e1ae16c78843fa,ashawkey_stable-dreamfusion/stable-dreamfusion/evaluation/Prompt.py,Voicelab/vlt5-base-keywords;Voicelab/vlt5-base-keywords;yanekyuk/bert-uncased-keyword-extractor;yanekyuk/bert-uncased-keyword-extractor;jasminejwebb/KeywordIdentifier;jasminejwebb/KeywordIdentifier,Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion.
transformers.AutoConfig.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,adeb9b3afb63deed18e86a8250eea5f6,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/seq2seq-distillation/_test_make_student.py,TINY_BART,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,9f6988d75fbed7e5c3e5cc459ccf7351,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dial2vec/data/data_provider.py,unknown;unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/Akegarasu/ChatGLM-webui,1825,61e197fe799f560404f599d126aa91ee,Akegarasu_ChatGLM-webui/ChatGLM-webui/modules/model.py,"unknown,trust_remote_code=True;unknown,trust_remote_code=True;unknown,config=config,trust_remote_code=True",A WebUI for ChatGLM-6B
transformers.T5TokenizerFast.from_pretrained,github.com/Yui010206/SeViLA,121,45cea48fdac4dce0d4567af080357998,Yui010206_SeViLA/SeViLA/lavis/models/blip2_models/blip2_t5.py,t5_model,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.XLMRobertaTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained;transformers.MLukeTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,519d96efff50e97faf960be40e93d9c2,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py,"metadata;pytorch_dump_folder_path;pytorch_dump_folder_path,task=entity_classification;pytorch_dump_folder_path",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.Wav2Vec2Processor.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained.to,github.com/AIGC-Audio/AudioGPT,9397,2e86b32dc33151455b60b5564c565228,AIGC-Audio_AudioGPT/AudioGPT/NeuralSeq/inference/tts/base_tts_infer.py,facebook/wav2vec2-base-960h;unknown,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,7abeff695fad7e3c3defca3b6627f9d8,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/scripts/wtq/run_row.py,pretrained_model_name_or_path=../../utils_file/gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/michiyasunaga/LinkBERT,369,4b75264ac703ca4a959787b05eb5b627,michiyasunaga_LinkBERT/LinkBERT/src/mc/run_multiple_choice.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",[ACL 2022] LinkBERT: A Knowledgeable Language Model 😎 Pretrained with Document Links
transformers.AutoConfig.from_pretrained,github.com/yangheng95/PyABSA,779,291e189cf47dade5c1e1f9c059141bc5,yangheng95_PyABSA/PyABSA/pyabsa/tasks/_Archive/ProteinRegression/models/__classic__/mhsa.py,bert-base-uncased,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.JukeboxConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,c6bffc79af29a9de1cc08628c58084c7,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/jukebox/convert_jukebox.py,model_name,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,105db9a67da518b887f9eae2fb3a05dc,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/translation/run_translation.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoModelForMaskedLM.from_pretrained,github.com/RUC-GSAI/YuLan-IR,181,3c726295aa2c864fe95901a4db34d6e8,RUC-GSAI_YuLan-IR/YuLan-IR/WebBrain/retriever/models/regen_retriever.py,plm_dir,YuLan-IR: Information Retrieval Boosted LMs
transformers.BertTokenizerFast.from_pretrained,github.com/salesforce/CodeRL,436,feed24e2cdb276f9e6cb26edc42dc976,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/vilt/processing_vilt.py,"pretrained_model_name_or_path,None=kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5Tokenizer.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained,github.com/clue-ai/ChatYuan,1859,1154e3f0ab378c77c1885527986f898e,clue-ai_ChatYuan/ChatYuan/app_gradio.py,ClueAI/ChatYuan-large-v2;ClueAI/ChatYuan-large-v2,ChatYuan: Large Language Model for Dialogue in Chinese and English
transformers.BertForSequenceClassification.from_pretrained,github.com/zjunlp/OntoProtein,122,461dbdc5e54febabac50eda7724d3413,zjunlp_OntoProtein/OntoProtein/src/test.py,data/output_data/checkpoint-1171/protein,"Code and datasets for the ICLR2022 paper ""OntoProtein: Protein Pretraining With Gene Ontology Embedding"""
transformers.T5Tokenizer.from_pretrained,github.com/Yui010206/SeViLA,121,594d3ac6d5f153202646774a6bc0d033,Yui010206_SeViLA/SeViLA/lavis/models/pnp_vqa_models/pnp_unifiedqav2_fid.py,model_path,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.RobertaTokenizer.from_pretrained;transformers.LukeTokenizer.from_pretrained;transformers.LukeTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,21fbec6eb303a06249a97a09c222b900,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py,"metadata;pytorch_dump_folder_path;pytorch_dump_folder_path,task=entity_classification",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.XLNetLMHeadModel.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,8d45b8497c3b29bf3607f385a49e0c3a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/language-modeling/run_plm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained,github.com/the-crypt-keeper/can-ai-code,308,3e0a24e48ac51ab7651595fdbabd9fb5,the-crypt-keeper_can-ai-code/can-ai-code/interview-autogptq-modal.py,"quantized_model_dir,use_fast=False",Self-evaluating interview for AI coders
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/salesforce/CodeRL,436,367ed70b5e884d08aa210c24518c1b06,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/translation/run_translation.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,59703164f23edc3202362ec04566278b,ylsung_VL_adapter/VL_adapter/VL-T5/src/video/tvc_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.RagConfig.from_pretrained;transformers.RagTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,80b57d4d8d12ba6fbb96d1675bc52697,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag/distributed_ray_retriever.py,"retriever_name_or_path,None=kwargs;retriever_name_or_path,config=config",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,9bf301b7690c70a7c15e9dd5cc2f6076,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/codeparrot/scripts/validation_loss.py,unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/bofenghuang/vigogne,448,af8e387fb83bfc8c136c3b726aeee3ed,bofenghuang_vigogne/vigogne/vigogne/train/train_sft.py,"unknown,torch_dtype=unknown,device_map=Dict,quantization_config=quantization_config,trust_remote_code=unknown,use_cache=unknown;unknown,padding_side=right,use_fast=False",French instruction-following and chat models
transformers.LlamaTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/Vahe1994/SpQR,466,1bbd4033bed2412a9fad47955a5eaf6c,Vahe1994_SpQR/SpQR/datautils.py,"model_path,use_fast=False;model_path,use_fast=False",
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/airsplay/vokenization,187,6e9192a1beec4944f279c47c882fece9,airsplay_vokenization/vokenization/vlm/run_glue.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown;unknown,do_lower_case=unknown,cache_dir=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown;unknown;unknown;unknown,do_lower_case=unknown;checkpoint","PyTorch code for EMNLP 2020 Paper ""Vokenization: Improving Language Understanding with Visual Supervision"""
transformers.RobertaModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,d5bf59c909cdea3a59327bc6dbe90318,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/r2sql/sparc/reranker/model.py,roberta-base,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizer.from_pretrained,github.com/fastnlp/CPT,448,48b623a0ef508f3b45be19917a186713,fastnlp_CPT/CPT/finetune/generation/run_bleu.py,/path/to/model,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForTokenClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,ef7e298fde7b7d348ced925355d6b71f,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/token-classification/run_ner_no_trainer.py,"unknown,num_labels=num_labels;unknown,num_labels=num_labels;tokenizer_name_or_path,use_fast=True,add_prefix_space=True;tokenizer_name_or_path,use_fast=True;unknown,from_tf=bool,config=config",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/michiyasunaga/LinkBERT,369,69f90c3e5ac04debea42d4339018b36c,michiyasunaga_LinkBERT/LinkBERT/src/qa/run_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",[ACL 2022] LinkBERT: A Knowledgeable Language Model 😎 Pretrained with Document Links
transformers.BertConfig.from_pretrained,github.com/fastnlp/CPT,448,23fe2dfeeee2e9aa47fae92d89c53eef,fastnlp_CPT/CPT/pretrain/megatron/model/bert_model.py,bert-base-cased,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,f85499b90315bcf985ecc484fe59b004,salesforce_CodeRL/CodeRL/transformers/examples/legacy/seq2seq/save_len_file.py,tokenizer_name,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5Tokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,190f7cd90720b72627fe2ddf3f6f1092,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/ssll/unitrain.py,"t5-base,cache_dir=out",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.TextClassificationPipeline.from_pretrained,github.com/songhaoyu/BoB,134,edcd9b992ddb00557e8665181738363b,songhaoyu_BoB/BoB/xlibs/commands/train.py,unknown,The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained,github.com/salesforce/CodeRL,436,1f1d2b039d968177ea088e6c23081ebf,salesforce_CodeRL/CodeRL/transformers/examples/tensorflow/text-classification/run_text_classification.py,"config_path,num_labels=num_labels,cache_dir=unknown,revision=unknown,use_auth_token=unknown;config_path,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;model_path,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/yuchenlin/SwiftSage,164,633abc1f89e768ee78b3944c9f6d7573,yuchenlin_SwiftSage/SwiftSage/eval_utils.py,args;args,SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks 
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,0d5416d9055cec6a5c70ecf025702286,ashkamath_mdetr/mdetr/datasets/clevrref.py,unknown,
transformers.Wav2Vec2Processor.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained.to,github.com/YuanGongND/whisper-at,199,7bd9d092bb894356222cc8b7e656686b,YuanGongND_whisper-at/whisper-at/src/noise_robust_asr/asr_experiments/transcribe_wav2vec_robust.py,facebook/wav2vec2-large-robust-ft-swbd-300h;device,"Code and Pretrained Models for Interspeech 2023 Paper ""Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong Audio Event Taggers"""
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained.eval.cuda,github.com/kmeng01/memit,325,66ce62c9ba6e697ea812e4a406895352,kmeng01_memit/memit/rome/layer_stats.py,unknown;,Mass-editing thousands of facts into a transformer memory (ICLR 2023)
transformers.BertForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,b278a3a49deecefaa292117c77351c46,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/distillation/scripts/extract_distilbert.py,unknown,Source code for Zalo AI 2021 submission
transformers.AutoProcessor.from_pretrained;transformers.Blip2ForConditionalGeneration.from_pretrained,github.com/sail-sg/EditAnything,2813,b0dc533f6382db615aae27d70d286604,sail-sg_EditAnything/EditAnything/tools/sam2image_ori_version.py,"Salesforce/blip2-opt-2.7b;Salesforce/blip2-opt-2.7b,torch_dtype=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.AutoModelForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,76ba50cae82a13f7b7432cf283884ea2,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/Condenser/modeling.py,"unknown,None=kwargs",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,c0ecc2aae95f2d8d4a39c726bd8dec50,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/by_feature/multi_process_metrics.py,"bert-base-cased;bert-base-cased,return_dict=True",We perform functional grounding of LLMs' knowledge in BabyAI-Text
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,2fac1c8f7343a5d842facf0158b3e7bf,ashkamath_mdetr/mdetr/datasets/vg.py,unknown,
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ucinlp/autoprompt,497,e7ae734273a5ec8d1c8f38034a47af98,ucinlp_autoprompt/autoprompt/autoprompt/run_linear_probe.py,"unknown,num_labels=unknown;unknown",AutoPrompt: Automatic Prompt Construction for Masked Language Models.
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/memray/OpenNMT-kpg-release,210,d0c28cbfb63ac9b3a1b95a7c946993dc,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/newssum/bart/example.py,"gpt2,cache_dir=/export/share/rmeng/output/pretrain_cache/;roberta-base,cache_dir=/export/share/rmeng/output/pretrain_cache/",Keyphrase Generation
transformers.BertModel.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,6d93a98da957d264d36665ff12e3c934,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py,"pretrained_model_name_or_path=unknown,state_dict=torch.load,cache_dir=unknown","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertTokenizerFast.from_pretrained,github.com/IBM/zshot,285,6cbdee7fc3065e4cde23849fd2ff8d23,IBM_zshot/zshot/zshot/relation_extractor/zsrc/data_helper.py,"bert-base-cased,do_lower_case=False",Zero and Few shot named entity & relationships recognition
transformers.QDQBertConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.QDQBertForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,f96d021dc98030ff2c9378d0d3fa2c0a,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/quantization-qdqbert/run_quant_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,ee782ff0f42c9312a264f67c69ea5cda,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/legacy/seq2seq/save_len_file.py,tokenizer_name,Source code for Zalo AI 2021 submission
transformers.models.roberta.modeling_roberta.RobertaModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,5760bca2bc57f6ae64ccf8c51a368ebd,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/spectra/modeling_spectra/model.py,"text,config=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,91ddebd863de845046f9f9dc64783a44,yangheng95_PyABSA/PyABSA/pyabsa/tasks/TextAdversarialDefense/instructor/tad_instructor.py,unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.ViTMAEConfig.from_pretrained;transformers.ViTMAEConfig.from_pretrained;transformers.ViTFeatureExtractor.from_pretrained;transformers.ViTFeatureExtractor.from_pretrained;transformers.ViTMAEForPreTraining.from_pretrained,github.com/salesforce/CodeRL,436,51e0aed73ec25894d7dd8fe52bdb965d,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/image-pretraining/run_mae.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained,github.com/salesforce/CodeRL,436,dcdbd29eb3c1e119ceec02829031c629,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/codeparrot/scripts/initialize_model.py,"unknown;unknown,None=config_kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,68a9bbef692d15156bc78aba6954be98,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/language-modeling/run_clm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.RobertaTokenizerFast.from_pretrained,github.com/ashkamath/mdetr,903,97fd816c774f2d5f5dbb8ad6aa4993a1,ashkamath_mdetr/mdetr/datasets/flickr.py,unknown,
transformers.AutoTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,856103deef6baaa452da040d7a741d83,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py,tokenizer_model_name,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.HubertModel.from_pretrained,github.com/zhvng/open-musiclm,398,068cc9a98194857296628ced9ce31fe9,zhvng_open-musiclm/open-musiclm/open_musiclm/hf_hubert_kmeans.py,model_name,"Implementation of MusicLM, a text to music model published by Google Research, with a few modifications."
transformers.HubertConfig.from_pretrained,github.com/salesforce/CodeRL,436,262586dea6c499f8c574a5b18ae436e0,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,573c8c9782f81a4578b88b0c15250e0e,ylsung_VL_adapter/VL_adapter/VL-T5/src/caption_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.BartConfig.from_pretrained;transformers.BartTokenizer.from_pretrained.encode.unsqueeze,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,f2220661a8efb28006181095f239a748,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py,hf_checkpoint_name;0,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained;transformers.RobertaTokenizer.from_pretrained;transformers.RobertaModel.from_pretrained;transformers.BartTokenizer.from_pretrained;transformers.BartModel.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,ba7adfcd9f44addadaa9b07e42dc8baa,AIGC-Audio_AudioGPT/AudioGPT/text_to_audio/Make_An_Audio/ldm/modules/encoders/open_clap/bert.py,bert-base-uncased;bert-base-uncased;roberta-base;roberta-base;facebook/bart-base;facebook/bart-base,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.BertTokenizer.from_pretrained,github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis,506,a2a418fe0ac4f33f35940692f360d637,ScalaConsultants_Aspect-Based-Sentiment-Analysis/Aspect-Based-Sentiment-Analysis/examples/train_classifier.py,base_model_name,💭 Aspect-Based-Sentiment-Analysis: Transformer & Explainable ML (TensorFlow)
transformers.BertTokenizer.from_pretrained,github.com/Yui010206/SeViLA,121,b50cc8d956ec6e681102c93c987fd636,Yui010206_SeViLA/SeViLA/lavis/models/blip2_models/blip2.py,bert-base-uncased,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/RUC-GSAI/YuLan-IR,181,25d6e868fd4b03dfbb0a1a36d0ca6a70,RUC-GSAI_YuLan-IR/YuLan-IR/WebBrain/retriever/utils/args.py,unknown;unknown,YuLan-IR: Information Retrieval Boosted LMs
transformers.BartForConditionalGeneration.from_pretrained;transformers.PreTrainedTokenizerFast.from_pretrained;transformers.PreTrainedTokenizerFast.from_pretrained,github.com/seujung/KoBART-summarization,179,42f0ecae04e64458bfa8956dbd74d879,seujung_KoBART-summarization/KoBART-summarization/train.py,gogamza/kobart-base-v1;gogamza/kobart-base-v1;gogamza/kobart-base-v1,Summarization module based on KoBART
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.TFAutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,0bbe41ae906f9e24b072e61434023d3a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/tensorflow/text-classification/run_text_classification.py,"config_path,num_labels=num_labels,cache_dir=unknown,revision=unknown,use_auth_token=unknown;config_path,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;model_path,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.SwinConfig.from_pretrained;transformers.SwinConfig.from_pretrained;transformers.DinatConfig.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTokenizer.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,2e3ea82c401bcecb04d432661ef08755,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/oneformer/convert_to_hf_oneformer.py,"microsoft/swin-tiny-patch4-window7-224,drop_path_rate=unknown,out_features=List;microsoft/swin-large-patch4-window12-384,drop_path_rate=unknown,out_features=List;shi-labs/dinat-large-11x11-in22k-in1k-384,dilations=unknown,kernel_size=unknown,out_features=List;model_repo;model_repo","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained.to,github.com/INK-USC/MHGRN,238,22f44d10bca0359b8e4380436cd89c40,INK-USC_MHGRN/MHGRN/utils/extract_bert_node_features.py,"bert-large-uncased,do_lower_case=True;device",Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering (EMNLP 2020)
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,9d52dca8ba0960dee2bb28d02807710c,ylsung_VL_adapter/VL_adapter/VL-T5/src/video/tvr_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.BertModel.from_pretrained,github.com/salesforce/CodeRL,436,35855be5a4cc114a140fd1ec32bf0db8,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py,"pretrained_model_name_or_path=unknown,state_dict=torch.load,cache_dir=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/microsoft/DialoGPT,2269,514af3a6c2aaef24086daa75232057b9,microsoft_DialoGPT/DialoGPT/gradiodemo.py,microsoft/DialoGPT-large;microsoft/DialoGPT-large,Large-scale pretraining for dialogue
transformers.GPT2LMHeadModel.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/salesforce/CodeRL,436,ef535da4ed67f8d1f95425cdd40472c0,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/pplm/run_pplm.py,"pretrained_model,output_hidden_states=True;pretrained_model",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoFeatureExtractor.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoModelForAudioClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,2e01bf20b42a4fd81aecbe2266da3495,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/audio-classification/run_audio_classification.py,"unknown,return_attention_mask=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,num_labels=len,label2id=label2id,id2label=id2label,finetuning_task=audio-classification,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,081fb6654dd9794d431fbc24a5b01484,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/text-classification/run_glue.py,"unknown,num_labels=num_labels,finetuning_task=unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.PLBartConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,fa020154483cc017c1c4dda66bccd74e,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/plbart/convert_plbart_original_checkpoint_to_torch.py,"hf_config_path,vocab_size=vocab_size","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.BertTokenizer.from_pretrained,github.com/LearnedVector/A-Hackers-AI-Voice-Assistant,886,19296c5068546fb5e50e8833008aa73d,LearnedVector_A-Hackers-AI-Voice-Assistant/A-Hackers-AI-Voice-Assistant/VoiceAssistant/nlu/neuralnet/config.py,"BASE_MODEL,do_lower_case=True","A hackers AI voice assistant, built using Python and PyTorch."
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,e02e083717ba52ac1ed9673235b5cacc,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/Condenser/run_pre_training.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown;unknown,cache_dir=unknown,use_fast=False;unknown,cache_dir=unknown,use_fast=False",Source code for Zalo AI 2021 submission
transformers.BertTokenizer.from_pretrained,github.com/declare-lab/conv-emotion,1210,0677679adf75e1a95f0c0fb62a7a0656,declare-lab_conv-emotion/conv-emotion/emotion-cause-extraction/Rank-Emotion-Cause/src/data_loader.py,unknown,This repo contains implementation of different architectures for emotion recognition in conversations.
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,ba0c6c2a8fa60a6aef329da8c82a6d04,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/text2sql/scripts/annotate_sql_program_wikitq.py,pretrained_model_name_or_path=os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoModel.from_pretrained;transformers.AutoModel.from_pretrained,github.com/yangheng95/PyABSA,779,dde79f8fdcab72f17f52c44807fe1ea7,yangheng95_PyABSA/PyABSA/pyabsa/tasks/_Archive/RNAClassification/prediction/rna_classifier.py,find_cwd_dir;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
transformers.LongformerModel.from_pretrained;transformers.LongformerForQuestionAnswering.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,a8de905a9321399f427e6ffb90d62301,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py,longformer_model;longformer_model,Source code for Zalo AI 2021 submission
transformers.AutoModel.from_pretrained.to;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,cb19cd53e9a83293b233bb2530900914,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/proton/preprocess/test_grappa_prob.py,device;os.path.join,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.LlamaForCausalLM.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/tabtoyou/KoLLaVA,133,aea840921064c2a1546ae1efabd9dcbe,tabtoyou_KoLLaVA/KoLLaVA/llava/train/train.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown,model_max_length=unknown,padding_side=right;unknown,cache_dir=unknown,model_max_length=unknown,padding_side=right,use_fast=False",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.DPRQuestionEncoderTokenizer.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,a4b8d3965d36b8d5153bb4aa30b902bd,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/rag/test_distributed_retriever.py,os.path.join;os.path.join,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained,github.com/karthikv792/LLMs-Planning,112,4b26424b998bfcc65ef913d722efb3a3,karthikv792_LLMs-Planning/LLMs-Planning/plan-bench/response_generation.py,"bigscience/bloom;bigscience/bloom,cache_dir=cache_dir,local_files_only=False,load_in_8bit=True,device_map=auto,max_memory=max_memory_mapping",An extensible benchmark for evaluating large language models on planning
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,56d6eb8271245181c3b9a296c44a7ebc,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/bird/finetuning/models/unified/rgat_adapt.py,"unknown,use_fast=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.modeling_gpt2.GPT2LMHeadModel.from_pretrained;transformers.GPT2Tokenizer.from_pretrained,github.com/allenai/real-toxicity-prompts,125,82bd2efac2de5a8102abaac4404d679e,allenai_real-toxicity-prompts/real-toxicity-prompts/generation/pplm_generation.py,"pretrained_model,output_hidden_states=True;pretrained_model",
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/siddk/voltron-robotics,147,d9dc15bc6fbb063bd7d21875b449c84f,siddk_voltron-robotics/voltron-robotics/voltron/models/reproductions/vr3m.py,"language_model,cache_dir=hf_cache;language_model,cache_dir=hf_cache",Voltron: Language-Driven Representation Learning for Robotics
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,29481bd271870a4f4e95011cff0b75fe,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/scripts/wtq/run_end2end.py,pretrained_model_name_or_path=../../utils_file/gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.BertTokenizer.from_pretrained;transformers.BertTokenizer.from_pretrained,github.com/facebookresearch/ParlAI,10365,9efd79ccddd22298c2f498a719b41c5e,facebookresearch_ParlAI/ParlAI/parlai/agents/rag/retrievers.py,bert-base-uncased;vocab_path,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.BertTokenizer.from_pretrained;transformers.BertModel.from_pretrained,github.com/renatoviolin/Semantic-Search,108,5e33ec89bc82eb1022aa0c82cc88e37c,renatoviolin_Semantic-Search/Semantic-Search/bert_model.py,bert-base-uncased;bert-base-uncased,Semantic search using Transformers and others
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.Wav2Vec2CTCTokenizer.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,7595e7ec06bd3404b1ba39031cfba902,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/wav2vec2/run_asr.py,"unknown,cache_dir=unknown;unknown,cache_dir=unknown,do_lower_case=unknown,word_delimiter_token=unknown;unknown,cache_dir=unknown,gradient_checkpointing=unknown,vocab_size=len",Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.GPT2TokenizerFast.from_pretrained,github.com/jzbjyb/FLARE,387,68862664271768e8e14376a593b830ce,jzbjyb_FLARE/FLARE/src/openai_api.py,google/flan-t5-xl;gpt2,Forward-Looking Active REtrieval-augmented generation (FLARE)
transformers.BertTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained,github.com/fastnlp/CPT,448,699375e8173d5dd8f9d5f7677afeb286,fastnlp_CPT/CPT/finetune/cws/run_cws.py,unknown;unknown,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,88881662d7f11b957e9cbb4d29656dad,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/finetune.py,"THUDM/chatglm-6b,trust_remote_code=True;THUDM/chatglm-6b,load_in_8bit=True,trust_remote_code=True,device_map=auto","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.models.layoutlmv2.feature_extraction_layoutlmv2.LayoutLMv2FeatureExtractor.from_pretrained,github.com/salesforce/CodeRL,436,c7b0e8ae5d7afa5c213353105a5b76b0,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/layoutxlm/processing_layoutxlm.py,"pretrained_model_name_or_path,None=kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/airsplay/vokenization,187,37ee8fb3c750e38d30555fd35b074407,airsplay_vokenization/vokenization/vokenization/revokenization.py,"forward_tokenizer_name,use_fast=True;backward_tokenizer_name,use_fast=True;backward_tokenizer_name;roberta-base","PyTorch code for EMNLP 2020 Paper ""Vokenization: Improving Language Understanding with Visual Supervision"""
transformers.AutoTokenizer.from_pretrained,github.com/Shark-NLP/CoNT,142,806d49101182d8b289ecb9a37cd4eaf8,Shark-NLP_CoNT/CoNT/preprocess/preprocess.py,model,[NeurIPS'22 Spotlight] CoNT: Contrastive Neural Text Generation 
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,a3831bc99ef42edd96dd6482f517b13d,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/testunseen.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Model.from_pretrained,github.com/facebookresearch/ParlAI,10365,79b1bc4938b442a86b0728f25cd18e59,facebookresearch_ParlAI/ParlAI/parlai/agents/hugging_face/dialogpt.py,fle_key,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
transformers.UniSpeechConfig.from_pretrained,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,25c7a30478b125905a773e1a4db6ae5f,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py,config_path,"探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForMaskedLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,90ff7e9dc67043d6abf60887af6a92c8,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/mlm_wwm/run_mlm_wwm.py,"unknown,None=config_kwargs;unknown,None=config_kwargs;unknown,None=tokenizer_kwargs;unknown,None=tokenizer_kwargs;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.LlamaForCausalLM.from_pretrained;transformers.LlamaConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/ypwhs/CreativeChatGLM,203,308cfc7dc7265a7eba4ce5b2d67618d8,ypwhs_CreativeChatGLM/CreativeChatGLM/gptq/llama_inference.py,"model,torch_dtype=auto;model;unknown",👋 欢迎来到 ChatGLM 创意世界！你可以使用修订和续写的功能来生成创意内容！
transformers.SEWConfig.from_pretrained,github.com/salesforce/CodeRL,436,c608e6cfdb4776758437f5f01dad3133,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py,config_path,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,36f6cb2b106c91afc9cbc7c712e712cd,ylsung_VL_adapter/VL_adapter/VL-T5/src/vqa_data.py,"unknown,max_length=unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoTokenizer.from_pretrained,github.com/KRR-Oxford/DeepOnto,119,9bff8de699fcab7f53574601c3f8ebc7,KRR-Oxford_DeepOnto/DeepOnto/src/deeponto/utils/text_utils.py,pretrained_path,A package for ontology engineering with deep learning and language models.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForQuestionAnswering.from_pretrained,github.com/salesforce/CodeRL,436,916426782318830f898e9f0d53b16e09,salesforce_CodeRL/CodeRL/transformers/examples/legacy/question-answering/run_squad.py,"unknown,cache_dir=unknown;unknown,do_lower_case=unknown,cache_dir=unknown,use_fast=False;unknown,from_tf=bool,config=config,cache_dir=unknown;unknown;unknown,do_lower_case=unknown,use_fast=False;checkpoint",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.ViTFeatureExtractor.from_pretrained;transformers.ViTForImageClassification.from_pretrained,github.com/qanastek/HugsVision,185,7e5ca5e3fcd8d77e6ca938df3d5597d1,qanastek_HugsVision/HugsVision/recipes/pneumothorax/binary_classification/predict.py,unknown;unknown,HugsVision is a easy to use huggingface wrapper for state-of-the-art computer vision
transformers.BertTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,225e0106d9d84aa8bc955e1eb60dd1ea,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/pace/pace/utils/write_mmconv_rg.py,"bert-base-uncased,do_lower_case=True",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.GPT2Tokenizer.from_pretrained,github.com/naver/gdc,112,2ac826a76b3482c83804035e8c24bef9,naver_gdc/gdc/dpg/gdc/gdc/metrics.py,gpt2,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.AutoTokenizer.from_pretrained.save_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained.eval,github.com/CuongNN218/zalo_ltr_2021,138,5ceba15e53eae2512911f67e1659af6a,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/seq2seq-distillation/make_student.py,save_path;,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoConfig.from_pretrained,github.com/facebookresearch/multihop_dense_retrieval,203,1c392aee2a7b6fac037b28149d7566ff,facebookresearch_multihop_dense_retrieval/multihop_dense_retrieval/mdr/retrieval/mhop_trainer.py,unknown;unknown,Multi-hop dense retrieval for question answering
transformers.BertTokenizer.from_pretrained,github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis,506,861f621bf8c36eaa72f1e87c2291c2e4,ScalaConsultants_Aspect-Based-Sentiment-Analysis/Aspect-Based-Sentiment-Analysis/aspect_based_sentiment_analysis/loads.py,name,💭 Aspect-Based-Sentiment-Analysis: Transformer & Explainable ML (TensorFlow)
transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,e9c58a9d41bf287be140a37125f36190,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dater/code/scripts/tabfact/run_row.py,pretrained_model_name_or_path=../../utils_file/gpt2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,68334414d69134808c2dd1a0aa9298d1,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/diana/downstream/diana.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=unknown,revision=unknown,use_auth_token=unknown",DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.TFRobertaForMaskedLM.from_pretrained,github.com/rinnakk/japanese-pretrained-models,553,4d5ffced2d237936cd58a499fe906061,rinnakk_japanese-pretrained-models/japanese-pretrained-models/src/task/pretrain_roberta/checkpoint2huggingface.py,"unknown,from_pt=True","Code for producing Japanese pretrained models provided by rinna Co., Ltd."
transformers.GPT2Tokenizer.from_pretrained;transformers.GPT2LMHeadModel.from_pretrained.to,github.com/devjwsong/gpt2-dialogue-generation-pytorch,163,88327eaa341d316047ec473f2573c808,devjwsong_gpt2-dialogue-generation-pytorch/gpt2-dialogue-generation-pytorch/src/main.py,unknown;unknown,The PyTorch implementation of fine-tuning the GPT-2(Generative Pre-Training 2) for dialogue generation.
transformers.RagConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,fa0c1992ad0116cffb57a79d6cbae393,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag/consolidate_rag_checkpoint.py,config_name_or_path;generator_name_or_path;question_encoder_name_or_path;generator_tokenizer_name_or_path;question_encoder_tokenizer_name_or_path,Source code for Zalo AI 2021 submission
transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.AutoModelForCausalLM.from_pretrained;transformers.CLIPImageProcessor.from_pretrained;transformers.CLIPVisionModel.from_pretrained.cuda,github.com/tabtoyou/KoLLaVA,133,e8c46d56d9f23917c7814485377bf16d,tabtoyou_KoLLaVA/KoLLaVA/llava/serve/model_worker.py,"model_path;model_path,torch_dtype=unknown,low_cpu_mem_usage=True,trust_remote_code=True,None=kwargs;model_path,torch_dtype=unknown,low_cpu_mem_usage=True,None=kwargs;unknown,torch_dtype=unknown;",KoLLaVA: Korean Large Language-and-Vision Assistant (feat.LLaVA)
transformers.T5Config.from_pretrained;transformers.T5ForConditionalGeneration.from_pretrained;transformers.T5Tokenizer.from_pretrained,github.com/declare-lab/conv-emotion,1210,92b84c04a281d222c44f23eecceac0dc,declare-lab_conv-emotion/conv-emotion/emotion-cause-extraction/RoBERTa Baseline/simpletransformers/t5/t5_model.py,"model_name,None=unknown;model_name,config=unknown;model_name,truncate=True",This repo contains implementation of different architectures for emotion recognition in conversations.
transformers.AutoConfig.from_pretrained;transformers.AutoModelForSequenceClassification.from_pretrained;transformers.AutoTokenizer.from_pretrained,github.com/thunlp/OpenBackdoor,114,de45755612cdb52692b9827121451855,thunlp_OpenBackdoor/OpenBackdoor/openbackdoor/victims/plms.py,"path;path,config=unknown;path","An open-source toolkit for textual backdoor attack and defense (NeurIPS 2022 D&B, Spotlight)"
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModelForSeq2SeqLM.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,affe2bb9141330f73b8f91152896d645,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/pytorch/question-answering/run_seq2seq_qa.py,"unknown,cache_dir=unknown,revision=unknown,use_auth_token=unknown;unknown,cache_dir=unknown,use_fast=True,revision=unknown,use_auth_token=unknown;unknown,from_tf=bool,config=config,cache_dir=unknown,revision=unknown,use_auth_token=unknown",Source code for Zalo AI 2021 submission
transformers.Wav2Vec2FeatureExtractor.from_pretrained;transformers.Wav2Vec2Config.from_pretrained,github.com/salesforce/CodeRL,436,1998259b4efb442dada11e86997441c2,salesforce_CodeRL/CodeRL/transformers/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py,unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.AutoFeatureExtractor.from_pretrained,github.com/7eu7d7/DreamArtist-stable-diffusion,863,d376e05ec2e05997b496ee5fdc9c1af1,7eu7d7_DreamArtist-stable-diffusion/DreamArtist-stable-diffusion/modules/safety.py,safety_model_id,stable diffusion webui with contrastive prompt tuning
transformers.DPRContextEncoder.from_pretrained.to;transformers.DPRContextEncoderTokenizerFast.from_pretrained,github.com/CuongNN218/zalo_ltr_2021,138,8377674d8e81a3cbba1296de9775e51d,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/rag-end2end-retriever/use_own_knowledge_dataset.py,device=device;unknown,Source code for Zalo AI 2021 submission
transformers.BertTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,bb564424666cbf94e376d5cf359be3fb,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/bertabs/convert_bertabs_original_pytorch_checkpoint.py,bert-base-uncased,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTokenizer.from_pretrained;transformers.CLIPTextModel.from_pretrained,github.com/sail-sg/EditAnything,2813,e749cac0d7c19a11b1029281205c58dd,sail-sg_EditAnything/EditAnything/utils/train_dreambooth_lora_inpaint.py,"unknown;unknown,subfolder=tokenizer;unknown,subfolder=text_encoder","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
transformers.RobertaModel.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,38f48de9e3eb790d702389553a528e5c,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/r2sql/cosql/reranker/model.py,./local_param/,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
transformers.T5TokenizerFast.from_pretrained;transformers.BartTokenizer.from_pretrained,github.com/ylsung/VL_adapter,187,d8d14145370a75e565d0158da192eeb1,ylsung_VL_adapter/VL_adapter/VL-T5/src/nlvr_raw_data.py,"unknown,do_lower_case=unknown;unknown,do_lower_case=unknown","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
transformers.AutoModel.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,e356da260a8dd1301c27e76cecae013e,AIGC-Audio_AudioGPT/AudioGPT/text_to_audio/Make_An_Audio/wav_evaluation/models/clap.py,text_model,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.OpenAIGPTTokenizer.from_pretrained;transformers.OpenAIGPTDoubleHeadsModel.from_pretrained;transformers.OpenAIGPTDoubleHeadsModel.from_pretrained;transformers.OpenAIGPTTokenizer.from_pretrained,github.com/salesforce/CodeRL,436,df959914e06331f75133c8519ffcf33d,salesforce_CodeRL/CodeRL/transformers/examples/legacy/run_openai_gpt.py,unknown;unknown;unknown;unknown,This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.Wav2Vec2Processor.from_pretrained;transformers.Wav2Vec2Model.from_pretrained,github.com/YuanGongND/whisper-at,199,a0deb4d475579ab35698436cc76b8dd2,YuanGongND_whisper-at/whisper-at/src/noise_robust_asr/intermediate_feat_extract/esc-50/extract_esc50_w2v_robust_all.py,facebook/wav2vec2-large-robust-ft-swbd-300h;mdl_size,"Code and Pretrained Models for Interspeech 2023 Paper ""Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong Audio Event Taggers"""
transformers.AutoConfig.from_pretrained;transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained,github.com/HHousen/TransformerSum,405,7419199d91fcecf0c830a1e5ab6fe90f,HHousen_TransformerSum/TransformerSum/src/extractive.py,"unknown,gradient_checkpointing=unknown;unknown,use_fast=unknown;unknown,config=unknown",Models to perform neural summarization (extractive and abstractive) using machine learning transformers and a tool to convert abstractive summarization datasets to the extractive task.
transformers.Wav2Vec2Processor.from_pretrained;transformers.Wav2Vec2ForCTC.from_pretrained.to,github.com/YuanGongND/whisper-at,199,583a35243e725a64c46d8921c2332779,YuanGongND_whisper-at/whisper-at/src/noise_robust_asr/asr_experiments/transcribe_wav2vec_base.py,facebook/wav2vec2-base-960h;device,"Code and Pretrained Models for Interspeech 2023 Paper ""Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong Audio Event Taggers"""
transformers.AutoTokenizer.from_pretrained;transformers.AutoModel.from_pretrained.half.cuda,github.com/guifaChild/text_to_vedio,158,9c99f032f990e6414cea4b212a0baa0a,guifaChild_text_to_vedio/text_to_vedio/ChatGLM-6B-main/api.py,"THUDM/chatglm-6b,trust_remote_code=True;",这是一个由文本直接生成视频的项目
transformers.BertConfig.from_pretrained;transformers.BertModel.from_pretrained,github.com/china-ai-law-challenge/CAIL2020,150,218dcd7808eeb7cbdfbef0b2722441cd,china-ai-law-challenge_CAIL2020/CAIL2020/ydlj/baseline/run_cail.py,unknown;unknown,
transformers.LongformerTokenizerFast.from_pretrained,github.com/AlibabaResearch/DAMO-ConvAI,788,3bee2158e18c1dab8257b8dba7ba9c45,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/dstc11-simmc/task2/eval_dstc11_task2.py,unknown,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
sentence_transformers.SentenceTransformer,github.com/renatoviolin/Semantic-Search,108,dee94fcdb89ffef3dc1cea574f83b745,renatoviolin_Semantic-Search/Semantic-Search/sentenceBERT_model.py,bert-base-nli-stsb-mean-tokens,Semantic search using Transformers and others
sentence_transformers.SentenceTransformer.to;sentence_transformers.SentenceTransformer,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,6090b31da499d8dcf1ac672b526d5c99,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/run_unsup_esimcse.py,device;model_save_path,NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
sentence_transformers.SentenceTransformer,github.com/andreamad8/FSB,119,509e3fd1b9c157a8c92d91fd6b368f14,andreamad8_FSB/FSB/data/wow/generate_closest_shots_embeddings.py,all-mpnet-base-v2,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
sentence_transformers.SentenceTransformer,github.com/usc-isi-i2/cskg,104,c682c4213b7285e452568c414e97dcc1,usc-isi-i2_cskg/cskg/grounding/groundcskg/graphify/link.py,bert-large-nli-cls-token,CSKG: The CommonSense Knowledge Graph
sentence_transformers.SentenceTransformer,github.com/usc-isi-i2/cskg,104,57f71db36eb57f81a9bc53c7f2837870,usc-isi-i2_cskg/cskg/embeddings/relation_analysis.py,model_name,CSKG: The CommonSense Knowledge Graph
sentence_transformers.SentenceTransformer;sentence_transformers.SentenceTransformer,github.com/Guzpenha/transformer_rankers,155,c417db73b8419c3d72b3711f6156010b,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/scripts/train_sentenceBERT_crr.py,unknown;modules=List,A library to conduct ranking experiments with transformers.
sentence_transformers.SentenceTransformer,github.com/andreamad8/FSB,119,f5e20a1f54d210ff13b257d42adc7b04,andreamad8_FSB/FSB/data/wit/generate_closest_shots_embeddings.py,all-mpnet-base-v2,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
sentence_transformers.SentenceTransformer,github.com/amazon-science/sccl,276,1549263744884f5ae85a6f626b60d7be,amazon-science_sccl/sccl/utils/.ipynb_checkpoints/optimizer-checkpoint.py,SBERT_CLASS,"Pytorch implementation of Supporting Clustering with Contrastive Learning, NAACL 2021"
sentence_transformers.SentenceTransformer,github.com/1rgs/clarity-reader,262,2cc2f22cc1c22cc2bfd424dacff98196,1rgs_clarity-reader/clarity-reader/backend/pkg/embed.py,EMBEDDING_MODEL_NAME,"Layered, depth-first reading—start with summaries, tap to explore details, and gain clarity on complex topics."
sentence_transformers.SentenceTransformer,github.com/usc-isi-i2/cskg,104,d93e7b23cba6fae43ba6f46941a85103,usc-isi-i2_cskg/cskg/embeddings/edge_analysis.py,bert-base-nli-mean-tokens,CSKG: The CommonSense Knowledge Graph
sentence_transformers.SentenceTransformer,github.com/shmsw25/FActScore,119,6e336f10f031d59766de4c3e337bf539,shmsw25_FActScore/FActScore/factscore/retrieval.py,unknown,"A package to evaluate factuality of long-form generation. Original implementation of our EMNLP 2023 paper ""FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"""
sentence_transformers.SentenceTransformer,github.com/facebookresearch/ParlAI,10365,66154df0d93d7bd2a5723310a26b898c,facebookresearch_ParlAI/ParlAI/projects/roscoe/score.py,sentence_embedding_model,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
sentence_transformers.SentenceTransformer,github.com/ashawkey/stable-dreamfusion,7150,ebc93cdc1ea3102504843be3ee88c6cf,ashawkey_stable-dreamfusion/stable-dreamfusion/evaluation/r_precision.py,unknown,Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion.
sentence_transformers.SentenceTransformer,github.com/deepset-ai/COVID-QA,340,7e98f21b40db89bde914b226729d1ca9,deepset-ai_COVID-QA/COVID-QA/covid_nlp/modeling/transformer/train_quora_dedup_bert.py,model_name,API & Webapp to answer questions about COVID-19. Using NLP (Question Answering) and trusted data sources.
sentence_transformers.SentenceTransformer,github.com/danielgross/teleprompter,317,f8f8a2333ca76c70f66012470e411fa5,danielgross_teleprompter/teleprompter/main.py,all-MiniLM-L6-v2,
sentence_transformers.SentenceTransformer,github.com/yuchenlin/SwiftSage,164,f70d42ddc00bf8ec3c2571a2888e673e,yuchenlin_SwiftSage/SwiftSage/baselines/eval_agent_saycan.py,paraphrase-MiniLM-L6-v2,SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks 
sentence_transformers.SentenceTransformer,github.com/farizrahman4u/loopgpt,1317,4106f75a7518370c2115e349603ac734,farizrahman4u_loopgpt/loopgpt/loopgpt/embeddings/hf.py,model_id,Modular Auto-GPT Framework
sentence_transformers.SentenceTransformer.__init__,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,13913ffc1930d7f2d76178d2e9b72d3d,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/sentence_transformers/ESimCSE.py,"self,model_name_or_path,modules,device,cache_folder",NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
sentence_transformers.SentenceTransformer,github.com/skeskinen/bert.cpp,346,7cfe4f85a995c6e0ea404784e0b743d4,skeskinen_bert.cpp/bert.cpp/benchmarks/run_mteb.py,unknown,ggml implementation of BERT
sentence_transformers.SentenceTransformer;sentence_transformers.SentenceTransformer,github.com/AIGC-Audio/AudioGPT,9397,c26528fad8e3888b47aaaa8877a47fef,AIGC-Audio_AudioGPT/AudioGPT/audio_to_text/captioning/utils/bert/create_sent_embedding.py,lang2model;paraphrase-MiniLM-L6-v2,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
sentence_transformers.SentenceTransformer;sentence_transformers.SentenceTransformer,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,dc8aab9dea1e77f271ea5e9892d84776,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/run_sup_simcse.py,"modules=List,device=device;model_save_path",NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
sentence_transformers.SentenceTransformer,github.com/andreamad8/FSB,119,46eea402393e6a3e72c691087f834f01,andreamad8_FSB/FSB/demo/app_parsing.py,all-mpnet-base-v2,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
sentence_transformers.SentenceTransformer,github.com/geeks-of-data/knowledge-gpt,240,9db954900f5b3a3a86dd7383bc98c042,geeks-of-data_knowledge-gpt/knowledge-gpt/knowledgegpt/utils/utils_embedding.py,model_name,Extract knowledge from all information sources using gpt and other language models. Index and make Q&A session with information sources.
sentence_transformers.SentenceTransformer;sentence_transformers.SentenceTransformer,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,9bd9474528163b291e1d477be476cd8a,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/run_unsup_simcse.py,"modules=List,device=device;model_save_path",NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
sentence_transformers.SentenceTransformer;sentence_transformers.SentenceTransformer,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,1b7c16c3a6d0408b3f42da69f8b32d6b,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/run_sbert.py,"model_name,device=device;model_save_path",NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
sentence_transformers.SentenceTransformer,github.com/andreamad8/FSB,119,0e3193f62ddb3b9dfeb3280f7c2e8923,andreamad8_FSB/FSB/data/TOP/generate_closest_shots_embeddings.py,"all-mpnet-base-v2,device=cpu",The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
sentence_transformers.SentenceTransformer,github.com/zhoujx4/NLP-Series-sentence-embeddings,160,568fd9130317547283f4613f59b17940,zhoujx4_NLP-Series-sentence-embeddings/NLP-Series-sentence-embeddings/run_unsup_consert.py,model_save_path,NLP句子编码、句子embedding、语义相似度：BERT_avg、BERT_whitening、SBERT、SmiCSE
sentence_transformers.SentenceTransformer,github.com/AlibabaResearch/DAMO-ConvAI,788,8d680a1265f96e0e50cec06cc21ce970,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/api-bank/apis/tool_search.py,sentence-transformers/paraphrase-MiniLM-L3-v2,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
sentence_transformers.SentenceTransformer,github.com/andreamad8/FSB,119,d18c6465ebf542e754ff211b218d528d,andreamad8_FSB/FSB/data/flowMWOZ/generate_closest_shots_embeddings.py,all-mpnet-base-v2,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
sentence_transformers.SentenceTransformer,github.com/CuongNN218/zalo_ltr_2021,138,1c629e030beab7d8c275a8bb138ca6e5,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/hard_negative_mining.py,unknown,Source code for Zalo AI 2021 submission
sentence_transformers.SentenceTransformer,github.com/Guzpenha/transformer_rankers,155,4aff77344374d0da20dca72d361b4ae3,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/negative_samplers/negative_sampling.py,unknown,A library to conduct ranking experiments with transformers.
sentence_transformers.SentenceTransformer,github.com/usc-isi-i2/cskg,104,e3ee0cdbf3fb8be3cb02cd78a5495e69,usc-isi-i2_cskg/cskg/node resolution/util.py,model_name,CSKG: The CommonSense Knowledge Graph
sentence_transformers.SentenceTransformer,github.com/1rgs/clarity-reader,262,acf82cbb47926e0674e3bc5a2f517032,1rgs_clarity-reader/clarity-reader/backend/pkg/modal_setup.py,EMBEDDING_MODEL_NAME,"Layered, depth-first reading—start with summaries, tap to explore details, and gain clarity on complex topics."
sentence_transformers.SentenceTransformer,github.com/renatoviolin/Semantic-Search,108,d3cbfbac08e7f993945081cc0822f17f,renatoviolin_Semantic-Search/Semantic-Search/sentenceROBERTA_model.py,roberta-base-nli-stsb-mean-tokens,Semantic search using Transformers and others
sentence_transformers.SentenceTransformer;sentence_transformers.SentenceTransformer;sentence_transformers.SentenceTransformer,github.com/sturdy-dev/semantic-code-search,247,24f72c0839bdef886a4790e4cc76d045,sturdy-dev_semantic-code-search/semantic-code-search/src/semantic_code_search/cli.py,unknown;unknown;unknown,Search your codebase with natural language • CLI • No data leaves your computer
sentence_transformers.SentenceTransformer,github.com/thunlp/OpenBackdoor,114,1dd381f3d31733bfa334759042c06e6e,thunlp_OpenBackdoor/OpenBackdoor/openbackdoor/utils/evaluator.py,"paraphrase-distilroberta-base-v1,device","An open-source toolkit for textual backdoor attack and defense (NeurIPS 2022 D&B, Spotlight)"
sentence_transformers.SentenceTransformer,github.com/1rgs/clarity-reader,262,b15c3010c5fa5a252d1f4ea648a39ff2,1rgs_clarity-reader/clarity-reader/embed.py,EMBEDDING_MODEL_NAME,"Layered, depth-first reading—start with summaries, tap to explore details, and gain clarity on complex topics."
sentence_transformers.SentenceTransformer,github.com/amazon-science/sccl,276,9f0c441827859e12e8cd0f093714291f,amazon-science_sccl/sccl/utils/optimizer.py,SBERT_CLASS,"Pytorch implementation of Supporting Clustering with Contrastive Learning, NAACL 2021"
sentence_transformers.SentenceTransformer,github.com/andreamad8/FSB,119,a41fdf405d2cf6f11c65ef13d4338eee,andreamad8_FSB/FSB/data/dialKG/generate_closest_shots_embeddings.py,all-mpnet-base-v2,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
sentence_transformers.SentenceTransformer,github.com/skeskinen/bert.cpp,346,a620b2800082b899c18dea8c4a6174be,skeskinen_bert.cpp/bert.cpp/benchmarks/run_mteb_server.py,unknown,ggml implementation of BERT
sentence_transformers.SentenceTransformer;sentence_transformers.SentenceTransformer,github.com/CuongNN218/zalo_ltr_2021,138,b19aeacff796a44babcb9c9f35cfa8f4,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/train_sentence_bert.py,modules=List;unknown,Source code for Zalo AI 2021 submission
sentence_transformers.SentenceTransformer,github.com/wangyuxinwhy/uniem,572,f3140b69b321fe96ed8fd7dec0d6e7aa,wangyuxinwhy_uniem/uniem/mteb-zh/mteb_zh/models.py,model_id,unified embedding model
sentence_transformers.SentenceTransformer,github.com/veekaybee/viberary,346,78188a65f113d366fee8cf5b7fc55674,veekaybee_viberary/viberary/src/search/knn_search.py,sentence-transformers/msmarco-distilbert-base-v3,"Good books, good vibes"
sentence_transformers.SentenceTransformer,github.com/usc-isi-i2/cskg,104,c4815b3349f80ec60614b6bd2499dd6f,usc-isi-i2_cskg/cskg/embeddings/clustering.py,model_name,CSKG: The CommonSense Knowledge Graph
sentence_transformers.SentenceTransformer,github.com/jim-schwoebel/allie,129,5f83a98c36495f3a0500d0cfe3ffdfc4,jim-schwoebel_allie/allie/features/text_features/featurize.py,bert-base-nli-mean-tokens,"🤖 An automated machine learning framework for audio, text, image, video, or .CSV files (50+ featurizers and 15+ model trainers). Python 3.6 required."
sentence_transformers.SentenceTransformer,github.com/yuchenlin/SwiftSage,164,633abc1f89e768ee78b3944c9f6d7573,yuchenlin_SwiftSage/SwiftSage/eval_utils.py,paraphrase-MiniLM-L6-v2,SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks 
sentence_transformers.SentenceTransformer.cuda,github.com/lupantech/ScienceQA,448,d949da39a68b2247b63864fd86f3a08f,lupantech_ScienceQA/ScienceQA/tools/evaluate_explaination.py,,"Data and code for NeurIPS 2022 Paper ""Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering""."
en_core_web_sm.load,github.com/microsoft/jericho,228,84a56059b0df137d24c91f80e27f8387,microsoft_jericho/jericho/jericho/util.py,,A learning environment for man-made Interactive Fiction games.
en_core_web_sm.load,github.com/brightics/studio,178,14101b07ac0f15400242faf1cf038ec4,brightics_studio/studio/function/python/brightics/function/textanalytics/ner.py,,Component based analytics studio on the web browser
transformers.pipeline,github.com/NorskRegnesentral/skweak,899,c5be57cfc265c579a3d754de322692f8,NorskRegnesentral_skweak/skweak/examples/sentiment/sentiment_models.py,"sentiment-analysis,model=nlptown/bert-base-multilingual-uncased-sentiment",skweak: A software toolkit for weak supervision applied to NLP tasks
transformers.pipeline,github.com/bminixhofer/wtpsplit,422,80b41d10a54a2080deb76ffe21519e08,bminixhofer_wtpsplit/wtpsplit/wtpsplit/evaluation/extrinsic.py,"translation,model=unknown,device=0",Code for Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation
transformers.pipeline,github.com/CuongNN218/zalo_ltr_2021,138,2a8ecddc896307e2c0aa6bc4d5fd6ed9,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/examples/research_projects/codeparrot/scripts/human_eval.py,"text-generation,model=model,tokenizer=tokenizer,device=unknown",Source code for Zalo AI 2021 submission
transformers.pipeline;transformers.pipeline,github.com/davidberenstein1957/classy-classification,183,51e9d56e34b886d7493d7787940eb9ac,davidberenstein1957_classy-classification/classy-classification/classy_classification/classifiers/classy_spacy.py,"zero-shot-classification,model=model,device=unknown,top_k=None,accelerator=ort;zero-shot-classification,model=unknown,device=unknown,top_k=None","This repository contains an easy and intuitive approach to few-shot classification using sentence-transformers or spaCy models, or zero-shot classification with Huggingface. "
transformers.pipeline,github.com/aws-samples/aws-genai-llm-chatbot,564,e13c48c140b43c001e645f0c610c84df,aws-samples_aws-genai-llm-chatbot/aws-genai-llm-chatbot/lib/large-language-model/hf-custom-script-model/samples/basic/inference.py,"text-generation,model=model,tokenizer=tokenizer,torch_dtype=unknown,trust_remote_code=True,device_map=auto","A modular and comprehensive solution to deploy a Multi-LLM and Multi-RAG powered chatbot (Amazon Bedrock, Anthropic, HuggingFace, OpenAI, AI21, Cohere) using AWS CDK on AWS"
transformers.pipelines.pipeline,github.com/songhaoyu/BoB,134,d33f4740962fc5038a673a6801e57dcd,songhaoyu_BoB/BoB/xlibs/convert_graph_to_onnx.py,"pipeline_name,model=model,tokenizer=tokenizer,framework=framework",The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.pipelines.pipeline,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,750c82b8d22a31cbe65968854f08376d,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/convert_graph_to_onnx.py,"pipeline_name,model=model,tokenizer=tokenizer,framework=framework,model_kwargs=models_kwargs","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
transformers.pipeline,github.com/AIGC-Audio/AudioGPT,9397,cf7cc7c53be231e796deb264c3b6aee4,AIGC-Audio_AudioGPT/AudioGPT/audio-chatgpt.py,"text-generation,model=unknown,tokenizer=unknown,device=unknown","AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
transformers.pipelines.pipeline,github.com/allenai/real-toxicity-prompts,125,90f70f8658a7c38429b5d7deb0629861,allenai_real-toxicity-prompts/real-toxicity-prompts/generation/generation.py,"text-generation,model=model_name_or_path,device=0",
transformers.pipelines.pipeline,github.com/CuongNN218/zalo_ltr_2021,138,19b02a0253342d344b2d8f2d30f1d904,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/convert_graph_to_onnx.py,"pipeline_name,model=model,tokenizer=tokenizer,framework=framework,model_kwargs=models_kwargs",Source code for Zalo AI 2021 submission
transformers.pipeline,github.com/salesforce/CodeRL,436,da1de2bbbf05b9e1ddec6fa5eb0af35d,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/robust-speech-event/eval.py,"automatic-speech-recognition,model=unknown,device=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.pipeline,github.com/salesforce/CodeRL,436,d85c54dd5b1853b00598bd1782ed1aa5,salesforce_CodeRL/CodeRL/transformers/examples/research_projects/codeparrot/scripts/human_eval.py,"text-generation,model=model,tokenizer=tokenizer,device=unknown",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
transformers.pipelines.pipeline,github.com/songhaoyu/BoB,134,5ed966470df195e14a74cd3d0b785202,songhaoyu_BoB/BoB/xlibs/commands/serving.py,"task=unknown,model=unknown,config=unknown,tokenizer=unknown,device=unknown",The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.pipeline,github.com/csinva/gpt-paper-title-generator,132,b9080769307b08513d3cd963097d5f2e,csinva_gpt-paper-title-generator/gpt-paper-title-generator/gptneo/04_generate_samples.py,"text-generation,model=model,tokenizer=tokenizer",Generating paper titles (and more!) with GPT trained on data scraped from arXiv.
transformers.pipeline,github.com/Guzpenha/transformer_rankers,155,4aff77344374d0da20dca72d361b4ae3,Guzpenha_transformer_rankers/transformer_rankers/transformer_rankers/negative_samplers/negative_sampling.py,"conversational,model=pre_trained_model,device=0",A library to conduct ranking experiments with transformers.
transformers.pipeline,github.com/Sanster/lama-cleaner,14081,5074579b0630de2a7d62b9e1c8988988,Sanster_lama-cleaner/lama-cleaner/lama_cleaner/model/controlnet.py,depth-estimation,"Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures."
transformers.pipeline,github.com/naver/gdc,112,ce89c81a1f66cc162b2b30609825b426,naver_gdc/gdc/cdpg/cdpg/metrics.py,"text-classification,microsoft/DialogRPT-updown,device=device","Code accompanying our papers on the ""Generative Distributional Control"" framework"
transformers.pipeline,github.com/IBM/zshot,285,76e82964b00ddedbf24dd16e559e1f97,IBM_zshot/zshot/zshot/relation_extractor/zsrc/decide_entity_order.py,"zero-shot-classification,model=facebook/bart-large-mnli",Zero and Few shot named entity & relationships recognition
transformers.pipelines.pipeline,github.com/songhaoyu/BoB,134,4212c680071c06a02b567d4adc8846fc,songhaoyu_BoB/BoB/xlibs/commands/run.py,"task=unknown,model=unknown,config=unknown,tokenizer=unknown,device=unknown",The released codes for ACL 2021 paper 'BoB: BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data'
transformers.pipelines.pipeline,github.com/salesforce/CodeRL,436,bd3585d75d74c4418607108f5dc123f2,salesforce_CodeRL/CodeRL/transformers/src/transformers/convert_graph_to_onnx.py,"pipeline_name,model=model,tokenizer=tokenizer,framework=framework,model_kwargs=models_kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
timm.models.create_model,github.com/locuslab/convmixer,1025,2d78299a9c5496c3a74dbc744f6ead32,locuslab_convmixer/convmixer/pytorch-image-models/train.py,"unknown,pretrained=unknown,num_classes=unknown,drop_rate=unknown,drop_connect_rate=unknown,drop_path_rate=unknown,drop_block_rate=unknown,global_pool=unknown,bn_tf=unknown,bn_momentum=unknown,bn_eps=unknown,scriptable=unknown,checkpoint_path=unknown","Implementation of ConvMixer for ""Patches Are All You Need? 🤷"""
timm.create_model,github.com/CuongNN218/zalo_ltr_2021,138,3f44e9635c286a8c19ae339ffbd1d518,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/deit/convert_deit_timm_to_pytorch.py,"deit_name,pretrained=True",Source code for Zalo AI 2021 submission
timm.models.create_model,github.com/RobustBench/robustbench,543,d2f419c8e7c4a2aad2615148687cdf13,RobustBench_robustbench/robustbench/robustbench/model_zoo/architectures/convstem_models.py,"deit_small_patch16_224,pretrained=pretrained",RobustBench: a standardized adversarial robustness benchmark [NeurIPS'21 Benchmarks and Datasets Track]
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,262460a4b4a8ed54367b8380999f7b4a,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/detr/modeling_detr.py,"unknown,pretrained=unknown,features_only=True,out_indices=Tuple,in_chans=unknown,None=kwargs","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,0067f43ad17e233a91db37ef211d3ea7,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/swin/convert_swin_timm_to_pytorch.py,"swin_name,pretrained=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/VideoCrafter/VideoCrafter,3181,05c47ebed3dcd644ad3278dc887c5423,VideoCrafter_VideoCrafter/VideoCrafter/extralibs/midas/midas/vit.py,"vit_large_patch16_384,pretrained=pretrained;vit_base_patch16_384,pretrained=pretrained;vit_deit_base_patch16_384,pretrained=pretrained;vit_deit_base_distilled_patch16_384,pretrained=pretrained;vit_base_resnet50_384,pretrained=pretrained",VideoCrafter1: Open Diffusion Models for High-Quality Video Generation
timm.models.create_model,github.com/ashkamath/mdetr,903,4c783c12f70adf7651a978660c98ea26,ashkamath_mdetr/mdetr/models/backbone.py,"name,pretrained=True,in_chans=3,features_only=True,out_indices=Tuple",
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/ashawkey/stable-dreamfusion,7150,b0852914cc496217c99fd219e4faafc9,ashawkey_stable-dreamfusion/stable-dreamfusion/dpt.py,"vit_large_patch16_384,pretrained=pretrained;vit_base_patch16_384,pretrained=pretrained;vit_deit_base_patch16_384,pretrained=pretrained;vit_deit_base_distilled_patch16_384,pretrained=pretrained;vit_base_resnet50_384,pretrained=pretrained",Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion.
timm.models.create_model,github.com/okojoalg/sequencer,131,712a613bdc3d103155e356056a81a7c5,okojoalg_sequencer/sequencer/validate.py,"unknown,pretrained=unknown,num_classes=unknown,in_chans=3,global_pool=unknown,scriptable=unknown",Sequencer: Deep LSTM for Image Classification
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,76f982f1563d1b824f6024dc8239b0e8,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/levit/convert_levit_timm_to_pytorch.py,"levit_128s,pretrained=True;levit_128,pretrained=True;levit_192,pretrained=True;levit_256,pretrained=True;levit_384,pretrained=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,a1ded7282d09bb579cd371e99ae97ee9,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/vit_hybrid/convert_vit_hybrid_timm_to_pytorch.py,"vit_name,pretrained=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.create_model;timm.create_model;timm.create_model,github.com/RobustBench/robustbench,543,b84274e1f6393e3af1afdbfca30ea394,RobustBench_robustbench/robustbench/robustbench/model_zoo/cifar10.py,"debenedetti2020light_xcit_s_cifar10_linf,pretrained=True;debenedetti2020light_xcit_m_cifar10_linf,pretrained=True;debenedetti2020light_xcit_l_cifar10_linf,pretrained=True",RobustBench: a standardized adversarial robustness benchmark [NeurIPS'21 Benchmarks and Datasets Track]
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,eed08914085e7493d2a3310e1ae69398,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/swinv2/convert_swinv2_timm_to_pytorch.py,"swinv2_name,pretrained=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.create_model,github.com/ylsung/VL_adapter,187,f2ed1974445fb860dccd8c232ca694ac,ylsung_VL_adapter/VL_adapter/VL-T5/src/clip_prepro_feats.py,"params,pretrained=True","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
timm.create_model,github.com/salesforce/CodeRL,436,44e6b2da76cfadbd90d043eaead1188f,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/swin/convert_swin_timm_to_pytorch.py,"swin_name,pretrained=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
timm.models.create_model,github.com/okojoalg/sequencer,131,1106880957d4cb547a3e7e4f2920b095,okojoalg_sequencer/sequencer/generate_erf.py,"unknown,pretrained=unknown,num_classes=unknown,in_chans=3,global_pool=unknown,scriptable=unknown",Sequencer: Deep LSTM for Image Classification
timm.create_model,github.com/Yui010206/SeViLA,121,c21fe792d1c7e1b7331701614b060b3c,Yui010206_SeViLA/SeViLA/lavis/models/clip_models/timm_model.py,"model_name,pretrained=pretrained",[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,ecbe5ef34747af2e3fb3574c38a95874,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/vit/convert_vit_timm_to_pytorch.py,"vit_name,pretrained=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.models.create_model;timm.models.create_model,github.com/Amshaker/SwiftFormer,159,f49ccfd7f8a82fb000653fcd092ad9b7,Amshaker_SwiftFormer/SwiftFormer/main.py,"unknown,num_classes=unknown,distillation=unknown,pretrained=unknown,fuse=unknown;unknown,pretrained=False,num_classes=unknown,global_pool=avg",[ICCV'23] Official repository of paper SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,d5fb8d0de6defb995a99b15561f3a101,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/deformable_detr/modeling_deformable_detr.py,"unknown,pretrained=unknown,features_only=True,out_indices=unknown,in_chans=unknown,None=kwargs","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.models.create_model,github.com/antoyang/TubeDETR,140,e027ad44ca1194faa84716b4461b1eaa,antoyang_TubeDETR/TubeDETR/models/backbone.py,"name,pretrained=True,in_chans=3,features_only=True,out_indices=Tuple",[CVPR 2022 Oral] TubeDETR: Spatio-Temporal Video Grounding with Transformers
timm.create_model,github.com/salesforce/CodeRL,436,77b10bfc9ff79502dde1b9d2d73ef716,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/deit/convert_deit_timm_to_pytorch.py,"deit_name,pretrained=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
timm.models.create_model,github.com/AlibabaResearch/AdvancedLiterateMachinery,388,e5616190624e3f06070f89c7aac69c2f,AlibabaResearch_AdvancedLiterateMachinery/AdvancedLiterateMachinery/OCR/MGP-STR/modules/char_str.py,"model,pretrained=True,num_classes=num_tokens,checkpoint_path=checkpoint_path,batch_max_length=batch_max_length","A collection of original, innovative ideas and algorithms towards Advanced Literate Machinery. This project is maintained by the OCR Team in the Language Technology Lab, Alibaba DAMO Academy."
timm.models.create_model,github.com/locuslab/convmixer,1025,a44a37d7e298e71fea84574951d8458e,locuslab_convmixer/convmixer/pytorch-image-models/validate.py,"unknown,pretrained=unknown,num_classes=unknown,in_chans=3,global_pool=unknown,scriptable=unknown","Implementation of ConvMixer for ""Patches Are All You Need? 🤷"""
timm.create_model,github.com/CuongNN218/zalo_ltr_2021,138,879558e765a4958023e96dab3bebe823,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/vit/convert_vit_timm_to_pytorch.py,"vit_name,pretrained=True",Source code for Zalo AI 2021 submission
timm.create_model,github.com/AIGC-Audio/AudioGPT,9397,15b9bbec109f283f963d34b40ece27fc,AIGC-Audio_AudioGPT/AudioGPT/text_to_audio/Make_An_Audio/ldm/modules/encoders/open_clap/timm_model.py,"model_name,pretrained=pretrained","AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,d715c80e6bfa090b5f2384c8c732a79e,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/table_transformer/modeling_table_transformer.py,"unknown,pretrained=unknown,features_only=True,out_indices=Tuple,in_chans=unknown,None=kwargs","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.create_model,github.com/TencentARC/AnimeSR,289,67d98a9f02c222f0b569d9e9712b7475,TencentARC_AnimeSR/AnimeSR/scripts/metrics/MANIQA/models/model_attentionIQA2.py,"vit_base_patch8_224,pretrained=True","Codes for ""AnimeSR: Learning Real-World Super-Resolution Models for Animation Videos"""
timm.create_model,github.com/salesforce/CodeRL,436,7961fc6e3e93fc948f648da23f45d54d,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/detr/modeling_detr.py,"name,pretrained=True,features_only=True,out_indices=Tuple,None=kwargs",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
timm.models.create_model,github.com/AlibabaResearch/AdvancedLiterateMachinery,388,9b0c2af7918e37d0bba66d5f0d727036,AlibabaResearch_AdvancedLiterateMachinery/AdvancedLiterateMachinery/OCR/MGP-STR/modules/mgp_str.py,"model,pretrained=True,num_classes=num_tokens,checkpoint_path=checkpoint_path,batch_max_length=batch_max_length","A collection of original, innovative ideas and algorithms towards Advanced Literate Machinery. This project is maintained by the OCR Team in the Language Technology Lab, Alibaba DAMO Academy."
timm.create_model,github.com/YuxinWenRick/tree-ring-watermark,145,9a3f17e5a63d5509721be041b04cd7f7,YuxinWenRick_tree-ring-watermark/tree-ring-watermark/open_clip/timm_model.py,"model_name,pretrained=pretrained,None=timm_kwargs",
timm.models.create_model,github.com/locuslab/convmixer,1025,e8af2b9fa048af4f046f09b2ea8e8f2e,locuslab_convmixer/convmixer/pytorch-image-models/inference.py,"unknown,num_classes=unknown,in_chans=3,pretrained=unknown,checkpoint_path=unknown","Implementation of ConvMixer for ""Patches Are All You Need? 🤷"""
timm.create_model,github.com/ylsung/VL_adapter,187,8ed61c989278699ca88730630ecb4783,ylsung_VL_adapter/VL_adapter/feature_extraction/coco_CLIP.py,"params,pretrained=True","PyTorch code for ""VL-Adapter: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks"" (CVPR2022)"
timm.create_model,github.com/leonnnop/GMMSeg,142,39dd28081f61e28f1b20e94211399b14,leonnnop_GMMSeg/GMMSeg/mmseg/models/backbones/timm_backbone.py,"model_name=model_name,features_only=features_only,pretrained=pretrained,in_chans=in_channels,checkpoint_path=checkpoint_path,None=kwargs",[NeurIPS 2022 Spotlight] GMMSeg:  Gaussian Mixture based Generative Semantic Segmentation Models
timm.create_model,github.com/Alibaba-MIIL/ImageNet21K,665,b50f2d8e61fc0bff48256c07687ab141,Alibaba-MIIL_ImageNet21K/ImageNet21K/visualize_detector.py,"vit_base_patch16_224_miil_in21k,pretrained=True","Official Pytorch Implementation of: ""ImageNet-21K Pretraining for the Masses""(NeurIPS, 2021) paper"
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/sail-sg/EditAnything,2813,c5ca0972aee29518a2c3a03b43576393,sail-sg_EditAnything/EditAnything/ldm/modules/midas/midas/vit.py,"vit_large_patch16_384,pretrained=pretrained;vit_base_patch16_384,pretrained=pretrained;vit_deit_base_patch16_384,pretrained=pretrained;vit_deit_base_distilled_patch16_384,pretrained=pretrained;vit_base_resnet50_384,pretrained=pretrained","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/isl-org/DPT,1725,1ec96a8b825eea6a3e6db6d6ccd9d21b,isl-org_DPT/DPT/dpt/vit.py,"vit_base_resnet50_384,pretrained=pretrained;vit_large_patch16_384,pretrained=pretrained;vit_base_patch16_384,pretrained=pretrained;vit_deit_base_patch16_384,pretrained=pretrained;vit_deit_base_distilled_patch16_384,pretrained=pretrained",Dense Prediction Transformers
timm.create_model,github.com/zhvng/open-musiclm,398,de0a8650c0994135cb55f633ad499213,zhvng_open-musiclm/open-musiclm/open_musiclm/laion_clap/clap_module/timm_model.py,"model_name,pretrained=pretrained","Implementation of MusicLM, a text to music model published by Google Research, with a few modifications."
timm.create_model,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,d4f3978c1742058c8715e36a3819654b,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/complete_cv_example.py,"resnet50d,pretrained=True,num_classes=len",We perform functional grounding of LLMs' knowledge in BabyAI-Text
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,4a60d49e78f08d7c11e4872619ad3520,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/bit/convert_bit_to_pytorch.py,"model_name,pretrained=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/salesforce/UniControl,525,f5fbe43b54214334f768a7a332752051,salesforce_UniControl/UniControl/annotator/midas/midas/vit.py,"vit_large_patch16_384,pretrained=pretrained;vit_base_patch16_384,pretrained=pretrained;vit_deit_base_patch16_384,pretrained=pretrained;vit_deit_base_distilled_patch16_384,pretrained=pretrained;vit_base_resnet50_384,pretrained=pretrained",Unified Controllable Visual Generation Model
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/RobustBench/robustbench,543,c30c99798a95d397661fefcad4614c02,RobustBench_robustbench/robustbench/robustbench/model_zoo/imagenet.py,"debenedetti2020light_xcit_s_imagenet_linf,pretrained=True;debenedetti2020light_xcit_m_imagenet_linf,pretrained=True;debenedetti2020light_xcit_l_imagenet_linf,pretrained=True;alexnet,pretrained=True;tian2022deeper_deit_s_imagenet_corruptions,pretrained=True;tian2022deeper_deit_b_imagenet_corruptions,pretrained=True",RobustBench: a standardized adversarial robustness benchmark [NeurIPS'21 Benchmarks and Datasets Track]
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/salesforce/UniControl,525,c129316d64c237a2274e84770d21879a,salesforce_UniControl/UniControl/ldm/modules/midas/midas/vit.py,"vit_large_patch16_384,pretrained=pretrained;vit_base_patch16_384,pretrained=pretrained;vit_deit_base_patch16_384,pretrained=pretrained;vit_deit_base_distilled_patch16_384,pretrained=pretrained;vit_base_resnet50_384,pretrained=pretrained",Unified Controllable Visual Generation Model
timm.models.create_model,github.com/SHI-Labs/Compact-Transformers,436,a32665d5248a7ec26dc8bedae236414c,SHI-Labs_Compact-Transformers/Compact-Transformers/train.py,"unknown,pretrained=unknown,num_classes=unknown,drop_rate=unknown,drop_connect_rate=unknown,drop_path_rate=unknown,drop_block_rate=unknown,global_pool=unknown,bn_tf=unknown,bn_momentum=unknown,bn_eps=unknown,scriptable=unknown,checkpoint_path=unknown","Escaping the Big Data Paradigm with Compact Transformers, 2021 (Train your Vision Transformers in 30 mins on CIFAR-10 with a single GPU!)"
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,f375afb9a83b87de46a8df07ac68bf9e,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/conditional_detr/modeling_conditional_detr.py,"unknown,pretrained=unknown,features_only=True,out_indices=Tuple,in_chans=unknown,None=kwargs","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/KU-CVLAB/3DFuse,667,4ba47c3f1f3b95a5329b190250eba880,KU-CVLAB_3DFuse/3DFuse/ldm/modules/midas/midas/vit.py,"vit_large_patch16_384,pretrained=pretrained;vit_base_patch16_384,pretrained=pretrained;vit_deit_base_patch16_384,pretrained=pretrained;vit_deit_base_distilled_patch16_384,pretrained=pretrained;vit_base_resnet50_384,pretrained=pretrained","Official implementation of ""Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation"""
timm.create_model,github.com/facebookresearch/ov-seg,552,ec9cf4f48ff9588e7c54ca85753bb815,facebookresearch_ov-seg/ov-seg/open_clip_training/src/open_clip/timm_model.py,"model_name,pretrained=pretrained",This is the official PyTorch implementation of the paper Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP.
timm.create_model,github.com/salesforce/CodeRL,436,bcfd6ca28eb0428ff2a71163165438d5,salesforce_CodeRL/CodeRL/transformers/src/transformers/models/vit/convert_vit_timm_to_pytorch.py,"vit_name,pretrained=True",This is the official code for the paper CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning (NeurIPS22).
timm.create_model,github.com/27182812/ChatGLM-LLaMA-chinese-insturct,384,994ac4787de1459c67181a8878620654,27182812_ChatGLM-LLaMA-chinese-insturct/ChatGLM-LLaMA-chinese-insturct/src/transformers/models/deit/convert_deit_timm_to_pytorch.py,"deit_name,pretrained=True","探索中文instruct数据在ChatGLM, LLaMA上的微调表现"
timm.models.create_model,github.com/okojoalg/sequencer,131,e1295592df72465168ff85b0ef5cdb40,okojoalg_sequencer/sequencer/train.py,"unknown,pretrained=unknown,num_classes=unknown,drop_rate=unknown,drop_connect_rate=unknown,drop_path_rate=unknown,drop_block_rate=unknown,global_pool=unknown,bn_momentum=unknown,bn_eps=unknown,scriptable=unknown,checkpoint_path=unknown",Sequencer: Deep LSTM for Image Classification
timm.create_model,github.com/ajayjain/DietNeRF,109,f25ff4de4d9afc64040528001bfb2171,ajayjain_DietNeRF/DietNeRF/dietnerf/run_nerf.py,"model_type,pretrained=True,num_classes=0",
timm.models.create_model,github.com/okojoalg/sequencer,131,0cf0c608f83b75dce7b8e4e2145ea0af,okojoalg_sequencer/sequencer/validate_c.py,"unknown,pretrained=unknown,num_classes=unknown,in_chans=3,global_pool=unknown,scriptable=unknown",Sequencer: Deep LSTM for Image Classification
timm.create_model,github.com/CuongNN218/zalo_ltr_2021,138,cf1c9a252f2569f3db671fb043c6f65e,CuongNN218_zalo_ltr_2021/zalo_ltr_2021/transformers/src/transformers/models/detr/modeling_detr.py,"name,pretrained=True,features_only=True,out_indices=Tuple,None=kwargs",Source code for Zalo AI 2021 submission
timm.create_model;timm.create_model;timm.create_model;timm.create_model;timm.create_model,github.com/thu-ml/controlvideo,183,764da6f25ef19397aa7c9fadadadb06c,thu-ml_controlvideo/controlvideo/annotator/midas/midas/vit.py,"vit_large_patch16_384,pretrained=pretrained;vit_base_patch16_384,pretrained=pretrained;vit_deit_base_patch16_384,pretrained=pretrained;vit_deit_base_distilled_patch16_384,pretrained=pretrained;vit_base_resnet50_384,pretrained=pretrained","Official implementation for ""ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing"" "
timm.create_model;timm.create_model;timm.create_model,github.com/RobustBench/robustbench,543,d3204a1f4f737cef42934bb85656cee1,RobustBench_robustbench/robustbench/robustbench/model_zoo/cifar100.py,"debenedetti2020light_xcit_s_cifar100_linf,pretrained=True;debenedetti2020light_xcit_m_cifar100_linf,pretrained=True;debenedetti2020light_xcit_l_cifar100_linf,pretrained=True",RobustBench: a standardized adversarial robustness benchmark [NeurIPS'21 Benchmarks and Datasets Track]
timm.create_model,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,0ca83c4f74d80049c476d0b1317fd8c7,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/v0.13.2/accelerate-0.13.2/examples/cv_example.py,"resnet50d,pretrained=True,num_classes=len",We perform functional grounding of LLMs' knowledge in BabyAI-Text
timm.models.create_model,github.com/zhoudaquan/Refiner_ViT,106,cb5b9b0edd1d43cf2e55b35da7af5b2c,zhoudaquan_Refiner_ViT/Refiner_ViT/main.py,"unknown,pretrained=unknown,num_classes=unknown,drop_rate=unknown,drop_connect_rate=unknown,drop_path_rate=unknown,drop_block_rate=unknown,global_pool=unknown,bn_tf=unknown,bn_momentum=unknown,bn_eps=unknown,scriptable=unknown,checkpoint_path=unknown,img_size=unknown,cos_reg=unknown,return_dense=unknown,mix_token=unknown",
timm.create_model,github.com/haoheliu/AudioLDM,2018,984c0d81b2f68c133397c22ee39cb4fd,haoheliu_AudioLDM/AudioLDM/audioldm/clap/open_clip/timm_model.py,"model_name,pretrained=pretrained","AudioLDM: Generate speech, sound effects, music and beyond, with text."
timm.models.create_model;timm.models.create_model,github.com/NVlabs/A-ViT,121,3168827e1c196a8b6f4b864857eabe4f,NVlabs_A-ViT/A-ViT/main_act.py,"unknown,pretrained=unknown,num_classes=unknown,drop_rate=unknown,drop_path_rate=unknown,drop_block_rate=None,args=args;unknown,pretrained=False,num_classes=unknown,global_pool=avg",Official PyTorch implementation of A-ViT: Adaptive Tokens for Efficient Vision Transformer (CVPR 2022)
diffusers.StableDiffusionPipeline.from_pretrained;diffusers.StableDiffusionImg2ImgPipeline.from_pretrained,github.com/SCUTlihaoyu/open-chat-video-editor,2354,04bf7c2c7541e96a7de497002730ad44,SCUTlihaoyu_open-chat-video-editor/open-chat-video-editor/generator/image/generation/stable_diffusion.py,"unknown,torch_dtype=unknown;model_id,torch_dtype=unknown",Open source short video automatic generation tool
diffusers.AutoencoderKL.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained,github.com/Mikubill/naifu-diffusion,222,1d6729d46a8b38b745146e43272552d9,Mikubill_naifu-diffusion/naifu-diffusion/experiment/custom_encoder.py,"unknown,subfolder=vae;unknown,subfolder=unet",Train stable diffusion model with Diffusers and Pytorch Lightning
diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker.from_pretrained,github.com/salesforce/UniControl,525,6cb2999d7d11bac070ee6a92f34a2723,salesforce_UniControl/UniControl/utils.py,safety_model_id,Unified Controllable Visual Generation Model
diffusers.DPMSolverMultistepScheduler.from_pretrained;diffusers.DDIMScheduler.from_pretrained;diffusers.DiffusionPipeline.from_pretrained,github.com/YuxinWenRick/tree-ring-watermark,145,e579d5d763794470b7781b6c0e12141b,YuxinWenRick_tree-ring-watermark/tree-ring-watermark/src/tree_ring_watermark/target_api.py,"model_id,subfolder=scheduler;model_id,subfolder=scheduler;model_id,scheduler=scheduler,torch_dtype=unknown",
diffusers.ControlNetModel.from_pretrained,github.com/sail-sg/EditAnything,2813,53425704c90144f4940316b5a95b3a8e,sail-sg_EditAnything/EditAnything/sam2groundingdino_edit.py,"controlnet_path,torch_dtype=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
diffusers.ControlNetModel.from_pretrained,github.com/sail-sg/EditAnything,2813,8211c985143564c79b57e6b5f93f6511,sail-sg_EditAnything/EditAnything/sam2vlpart_edit.py,"controlnet_path,torch_dtype=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
diffusers.pipelines.stable_diffusion.StableDiffusionSafetyChecker.from_pretrained,github.com/cloneofsimo/paint-with-words-sd,601,34d7ac8f3a163e4b029abe1a1e832076,cloneofsimo_paint-with-words-sd/paint-with-words-sd/change_model_path.py,CompVis/stable-diffusion-safety-checker,Implementation of Paint-with-words with Stable Diffusion : method from eDiff-I that let you generate image from text-labeled segmentation map.
diffusers.DDPMScheduler.from_pretrained;diffusers.AutoencoderKL.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.training_utils.EMAModel.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.StableDiffusionInstructPix2PixPipeline.from_pretrained;diffusers.StableDiffusionInstructPix2PixPipeline.from_pretrained,github.com/huggingface/instruction-tuned-sd,127,87fadb6ad5b562273d57db3afe4d755d,huggingface_instruction-tuned-sd/instruction-tuned-sd/finetune_instruct_pix2pix.py,"unknown,subfolder=scheduler;unknown,subfolder=vae,revision=unknown;unknown,subfolder=unet,revision=unknown;os.path.join,UNet2DConditionModel;input_dir,subfolder=unet;unknown,unet=unet,revision=unknown,torch_dtype=weight_dtype;unknown,text_encoder=text_encoder,vae=vae,unet=unet,revision=unknown",Code for instruction-tuning Stable Diffusion.
diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker.from_pretrained,github.com/timothybrooks/instruct-pix2pix,5428,8eebc1e69e6f7c14595d7891ddd93ceb,timothybrooks_instruct-pix2pix/instruct-pix2pix/stable_diffusion/scripts/txt2img.py,safety_model_id,
diffusers.StableDiffusionInpaintPipeline.from_pretrained;diffusers.AutoencoderKL.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.DDPMScheduler.from_pretrained;diffusers.StableDiffusionPipeline.from_pretrained,github.com/sail-sg/EditAnything,2813,90cdc0ae8d861e462f5573909923cea5,sail-sg_EditAnything/EditAnything/tools/train_dreambooth_inpaint.py,"unknown,torch_dtype=torch_dtype,safety_checker=None;unknown,subfolder=vae;unknown,subfolder=unet;unknown,subfolder=scheduler;unknown,unet=accelerator.unwrap_model,text_encoder=accelerator.unwrap_model","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
diffusers.UNet2DConditionModel.from_pretrained,github.com/Mikubill/naifu-diffusion,222,f60d0b12cc392323941fd10bbe247d06,Mikubill_naifu-diffusion/naifu-diffusion/experiment/noisy_learning.py,/root/dataset/animesfw/unet,Train stable diffusion model with Diffusers and Pytorch Lightning
diffusers.StableDiffusionPipeline.from_pretrained,github.com/AIGC-Audio/AudioGPT,9397,cf7cc7c53be231e796deb264c3b6aee4,AIGC-Audio_AudioGPT/AudioGPT/audio-chatgpt.py,"runwayml/stable-diffusion-v1-5,torch_dtype=unknown","AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
diffusers.ControlNetModel.from_pretrained,github.com/vijishmadhavan/SkinDeep,914,d5e9d7dcdb5c1723552a409c9ae488ef,vijishmadhavan_SkinDeep/SkinDeep/tattoorem.py,"thepowefuldeez/sd21-controlnet-canny,torch_dtype=unknown",Get Deinked!!
diffusers.AutoencoderKL.from_pretrained,github.com/johannakarras/DreamPose,748,658545002fd600377d15f9416f700ade,johannakarras_DreamPose/DreamPose/finetune-unet.py,"CompVis/stable-diffusion-v1-4,subfolder=vae,revision=ebb811dd71cdc38a204ecbdd6ac5d580f529fd8c","Official implementation of ""DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"""
diffusers.IFPipeline.from_pretrained,github.com/ashawkey/stable-dreamfusion,7150,4a8f7cd5a710e4d5a35979652f8653e4,ashawkey_stable-dreamfusion/stable-dreamfusion/guidance/if_utils.py,"model_key,variant=fp16,torch_dtype=unknown",Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion.
diffusers.AutoencoderKL.from_pretrained.to;diffusers.UNet2DConditionModel.from_pretrained.to;diffusers.DDIMScheduler.from_pretrained,github.com/omerbt/MultiDiffusion,774,b3d194c073801563b3bf15e17595332e,omerbt_MultiDiffusion/MultiDiffusion/panorama.py,"unknown;unknown;model_key,subfolder=scheduler","Official Pytorch Implementation for ""MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation"" presenting ""MultiDiffusion"" (ICML 2023)"
diffusers.AutoencoderKL.from_pretrained;diffusers.AutoencoderKL.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.StableDiffusionPipeline.from_pretrained,github.com/cloneofsimo/paint-with-words-sd,601,aac28965aad5ba258dd838d361e0062c,cloneofsimo_paint-with-words-sd/paint-with-words-sd/paint_with_words/paint_with_words.py,"model_path,subfolder=vae,use_auth_token=model_token,torch_dtype=dtype,local_files_only=local_path_only,revision=fp16;model_path,subfolder=vae,use_auth_token=model_token,torch_dtype=dtype,local_files_only=local_path_only;model_path,subfolder=unet,use_auth_token=model_token,torch_dtype=dtype,local_files_only=local_path_only,revision=fp16;model_path,subfolder=unet,use_auth_token=model_token,torch_dtype=dtype,local_files_only=local_path_only;save_dir,None=kwargs",Implementation of Paint-with-words with Stable Diffusion : method from eDiff-I that let you generate image from text-labeled segmentation map.
diffusers.UnCLIPPipeline.from_pretrained;diffusers.DiffusionPipeline.from_pretrained,github.com/KU-CVLAB/3DFuse,667,7c0a933cd2b95dccf6f8872e69864ce5,KU-CVLAB_3DFuse/3DFuse/semantic_coding.py,"kakaobrain/karlo-v1-alpha,torch_dtype=unknown;runwayml/stable-diffusion-v1-5","Official implementation of ""Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation"""
diffusers.DDPMScheduler.from_pretrained;diffusers.AutoencoderKL.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.training_utils.EMAModel.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.StableDiffusionInstructPix2PixPipeline.from_pretrained;diffusers.StableDiffusionInstructPix2PixPipeline.from_pretrained,github.com/huggingface/instruction-tuned-sd,127,438ab8096b02ac0d7b56e5a3cb3dc821,huggingface_instruction-tuned-sd/instruction-tuned-sd/train_instruct_pix2pix.py,"unknown,subfolder=scheduler;unknown,subfolder=vae,revision=unknown;unknown,subfolder=unet,revision=unknown;os.path.join,UNet2DConditionModel;input_dir,subfolder=unet;unknown,unet=unet,revision=unknown,torch_dtype=weight_dtype;unknown,text_encoder=accelerator.unwrap_model,vae=accelerator.unwrap_model,unet=unet,revision=unknown",Code for instruction-tuning Stable Diffusion.
diffusers.DiffusionPipeline.from_pretrained;diffusers.DDPMScheduler.from_pretrained;diffusers.AutoencoderKL.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.StableDiffusionPipeline.from_pretrained,github.com/sail-sg/EditAnything,2813,2745b7e7d5adb496fae376ec954e5798,sail-sg_EditAnything/EditAnything/utils/texutal_inversion.py,"unknown,text_encoder=accelerator.unwrap_model,tokenizer=tokenizer,unet=unet,vae=vae,safety_checker=None,revision=unknown,torch_dtype=weight_dtype;unknown,subfolder=scheduler;unknown,subfolder=vae,revision=unknown;unknown,subfolder=unet,revision=unknown;unknown,text_encoder=accelerator.unwrap_model,vae=vae,unet=unet,tokenizer=tokenizer","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker.from_pretrained,github.com/kangyeolk/Paint-by-Sketch,156,a8080425eea7fd5d3e2cb7c0a9606636,kangyeolk_Paint-by-Sketch/Paint-by-Sketch/scripts/folder_inference.py,safety_model_id,Stable Diffusion-based image manipulation method with a sketch and reference image
diffusers.AutoencoderKL.from_pretrained.to;diffusers.UNet2DConditionModel.from_pretrained.to,github.com/songrise/AvatarCraft,157,2359e8d5fd2a78e86b5fb4d690a0f316,songrise_AvatarCraft/AvatarCraft/models/diffusion.py,unknown;unknown,[ICCV23] AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control
diffusers.StableDiffusionPipeline.from_pretrained;diffusers.DDIMScheduler.from_pretrained,github.com/ashawkey/stable-dreamfusion,7150,08849c2fa16f7036cbfbd8c953d5d4b9,ashawkey_stable-dreamfusion/stable-dreamfusion/guidance/sd_utils.py,"model_key,torch_dtype=unknown;model_key,subfolder=scheduler,torch_dtype=unknown",Text-to-3D & Image-to-3D & Mesh Exportation with NeRF + Diffusion.
diffusers.StableDiffusionInstructPix2PixPipeline.from_pretrained,github.com/Sanster/lama-cleaner,14081,66d88ee429c86673ec2540594bf5913e,Sanster_lama-cleaner/lama-cleaner/lama_cleaner/model/instruct_pix2pix.py,"timbrooks/instruct-pix2pix,revision=unknown,torch_dtype=torch_dtype,None=model_kwargs","Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures."
diffusers.AutoencoderKL.from_pretrained.to;diffusers.UNet2DConditionModel.from_pretrained.to;diffusers.DDIMScheduler.from_pretrained,github.com/junshutang/Make-It-3D,1517,b8baad649af0270cc1db6b53287ea3ff,junshutang_Make-It-3D/Make-It-3D/nerf/sd.py,"unknown;unknown;model_key,subfolder=scheduler",[ICCV 2023] Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior
diffusers.AutoencoderKL.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained,github.com/johannakarras/DreamPose,748,644f2cee00447c8b93b4351d99f80996,johannakarras_DreamPose/DreamPose/finetune-vae.py,"unknown,subfolder=vae,revision=unknown;unknown,subfolder=unet,revision=unknown","Official implementation of ""DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"""
diffusers.ControlNetModel.from_pretrained,github.com/Sanster/lama-cleaner,14081,5074579b0630de2a7d62b9e1c8988988,Sanster_lama-cleaner/lama-cleaner/lama_cleaner/model/controlnet.py,"unknown,torch_dtype=torch_dtype","Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures."
diffusers.UnCLIPPipeline.from_pretrained;diffusers.DiffusionPipeline.from_pretrained,github.com/KU-CVLAB/3DFuse,667,c1c010a97698c4968224c66a7f4f1a79,KU-CVLAB_3DFuse/3DFuse/gradio_app.py,"kakaobrain/karlo-v1-alpha,torch_dtype=unknown;runwayml/stable-diffusion-v1-5","Official implementation of ""Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation"""
diffusers.AutoencoderKL.from_pretrained,github.com/johannakarras/DreamPose,748,78effc1d9329e8593235760530927ade,johannakarras_DreamPose/DreamPose/train.py,"CompVis/stable-diffusion-v1-4,subfolder=vae,revision=ebb811dd71cdc38a204ecbdd6ac5d580f529fd8c","Official implementation of ""DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"""
diffusers.DiffusionPipeline.from_pretrained,github.com/Sanster/lama-cleaner,14081,fbed1e6641310f90baea8cb4c4283335,Sanster_lama-cleaner/lama-cleaner/scripts/tool.py,"cached_folders,torch_dtype=torch_dtype,device_map=device_map","Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures."
diffusers.ControlNetModel.from_pretrained;diffusers.StableDiffusionControlNetPipeline.from_pretrained,github.com/sail-sg/EditAnything,2813,3741c9ba1389878f1cc6dc9ed8567b24,sail-sg_EditAnything/EditAnything/sam2image.py,"controlnet_path,torch_dtype=unknown;base_model_path,controlnet=controlnet,torch_dtype=unknown","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
diffusers.StableDiffusionPipeline.from_pretrained,github.com/peterwilli/sd-leap-booster,111,f80039040b45b4e7054d95c8fac9c3fc,peterwilli_sd-leap-booster/sd-leap-booster/training/dataset_creator/sd_extractor.py,"model_id_or_path,revision=fp16,torch_dtype=unknown",Fast finetuning using a booster model that puts the initial state to a local minimum
diffusers.StableDiffusionControlNetPipeline.from_pretrained,github.com/sail-sg/EditAnything,2813,aabe09fbe87820072a3daddfeeded73a,sail-sg_EditAnything/EditAnything/editany_lora.py,"base_model_path,controlnet=controlnet,torch_dtype=unknown,safety_checker=None","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
diffusers.DDPMScheduler.from_pretrained;diffusers.AutoencoderKL.from_pretrained;diffusers.DDIMScheduler.from_pretrained;diffusers.DDIMScheduler.from_pretrained,github.com/thu-ml/controlvideo,183,4c0b93e9c162574bbf17bbd74ce5c8ed,thu-ml_controlvideo/controlvideo/train.py,"pretrained_model_path,subfolder=scheduler;pretrained_model_path,subfolder=vae;pretrained_model_path,subfolder=scheduler;pretrained_model_path,subfolder=scheduler","Official implementation for ""ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing"" "
diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker.from_pretrained,github.com/kangyeolk/Paint-by-Sketch,156,deac862649954b8c7cf35438d8c0ab92,kangyeolk_Paint-by-Sketch/Paint-by-Sketch/scripts/eval_dataloader.py,safety_model_id,Stable Diffusion-based image manipulation method with a sketch and reference image
diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker.from_pretrained,github.com/cross-domain-compositing/cross-domain-compositing,167,25e2e3fe95d7c601c5a1dd8b881ab978,cross-domain-compositing_cross-domain-compositing/cross-domain-compositing/scripts/txt2img.py,safety_model_id,
diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker.from_pretrained,github.com/kangyeolk/Paint-by-Sketch,156,683567a794db631837edae76a8d4b11d,kangyeolk_Paint-by-Sketch/Paint-by-Sketch/scripts/inference.py,safety_model_id,Stable Diffusion-based image manipulation method with a sketch and reference image
diffusers.DiffusionPipeline.from_pretrained,github.com/Sanster/lama-cleaner,14081,8ef663b3ebc534a4009f57220a087472,Sanster_lama-cleaner/lama-cleaner/lama_cleaner/model/paint_by_example.py,"Fantasy-Studio/Paint-by-Example,torch_dtype=torch_dtype,None=model_kwargs","Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures."
diffusers.AudioLDMPipeline.from_pretrained.to,github.com/gitmylo/audio-webui,657,abd9f87b226350b2c0f3dfdcc1aca8f6,gitmylo_audio-webui/audio-webui/webui/modules/implementations/audioldm.py,map_device,A webui for different audio related Neural Networks
diffusers.DPMSolverMultistepScheduler.from_pretrained,github.com/YuxinWenRick/tree-ring-watermark,145,3c5f723a8b528471af6c48295bf6a963,YuxinWenRick_tree-ring-watermark/tree-ring-watermark/run_tree_ring_watermark_fid.py,"unknown,subfolder=scheduler",
diffusers.StableDiffusionInpaintPipeline.from_pretrained;diffusers.StableDiffusionInpaintPipeline.from_pretrained;diffusers.StableDiffusionInpaintPipeline.from_pretrained,github.com/Uminosachi/inpaint-anything,102,83b1f6f1c5eec42a736266a314828142,Uminosachi_inpaint-anything/inpaint-anything/iasam_app.py,"inp_model_id,torch_dtype=torch_dtype,local_files_only=local_files_only;inp_model_id,torch_dtype=torch_dtype,resume_download=True;inp_model_id,torch_dtype=torch_dtype,force_download=True",Inpaint Anything performs stable diffusion inpainting on a browser UI using masks from Segment Anything.
diffusers.AutoencoderKL.from_pretrained,github.com/Mikubill/naifu-diffusion,222,b915d1acad97bad8d5d25238ecfdcdbb,Mikubill_naifu-diffusion/naifu-diffusion/scripts/encode_to_latent.py,"unknown,subfolder=vae",Train stable diffusion model with Diffusers and Pytorch Lightning
diffusers.DDIMScheduler.from_pretrained;diffusers.StableDiffusionPanoramaPipeline.from_pretrained,github.com/omerbt/MultiDiffusion,774,d861f70ef494759fdd9e42981883ae03,omerbt_MultiDiffusion/MultiDiffusion/app_gradio.py,"model_ckpt,subfolder=scheduler;model_ckpt,scheduler=scheduler,torch_dtype=unknown","Official Pytorch Implementation for ""MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation"" presenting ""MultiDiffusion"" (ICML 2023)"
diffusers.StableDiffusionPipeline.from_pretrained,github.com/Mikubill/naifu-diffusion,222,0c233e7be6e4e8c007b270f14036703f,Mikubill_naifu-diffusion/naifu-diffusion/lib/model.py,unknown,Train stable diffusion model with Diffusers and Pytorch Lightning
diffusers.models.UNet2DConditionModel.from_pretrained,github.com/johannakarras/DreamPose,748,25b5f34344b5e9262dca60dec234412d,johannakarras_DreamPose/DreamPose/models/unet_dual_encoder.py,"CompVis/stable-diffusion-v1-4,subfolder=unet,revision=ebb811dd71cdc38a204ecbdd6ac5d580f529fd8c","Official implementation of ""DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion"""
diffusers.DPMSolverMultistepScheduler.from_pretrained,github.com/YuxinWenRick/tree-ring-watermark,145,6c967dc3e1155231ca2aeed335c38419,YuxinWenRick_tree-ring-watermark/tree-ring-watermark/run_tree_ring_watermark.py,"unknown,subfolder=scheduler",
diffusers.StableDiffusionInstructPix2PixPipeline.from_pretrained.to,github.com/huggingface/instruction-tuned-sd,127,e1acd40fda6c6a7fd80c1d12077ee554,huggingface_instruction-tuned-sd/instruction-tuned-sd/validation/compare_models.py,cuda,Code for instruction-tuning Stable Diffusion.
diffusers.pipelines.stable_diffusion.StableDiffusionInpaintPipeline.from_pretrained,github.com/Sanster/lama-cleaner,14081,e644523738799938c4b01b510130dee0,Sanster_lama-cleaner/lama-cleaner/lama_cleaner/model/sd.py,"unknown,revision=unknown,torch_dtype=torch_dtype,use_auth_token=kwargs,None=model_kwargs","Image inpainting tool powered by SOTA AI Model. Remove any unwanted object, defect, people from your pictures or erase and replace(powered by stable diffusion) any thing on your pictures."
diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker.from_pretrained,github.com/7eu7d7/DreamArtist-stable-diffusion,863,d376e05ec2e05997b496ee5fdc9c1af1,7eu7d7_DreamArtist-stable-diffusion/DreamArtist-stable-diffusion/modules/safety.py,safety_model_id,stable diffusion webui with contrastive prompt tuning
diffusers.StableDiffusionInpaintPipeline.from_pretrained;diffusers.AutoencoderKL.from_pretrained;diffusers.UNet2DConditionModel.from_pretrained;diffusers.StableDiffusionPipeline.from_pretrained;diffusers.DDPMScheduler.from_pretrained,github.com/sail-sg/EditAnything,2813,e749cac0d7c19a11b1029281205c58dd,sail-sg_EditAnything/EditAnything/utils/train_dreambooth_lora_inpaint.py,"unknown,torch_dtype=torch_dtype,safety_checker=None;unknown,subfolder=vae;unknown,subfolder=unet;unknown,text_encoder=text_encoder;unknown,subfolder=scheduler","Edit anything in images  powered by segment-anything, ControlNet, StableDiffusion, etc."
spacy.load,github.com/shmsw25/FActScore,119,8740689e5c846e02189723c98d607dc9,shmsw25_FActScore/FActScore/factscore/atomic_facts.py,en_core_web_sm,"A package to evaluate factuality of long-form generation. Original implementation of our EMNLP 2023 paper ""FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation"""
spacy.load;spacy.load;spacy.load,github.com/INK-USC/MHGRN,238,7f1f7de16f959e9652fb0d8001ed9c21,INK-USC_MHGRN/MHGRN/utils/tokenization_utils.py,en_core_web_sm;en_core_web_sm;en_core_web_sm,Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering (EMNLP 2020)
spacy.load;spacy.load,github.com/robustness-gym/summvis,249,489ba498b2a22751faaf98dc93dce405,robustness-gym_summvis/summvis/summvis.py,en_core_web_lg;en_core_web_sm,SummVis is an interactive visualization tool for text summarization.
spacy.load,github.com/AlibabaResearch/DAMO-ConvAI,788,3ce26295ef03707c0246b52bd65831ab,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/space-1/galaxy/data/field.py,en_core_web_sm,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
spacy.load,github.com/lavis-nlp/spert,624,8d3b864c5035c387a4c78c385caaa30f,lavis-nlp_spert/spert/scripts/conversion/convert_ade.py,spacy_model,PyTorch code for SpERT: Span-based Entity and Relation Transformer
spacy.load,github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis,506,52a8da047188059f858d36cc39c1df44,ScalaConsultants_Aspect-Based-Sentiment-Analysis/Aspect-Based-Sentiment-Analysis/aspect_based_sentiment_analysis/text_splitters.py,name,💭 Aspect-Based-Sentiment-Analysis: Transformer & Explainable ML (TensorFlow)
spacy.load,github.com/flowersteam/Grounding_LLMs_with_online_RL,137,f21a6245ea03bb33d43269a48aa1759f,flowersteam_Grounding_LLMs_with_online_RL/Grounding_LLMs_with_online_RL/babyai-text/babyai/babyai/shaped_env_paral.py,en_core_web_sm,We perform functional grounding of LLMs' knowledge in BabyAI-Text
spacy.load,github.com/rpryzant/neutralizing-bias,185,3d0d50f102fdc1f2f33dc057c4a561cb,rpryzant_neutralizing-bias/neutralizing-bias/harvest/add_tags.py,en_core_web_sm,"Code and data for the paper, ""Automatically Neutralizing Subjective Bias in Text"""
spacy.load;spacy.load,github.com/yangheng95/PyABSA,779,171ca30ad09237ce03dd23341f7f0aee,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectPolarityClassification/dataset_utils/__lcf__/apc_utils.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
spacy.load,github.com/andreamad8/FSB,119,cd483631cede22905345cb549d21f5af,andreamad8_FSB/FSB/data/wow/KILT/kilt/datasets/hotpotqa.py,en_core_web_sm,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
spacy.load,github.com/usc-isi-i2/cskg,104,c682c4213b7285e452568c414e97dcc1,usc-isi-i2_cskg/cskg/grounding/groundcskg/graphify/link.py,SPACY_MODEL,CSKG: The CommonSense Knowledge Graph
spacy.load,github.com/AlibabaResearch/DAMO-ConvAI,788,e726022800e8b57968d75934f33ae782,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/dpr/utils/tokenizers.py,model,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
spacy.load,github.com/avinashkranjan/Amazing-Python-Scripts,1614,4771625916445c856a40a5d5b2bf1c89,avinashkranjan_Amazing-Python-Scripts/Amazing-Python-Scripts/Document-Summary-Creator/preprocessing.py,en_core_web_sm,🚀 Curated collection of Amazing Python scripts from Basics to Advance with automation task scripts.
spacy.load,github.com/facebookresearch/KILT,859,59ce02e2dffffd441b02eff674f8ef01,facebookresearch_KILT/KILT/scripts/create_kilt_data_paragraphs.py,en_core_web_sm,Library for Knowledge Intensive Language Tasks
spacy.load;spacy.load,github.com/yangheng95/PyABSA,779,5e04cc835b01d328635e21b0b124f151,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectPolarityClassification/dataset_utils/__lcf__/apc_utils_for_dlcf_dca.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
spacy.load,github.com/kengz/spacy-nlp,218,c7bc0c649c5352e3e9b0f0b314f12e87,kengz_spacy-nlp/spacy-nlp/src/py/nlp.py,en_core_web_md,Expose Spacy nlp text parsing to Nodejs (and other languages) via socketIO
spacy.load,github.com/HHousen/TransformerSum,405,70a27f69ab9f993b652c870232859ce4,HHousen_TransformerSum/TransformerSum/src/abstractive.py,en_core_web_sm,Models to perform neural summarization (extractive and abstractive) using machine learning transformers and a tool to convert abstractive summarization datasets to the extractive task.
spacy.load,github.com/jzbjyb/FLARE,387,58c2a4a80e5112fb818572c8a47ebd3f,jzbjyb_FLARE/FLARE/src/templates.py,en_core_web_sm,Forward-Looking Active REtrieval-augmented generation (FLARE)
spacy.load;spacy.load,github.com/jim-schwoebel/allie,129,f5d889fa883c7079087cdf5a96ef2a4e,jim-schwoebel_allie/allie/features/text_features/textacy_features.py,en_core_web_sm;en_core_web_sm,"🤖 An automated machine learning framework for audio, text, image, video, or .CSV files (50+ featurizers and 15+ model trainers). Python 3.6 required."
spacy.load,github.com/Yui010206/SeViLA,121,2b692bee2cd6f8702534c7e5fef3b7d6,Yui010206_SeViLA/SeViLA/lavis/models/img2prompt_models/img2prompt_vqa.py,en_core_web_sm,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
spacy.load;spacy.load,github.com/KRR-Oxford/DeepOnto,119,ceebaa3b4622a0040415a9199c2cdcbb,KRR-Oxford_DeepOnto/DeepOnto/src/deeponto/onto/verbalisation.py,en_core_web_sm;en_core_web_sm,A package for ontology engineering with deep learning and language models.
spacy.load,github.com/Yui010206/SeViLA,121,57e51ef24a91aaf846f8656b1b82d0f3,Yui010206_SeViLA/SeViLA/lavis/models/sevila_models/sevila.py,en_core_web_sm,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
spacy.load,github.com/memray/OpenNMT-kpg-release,210,0c869f1fab008c61fdc32540902fd635,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/keyphrase/eval.py,en_core_web_sm,Keyphrase Generation
spacy.load,github.com/boudinfl/ake-datasets,134,10ec7354a9ecfa727ec174366f867145,boudinfl_ake-datasets/ake-datasets/datasets/110-PT-BN-KP/src/preprocess.py,pt_core_news_sm,"Large, curated set of benchmark datasets for evaluating automatic keyphrase extraction algorithms."
spacy.load,github.com/memray/OpenNMT-kpg-release,210,b63f0b11647ff3fdd65372b3528af252,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/kp_evaluate.py,en_core_web_sm,Keyphrase Generation
spacy.load,github.com/robustness-gym/summvis,249,f69089722f75ebc010e16ed85ac179ff,robustness-gym_summvis/summvis/preprocessing.py,en_core_web_lg,SummVis is an interactive visualization tool for text summarization.
spacy.load,github.com/facebookresearch/KILT,859,976e68740c0a4a809b0017e930dfe1ec,facebookresearch_KILT/KILT/kilt/datasets/zero_shot_re.py,en_core_web_sm,Library for Knowledge Intensive Language Tasks
spacy.load,github.com/memray/OpenNMT-kpg-release,210,fac1bc58027b56099a30a1b408b5d2b5,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/keyphrase/kpg_example.py,en_core_web_sm,Keyphrase Generation
spacy.load,github.com/NorskRegnesentral/skweak,899,8e0e8ca814c199ff4fb3fb17679c0042,NorskRegnesentral_skweak/skweak/examples/sentiment/weak_supervision_sentiment.py,nb_core_news_md,skweak: A software toolkit for weak supervision applied to NLP tasks
spacy.load;spacy.load,github.com/facebookresearch/ParlAI,10365,e62ee545af958b6a8b11090c6b0a3f1a,facebookresearch_ParlAI/ParlAI/parlai/tasks/reasoning/reason_types/step_by_step.py,en_core_web_sm;en_core_web_sm,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
spacy.load,github.com/andreamad8/FSB,119,a506ebb91b288f827041b1f1d95ff19a,andreamad8_FSB/FSB/data/wow/KILT/scripts/create_kilt_data_paragraphs.py,en_core_web_sm,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
spacy.load,github.com/andreamad8/FSB,119,c19fe6b734824a523e06ad88d0be660f,andreamad8_FSB/FSB/metric/feqa.py,en_core_web_sm,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
spacy.load,github.com/facebookresearch/KILT,859,e03a32da23f38e16be305e774adf5851,facebookresearch_KILT/KILT/kilt/datasets/hotpotqa.py,en_core_web_sm,Library for Knowledge Intensive Language Tasks
spacy.load;spacy.load,github.com/yangheng95/PyABSA,779,31255ab4b778bad5ac859a1a0245ccfd,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectPolarityClassification/dataset_utils/__classic__/dependency_graph.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
spacy.load;spacy.load,github.com/yangheng95/PyABSA,779,627768d8fc6ef9aa3fba9160ccaa6bdb,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectPolarityClassification/dataset_utils/__plm__/dependency_graph.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
spacy.load,github.com/memray/OpenNMT-kpg-release,210,53e1afb5c240db331df501301c32a40e,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/newssum/docutils.py,en_core_web_sm,Keyphrase Generation
spacy.load;spacy.load;spacy.load,github.com/nabeel-oz/qlik-py-tools,175,b17c24adca01f7981e9a17eae6dabc74,nabeel-oz_qlik-py-tools/qlik-py-tools/core/_spacy.py,unknown;unknown;unknown,Data Science algorithms for Qlik implemented as a Python Server Side Extension (SSE).
spacy.load,github.com/memray/OpenNMT-kpg-release,210,09e4d21fd63e1bb2df9eaa5785adf6f3,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/keyphrase/extract_np.py,en_core_web_sm,Keyphrase Generation
spacy.load,github.com/stanfordnlp/chirpycardinal,125,25676d4773358713328bcf9ac1713cb9,stanfordnlp_chirpycardinal/chirpycardinal/docker/coref/app/remote_module.py,en_core_web_sm,Stanford's Alexa Prize socialbot
spacy.load,github.com/facebookresearch/KILT,859,4485c13487ebdc07174e307c038dacf3,facebookresearch_KILT/KILT/kilt/datasets/fact_verification.py,en_core_web_sm,Library for Knowledge Intensive Language Tasks
spacy.load,github.com/andreamad8/FSB,119,99c03e93006eb7c99407b1ca8ee6fd49,andreamad8_FSB/FSB/data/wow/KILT/kilt/datasets/natural_questions.py,en_core_web_sm,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
spacy.load,github.com/jonatasgrosman/wav2vec2-sprint,143,3531c62cd9f4ac2e1d66e0e9561aa02f,jonatasgrosman_wav2vec2-sprint/wav2vec2-sprint/playground/spellcheck_tests_2.py,fr_core_news_sm,
spacy.load,github.com/awslabs/pptod,148,8b311fb3b6ef6e308bf51d1d9143fcaa,awslabs_pptod/pptod/data/multiwoz/utlis/reader.py,en_core_web_sm,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
spacy.load,github.com/ashkamath/mdetr,903,2fd093c871992bf2a6fe3ffe3e216a3f,ashkamath_mdetr/mdetr/scripts/utils/text.py,en_core_web_sm,
spacy.load,github.com/NorskRegnesentral/skweak,899,bead51f7ae1416366712978cc277196e,NorskRegnesentral_skweak/skweak/skweak/spacy.py,model_path,skweak: A software toolkit for weak supervision applied to NLP tasks
spacy.load,github.com/HHousen/TransformerSum,405,5233fc3206d50c764f85cecc3d7fe0dd,HHousen_TransformerSum/TransformerSum/src/convert_to_extractive.py,en_core_web_sm,Models to perform neural summarization (extractive and abstractive) using machine learning transformers and a tool to convert abstractive summarization datasets to the extractive task.
spacy.load;spacy.load,github.com/yangheng95/PyABSA,779,dde1c1ea91aa79f8c60c1dba87e2518d,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectPolarityClassification/dataset_utils/__classic__/classic_glove_apc_utils.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
spacy.load,github.com/AlibabaResearch/DAMO-ConvAI,788,9b447f484fa96d01a0d855ce6739c530,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/r2sql/sparc/model/grapher.py,en_core_web_sm,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
spacy.load,github.com/Jcharis/Machine-Learning-Web-Apps,525,b816ee7cd22b4cb2a3a7d2359e925487,Jcharis_Machine-Learning-Web-Apps/Machine-Learning-Web-Apps/Build-n-Deploy-Flask-App-with-Waypoint/app/app.py,en_core_web_sm,"Building and Embedding Machine Learning Model into a Web App(With Flask,Streamlit,etc)"
spacy.load,github.com/NorskRegnesentral/skweak,899,4278a3253e868b524aa11b041fb0c0ff,NorskRegnesentral_skweak/skweak/skweak/utils.py,spacy_model_name,skweak: A software toolkit for weak supervision applied to NLP tasks
spacy.load,github.com/ContextScout/gcn_ner,135,41de6f6bcd978f24cd6025f67459c336,ContextScout_gcn_ner/gcn_ner/gcn_ner/utils/aux/__init__.py,en_core_web_md,Graph Convolutional neural network named entity recognition
spacy.load,github.com/bminixhofer/wtpsplit,422,03db11c2748e6ef2d653bc38341d4d30,bminixhofer_wtpsplit/wtpsplit/wtpsplit/evaluation/__init__.py,SPACY_LANG_TO_DP_MODEL,Code for Where's the Point? Self-Supervised Multilingual Punctuation-Agnostic Sentence Segmentation
spacy.load;spacy.load,github.com/usc-isi-i2/cskg,104,7e49ad9a4d114129c08aa5128994f95f,usc-isi-i2_cskg/cskg/grounding/groundcn/graphify/graphify.py,SPACY_MODEL;SPACY_MODEL,CSKG: The CommonSense Knowledge Graph
spacy.load,github.com/facebookresearch/ParlAI,10365,dae342b23777123148294a21e8fbe628,facebookresearch_ParlAI/ParlAI/parlai/tasks/md_gender/wikipedia.py,en_core_web_sm,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
spacy.load,github.com/stanfordnlp/chirpycardinal,125,2ab52cbc127af32ec044e8858d3d636d,stanfordnlp_chirpycardinal/chirpycardinal/docker/colbertinfiller/app/remote_module.py,en_core_web_lg,Stanford's Alexa Prize socialbot
spacy.load;spacy.load,github.com/INK-USC/MHGRN,238,2a67efdfd3cbbf60053db1320f6f0988,INK-USC_MHGRN/MHGRN/utils/grounding.py,en_core_web_sm;en_core_web_sm,Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering (EMNLP 2020)
spacy.load,github.com/AlibabaResearch/DAMO-ConvAI,788,c65cece4b8bb42748526b1b0c1055a73,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/oltqa/dpr/data/tables.py,en_core_web_sm,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
spacy.load,github.com/luogen1996/MCN,131,3be43da9ce6f201a74dbeccf6a7e294f,luogen1996_MCN/MCN/callbacks/eval.py,config,"[CVPR2020] Multi-task Collaborative  Network for Joint  Referring Expression Comprehension and Segmentation, CVPR2020 (oral)"
spacy.load,github.com/memray/OpenNMT-kpg-release,210,07a7cfd7e7732d73e26a6a0725c8546e,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/keyphrase/kpg_example_hfdatasets.py,en_core_web_sm,Keyphrase Generation
spacy.load,github.com/usc-isi-i2/cskg,104,8c6f191ef541fc640c1c99445e04a4b6,usc-isi-i2_cskg/cskg/grounding/groundcn/graphify/link.py,SPACY_MODEL,CSKG: The CommonSense Knowledge Graph
spacy.load;spacy.load,github.com/usc-isi-i2/cskg,104,1637716ed7b51b1a9df58f3d36805b9c,usc-isi-i2_cskg/cskg/grounding/groundcskg/graphify/graphify.py,SPACY_MODEL;SPACY_MODEL,CSKG: The CommonSense Knowledge Graph
spacy.load,github.com/facebookresearch/ParlAI,10365,d7ee47a90ae7e092be838b0f42a0a982,facebookresearch_ParlAI/ParlAI/projects/seeker/tasks/mutators.py,en_core_web_sm,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
spacy.load,github.com/AIGC-Audio/AudioGPT,9397,0e7fb0b57423863f2c1855f53eb9ae13,AIGC-Audio_AudioGPT/AudioGPT/audio_to_text/captioning/utils/build_vocab_spacy.py,en_core_web_sm,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"
spacy.load,github.com/naver/gdc,112,ce89c81a1f66cc162b2b30609825b426,naver_gdc/gdc/cdpg/cdpg/metrics.py,en_core_web_sm,"Code accompanying our papers on the ""Generative Distributional Control"" framework"
spacy.load,github.com/allenai/real-toxicity-prompts,125,07f1550c3a28e3e5ed5d1a61f0353f48,allenai_real-toxicity-prompts/real-toxicity-prompts/scripts/data/count_words.py,model,
spacy.load,github.com/allenai/real-toxicity-prompts,125,5963e7e379a05a106b625299797f22f2,allenai_real-toxicity-prompts/real-toxicity-prompts/scripts/data/create_prompts_dataset.py,en_core_web_sm,
spacy.load,github.com/facebookresearch/ParlAI,10365,3820bf17152215f73279c962a7767366,facebookresearch_ParlAI/ParlAI/parlai/agents/tfidf_retriever/tokenizers/spacy_tokenizer.py,model,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
spacy.load,github.com/andreamad8/FSB,119,de93d6e1856fc6e948ccafa96d399155,andreamad8_FSB/FSB/data/wow/KILT/kilt/datasets/fact_verification.py,en_core_web_sm,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
spacy.load,github.com/stanfordnlp/chirpycardinal,125,18e9274644bb0d475be2b1ac89874009,stanfordnlp_chirpycardinal/chirpycardinal/docker/infiller/app/remote_module.py,en_core_web_lg,Stanford's Alexa Prize socialbot
spacy.load,github.com/luogen1996/MCN,131,9397d9ae70707ed81c3751eb7f33e3c7,luogen1996_MCN/MCN/loader/loader.py,config,"[CVPR2020] Multi-task Collaborative  Network for Joint  Referring Expression Comprehension and Segmentation, CVPR2020 (oral)"
spacy.load,github.com/andreamad8/FSB,119,818b84b2e90d3383d173dd7e1a39ea88,andreamad8_FSB/FSB/data/wow/KILT/kilt/datasets/triviaqa.py,en_core_web_sm,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
spacy.load,github.com/facebookresearch/KILT,859,6cc4d6c3f3d566e5c484a5904874816c,facebookresearch_KILT/KILT/kilt/datasets/triviaqa.py,en_core_web_sm,Library for Knowledge Intensive Language Tasks
spacy.load,github.com/Yui010206/SeViLA,121,1fbddf58641ea256023bcc6bf56d86b7,Yui010206_SeViLA/SeViLA/lavis/models/blip2_models/blip2_fmr.py,en_core_web_sm,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
spacy.load;spacy.load;spacy.load,github.com/msg-systems/holmes-extractor,386,fec44a49a930f87ce364d83d89363a66,msg-systems_holmes-extractor/holmes-extractor/holmes_extractor/manager.py,model_name;model_name;nlp_name,Information extraction from English and German texts based on predicate logic
spacy.load;spacy.load,github.com/memray/OpenNMT-kpg-release,210,1ceb1f0ee72b60154e7c209e19a03383,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/keyphrase/pke/readers.py,unknown;unknown,Keyphrase Generation
spacy.load;spacy.load,github.com/snap-stanford/GreaseLM,200,87799ea6ddfc380052d7da7487bb0387,snap-stanford_GreaseLM/GreaseLM/preprocess_utils/grounding.py,en_core_web_sm;en_core_web_sm,[ICLR 2022 spotlight]GreaseLM: Graph REASoning Enhanced Language Models for Question Answering
spacy.load,github.com/jzbjyb/FLARE,387,9682d8a3db0d03b5db2401c52a2c9c12,jzbjyb_FLARE/FLARE/src/datasets.py,en_core_web_sm,Forward-Looking Active REtrieval-augmented generation (FLARE)
spacy.load,github.com/davidberenstein1957/classy-classification,183,3df06f35c25afca1ca154a38fca3640c,davidberenstein1957_classy-classification/classy-classification/classy_classification/examples/spacy_internal_embeddings.py,en_core_web_md,"This repository contains an easy and intuitive approach to few-shot classification using sentence-transformers or spaCy models, or zero-shot classification with Huggingface. "
spacy.load,github.com/andreamad8/FSB,119,ac742fdf007964b06eede39638264cbe,andreamad8_FSB/FSB/data/wow/KILT/kilt/datasets/zero_shot_re.py,en_core_web_sm,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
spacy.load;spacy.load,github.com/jim-schwoebel/allie,129,5cae03bce7d974cfe7edd8050d00e7d9,jim-schwoebel_allie/allie/features/text_features/spacy_features.py,en_core_web_sm;en_core_web_sm,"🤖 An automated machine learning framework for audio, text, image, video, or .CSV files (50+ featurizers and 15+ model trainers). Python 3.6 required."
spacy.load,github.com/geeks-of-data/knowledge-gpt,240,d58177dc9000ab460110963755a2bc6c,geeks-of-data_knowledge-gpt/knowledge-gpt/knowledgegpt/utils/utils_pdf.py,en_core_web_sm,Extract knowledge from all information sources using gpt and other language models. Index and make Q&A session with information sources.
spacy.load,github.com/openaudiosearch/openaudiosearch,109,e9cfa956a53c0555c65b40f394095231,openaudiosearch_openaudiosearch/openaudiosearch/oas_worker/app/jobs/spacy_pipe.py,model,Open Audio Search
spacy.load,github.com/awslabs/pptod,148,69ad2726d7225f7c3d4840f78a6f1ec8,awslabs_pptod/pptod/E2E_TOD/reader.py,en_core_web_sm,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System (ACL 2022)
spacy.load,github.com/facebookresearch/ParlAI,10365,ed39771e6fceedd4a0a305973bf3fec4,facebookresearch_ParlAI/ParlAI/projects/seeker/utils.py,en_core_web_sm,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
spacy.load,github.com/IBM/zshot,285,aec143a7b80cb07b9f78d982f856089c,IBM_zshot/zshot/zshot/evaluation/run_evaluation.py,en_core_web_sm,Zero and Few shot named entity & relationships recognition
spacy.load,github.com/andreamad8/FSB,119,e0a7f82dc461e08e0fec46d49d60ea38,andreamad8_FSB/FSB/metric/dataflow/core/utterance_tokenizer.py,spacy_model_name,The Few-Shot Bot: Prompt-Based Learning for Dialogue Systems
spacy.load,github.com/Yui010206/SeViLA,121,45cea48fdac4dce0d4567af080357998,Yui010206_SeViLA/SeViLA/lavis/models/blip2_models/blip2_t5.py,en_core_web_sm,[NeurIPS 2023] Self-Chained Image-Language Model for Video Localization and Question Answering
spacy.load,github.com/memray/OpenNMT-kpg-release,210,af1f2652f0d9a3c55c60ba9a0d7bf56c,memray_OpenNMT-kpg-release/OpenNMT-kpg-release/onmt/transforms/keyphrase.py,en_core_web_sm,Keyphrase Generation
spacy.load,github.com/AlibabaResearch/DAMO-ConvAI,788,b182cba7b6422555dd126328fff90b08,AlibabaResearch_DAMO-ConvAI/DAMO-ConvAI/space-3/space/data/fields/gen_field.py,en_core_web_sm,DAMO-ConvAI: The official repository which contains the codebase for Alibaba DAMO Conversational AI.
spacy.load;spacy.load,github.com/yangheng95/PyABSA,779,2aced2dd606e4dec68d29c47ed99197f,yangheng95_PyABSA/PyABSA/pyabsa/tasks/AspectPolarityClassification/dataset_utils/__plm__/classic_bert_apc_utils.py,unknown;unknown,"Sentiment Analysis, Text Classification, Text Augmentation,  Text Adversarial defense, etc.; "
spacy.load,github.com/facebookresearch/KILT,859,f0ef24d34454f1681c44c589c2e73e81,facebookresearch_KILT/KILT/kilt/datasets/natural_questions.py,en_core_web_sm,Library for Knowledge Intensive Language Tasks
spacy.load,github.com/IBM/zshot,285,714170613fba11cb61e00bfe15910c2e,IBM_zshot/zshot/zshot/evaluation/dataset/med_mentions/utils.py,en_core_web_sm,Zero and Few shot named entity & relationships recognition
spacy.load,github.com/fastnlp/CPT,448,45e7c90249cbca8b1093b0871dcbf012,fastnlp_CPT/CPT/pretrain/tasks/orqa/natural_questions/tokenizers.py,model,CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation
spacy.load,github.com/facebookresearch/ParlAI,10365,637da6cb637898c2bdab1c76e0ecc0fe,facebookresearch_ParlAI/ParlAI/projects/k2r/stacked_agent/task/agents.py,en_core_web_sm,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.
