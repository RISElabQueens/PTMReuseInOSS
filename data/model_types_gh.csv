github_url,model_name,model_type
github.com/1rgs/clarity-reader,sentence-transformers/all-MiniLM-L6-v2,nlp
github.com/27182812/chatglm-llama-chinese-insturct,LLaMA-7B,nlp
github.com/27182812/chatglm-llama-chinese-insturct,THUDM/chatglm-6b,nlp
github.com/7eu7d7/dreamartist-stable-diffusion,CompVis/stable-diffusion-safety-checker,vision
github.com/abhijithneilabraham/tableqa,bert-large-uncased-whole-word-masking-finetuned-squad,nlp
github.com/ai-forever/kandinsky-2,bert,nlp
github.com/ai-forever/kandinsky-2,T5,nlp
github.com/ai-forever/kandinsky-2,MT5,nlp
github.com/ai-forever/kandinsky-2,XLM-Roberta-Large-Vit-L-14,multi_modal
github.com/ai-forever/kandinsky-2,CLIP,multi_modal
github.com/aigc-audio/audiogpt,bert-base-uncased,nlp
github.com/aigc-audio/audiogpt,facebook/wav2vec2-base-960h,speech
github.com/aigc-audio/audiogpt,runwayml/stable-diffusion-v1-5,multi_modal
github.com/aigc-audio/audiogpt,Gustavosta/MagicPrompt-Stable-Diffusion,multi_modal
github.com/airsplay/vokenization,albert,nlp
github.com/airsplay/vokenization,bert,nlp
github.com/airsplay/vokenization,DistilBERT,nlp
github.com/airsplay/vokenization,RoBERTa,nlp
github.com/airsplay/vokenization,xlm,nlp
github.com/airsplay/vokenization,XLM-RoBERTa,nlp
github.com/airsplay/vokenization,xlnet,nlp
github.com/ajayjain/dietnerf,resnext50_32x4d,vision
github.com/ajayjain/dietnerf,clip_rn50,multi_modal
github.com/ajayjain/dietnerf,clip_vit,multi_modal
github.com/ajayjain/dietnerf,clip_vit_b16,multi_modal
github.com/ajayjain/dietnerf,clip_rn50x4,multi_modal
github.com/akegarasu/chatglm-webui,THUDM/chatglm-6b,nlp
github.com/alibaba-miil/imagenet21k,mixer_b16_224_miil_in21k,vision
github.com/alibaba-miil/imagenet21k,mobilenetv3_large_100_miil_in21k,vision
github.com/alibaba-miil/imagenet21k,resnet_m_miil_in21k,vision
github.com/alibaba-miil/imagenet21k,vit_base_patch16_224_miil_in21k,vision
github.com/alibabaresearch/advancedliteratemachinery,char_str_base_patch4_3_32_128,vision
github.com/alibabaresearch/advancedliteratemachinery,mgp_str_base_patch4_3_32_128,vision
github.com/alibabaresearch/advancedliteratemachinery,mgp_str_large_patch4_3_32_128,vision
github.com/alibabaresearch/advancedliteratemachinery,mgp_str_small_patch4_3_32_128,vision
github.com/alibabaresearch/advancedliteratemachinery,mgp_str_tiny_patch4_3_32_128,vision
github.com/alibabaresearch/damo-convai,facebook/bart-large,nlp
github.com/alibabaresearch/damo-convai,gpt2,nlp
github.com/alirezazareian/ovr-cnn,bert-base-uncased,nlp
github.com/allenai/comet-atomic-2020,facebook/bart-large,nlp
github.com/allenai/comet-atomic-2020,GPT2,nlp
github.com/allenai/comet-atomic-2020,T5,nlp
github.com/allenai/real-toxicity-prompts,gpt2,nlp
github.com/amazon-science/sccl,distilbert-base-nli-stsb-mean-tokens,nlp
github.com/amazon-science/sccl,distilbert-base-uncased,nlp
github.com/amshaker/swiftformer,SwiftFormer_XS,vision
github.com/andreamad8/fsb,EleutherAI/gpt-j-6B,nlp
github.com/andreamad8/fsb,EleutherAI/gpt-neo-1.3B,nlp
github.com/andreamad8/fsb,EleutherAI/gpt-neo-2.7B,nlp
github.com/andreamad8/fsb,gpt2,nlp
github.com/andreamad8/fsb,gpt2-large,nlp
github.com/andreamad8/fsb,gpt2-medium,nlp
github.com/andreamad8/fsb,gpt2-xl,nlp
github.com/andreamad8/fsb,all-mpnet-base-v2,nlp
github.com/andreamad8/fsb,en_core_web_sm,nlp
github.com/antoyang/tubedetr,resnet101,vision
github.com/antoyang/tubedetr,resnet18,vision
github.com/antoyang/tubedetr,resnet34,vision
github.com/antoyang/tubedetr,roberta-base,nlp
github.com/antoyang/tubedetr,resnet50,vision
github.com/ashawkey/rad-nerf,pierse/wav2vec2-large-xlsr-53-esperanto,speech
github.com/ashawkey/stable-dreamfusion,resnet34,vision
github.com/ashawkey/stable-dreamfusion,bert-base-uncased,nlp
github.com/ashawkey/stable-dreamfusion,vit_large_patch16_384,vision
github.com/ashawkey/stable-dreamfusion,Voicelab/vlt5-base-keywords,nlp
github.com/ashawkey/stable-dreamfusion,Salesforce/blip2-opt-2.7b,multi_modal
github.com/ashkamath/mdetr,roberta-base,nlp
github.com/avinashkranjan/amazing-python-scripts,efficientnet_b2,vision
github.com/avinashkranjan/amazing-python-scripts,en_core_web_sm,nlp
github.com/aws-samples/aws-genai-llm-chatbot,cross-encoder/ms-marco-MiniLM-L-12-v2,nlp
github.com/aws-samples/aws-genai-llm-chatbot,intfloat/multilingual-e5-large,nlp
github.com/aws-samples/aws-genai-llm-chatbot,sentence-transformers/all-MiniLM-L6-v2,nlp
github.com/awslabs/pptod,en_core_web_sm,nlp
github.com/awslabs/pptod,t5-base,nlp
github.com/awslabs/pptod,t5-small,nlp
github.com/awslabs/pptod,facebook/bart-base,nlp
github.com/awslabs/pptod,facebook/bart-large,nlp
github.com/awslabs/pptod,t5-large,nlp
github.com/bminixhofer/wtpsplit,xx_sent_ud_sm,nlp
github.com/bminixhofer/wtpsplit,sat-1l,nlp
github.com/bminixhofer/wtpsplit,sat-1l-sm,nlp
github.com/bminixhofer/wtpsplit,sat-3l,nlp
github.com/bminixhofer/wtpsplit,sat-3l-lora,nlp
github.com/bminixhofer/wtpsplit,sat-3l-sm,nlp
github.com/bminixhofer/wtpsplit,sat-6l,nlp
github.com/bminixhofer/wtpsplit,sat-6l-sm,nlp
github.com/bminixhofer/wtpsplit,sat-9l,nlp
github.com/bminixhofer/wtpsplit,sat-12l,nlp
github.com/bminixhofer/wtpsplit,sat-12l-lora,nlp
github.com/bminixhofer/wtpsplit,sat-12l-sm,nlp
github.com/bofenghuang/vigogne,bofenghuang/vigogne-2-7b-chat,nlp
github.com/bofenghuang/vigogne,bofenghuang/vigogne-2-7b-instruct,nlp
github.com/boudinfl/ake-datasets,pt_core_news_sm,nlp
github.com/brightics/studio,en_core_web_sm,nlp
github.com/chenwu98/cycle-diffusion,bert-base-uncased,nlp
github.com/chenwu98/cycle-diffusion,openai/clip-vit-large-patch14,multi_modal
github.com/chenwu98/cycle-diffusion,ViT-L/14,vision
github.com/china-ai-law-challenge/cail2020,bert-large-uncased,nlp
github.com/china-ai-law-challenge/cail2020,bert-base-uncased,nlp
github.com/chrisociepa/allamo,llama,nlp
github.com/cloneofsimo/paint-with-words-sd,openai/clip-vit-large-patch14,multi_modal
github.com/cloneofsimo/paint-with-words-sd,CompVis/stable-diffusion-v1-4,multi_modal
github.com/cloneofsimo/paint-with-words-sd,runwayml/stable-diffusion-inpainting,multi_modal
github.com/cloneofsimo/paint-with-words-sd,CompVis/stable-diffusion-safety-checker,vision
github.com/clue-ai/chatyuan,ClueAI/ChatYuan-large-v2,nlp
github.com/contextscout/gcn_ner,en_core_web_md,nlp
github.com/cross-domain-compositing/cross-domain-compositing,resnet18,vision
github.com/cross-domain-compositing/cross-domain-compositing,bert-base-uncased,nlp
github.com/cross-domain-compositing/cross-domain-compositing,openai/clip-vit-large-patch14,multi_modal
github.com/cross-domain-compositing/cross-domain-compositing,CompVis/stable-diffusion-safety-checker,vision
github.com/csinva/gpt-paper-title-generator,EleutherAI/gpt-j-6B,nlp
github.com/csinva/gpt-paper-title-generator,EleutherAI/gpt-neo-2.7B,nlp
github.com/csinva/gpt-paper-title-generator,facebook/opt-2.7b,nlp
github.com/cuongnn218/zalo_ltr_2021,resnet152,vision
github.com/cuongnn218/zalo_ltr_2021,FPTAI/vibert-base-cased,nlp
github.com/cuongnn218/zalo_ltr_2021,vinai/phobert-base,nlp
github.com/cuongnn218/zalo_ltr_2021,vinai/phobert-large,nlp
github.com/cuongnn218/zalo_ltr_2021,FPTAI/velectra-base-discriminator-cased,nlp
github.com/danielgross/llamaacademy,jeffwan/vicuna-13b,nlp
github.com/danielgross/teleprompter,all-MiniLM-L6-v2,nlp
github.com/danielgross/teleprompter,jeffwan/vicuna-13b,nlp
github.com/davidberenstein1957/classy-classification,en_core_web_trf,nlp
github.com/davidberenstein1957/classy-classification,typeform/distilbert-base-uncased-mnli,nlp
github.com/davidhuji/capdec,gpt2,nlp
github.com/declare-lab/conv-emotion,resnet152,vision
github.com/declare-lab/conv-emotion,bert-base-uncased,nlp
github.com/deepset-ai/covid-qa,deepset/sentence_bert,nlp
github.com/deepset-ai/covid-qa,bert-base-nli-stsb-mean-tokens,nlp
github.com/demi6od/chatbot,bert-base-uncased,nlp
github.com/demi6od/chatbot,distilgpt2,nlp
github.com/devjwsong/gpt2-dialogue-generation-pytorch,gpt2,nlp
github.com/devjwsong/gpt2-dialogue-generation-pytorch,gpt2-large,nlp
github.com/devjwsong/gpt2-dialogue-generation-pytorch,gpt2-medium,nlp
github.com/devjwsong/gpt2-dialogue-generation-pytorch,gpt2-xl,nlp
github.com/devjwsong/gpt2-dialogue-generation-pytorch,microsoft/DialoGPT-large,nlp
github.com/devjwsong/gpt2-dialogue-generation-pytorch,microsoft/DialoGPT-medium,nlp
github.com/devjwsong/gpt2-dialogue-generation-pytorch,microsoft/DialoGPT-small,nlp
github.com/dpfried/incoder,facebook/incoder-1B,nlp
github.com/dpfried/incoder,facebook/incoder-6B,nlp
github.com/dqshuai/metaformer,bert-base-uncased,nlp
github.com/eleutherai/gpt-neox,EleutherAI,nlp
github.com/facebookresearch/kilt,t5,nlp
github.com/facebookresearch/kilt,en_core_web_sm,nlp
github.com/facebookresearch/kilt,t5-base,nlp
github.com/facebookresearch/kilt,t5-large,nlp
github.com/facebookresearch/kilt,t5-3b,nlp
github.com/facebookresearch/kilt,t5-11b,nlp
github.com/facebookresearch/multihop_dense_retrieval,Roberta,nlp
github.com/facebookresearch/multihop_dense_retrieval,google/electra-large-discriminator,nlp
github.com/facebookresearch/multihop_dense_retrieval,roberta-base,nlp
github.com/facebookresearch/ov-seg,ViT-B-32-quickgelu,vision
github.com/facebookresearch/ov-seg,RN50,vision
github.com/facebookresearch/ov-seg,RN50-quickgelu,vision
github.com/facebookresearch/ov-seg,RN101,vision
github.com/facebookresearch/ov-seg,RN101-quickgelu,vision
github.com/facebookresearch/ov-seg,RN50x4,vision
github.com/facebookresearch/ov-seg,RN50x16,vision
github.com/facebookresearch/ov-seg,RN50x64,vision
github.com/facebookresearch/ov-seg,ViT-B-32,vision
github.com/facebookresearch/ov-seg,ViT-B-16,vision
github.com/facebookresearch/ov-seg,ViT-B-16-plus-240,vision
github.com/facebookresearch/ov-seg,ViT-L-14,vision
github.com/facebookresearch/ov-seg,ViT-L-14-336,vision
github.com/facebookresearch/parlai,resnet152,vision
github.com/facebookresearch/parlai,resnet101,vision
github.com/facebookresearch/parlai,resnet50,vision
github.com/facebookresearch/parlai,resnet34,vision
github.com/facebookresearch/parlai,resnet18,vision
github.com/facebookresearch/parlai,resnet152_spatial,vision
github.com/facebookresearch/parlai,resnet101_spatial,vision
github.com/facebookresearch/parlai,resnet50_spatial,vision
github.com/facebookresearch/parlai,resnet34_spatial,vision
github.com/facebookresearch/parlai,resnet18_spatial,vision
github.com/facebookresearch/parlai,resnext101_32x8d_wsl,vision
github.com/facebookresearch/parlai,resnext101_32x16d_wsl,vision
github.com/facebookresearch/parlai,resnext101_32x32d_wsl,vision
github.com/facebookresearch/parlai,resnext101_32x48d_wsl,vision
github.com/facebookresearch/parlai,resnext101_32x8d_wsl_spatial,vision
github.com/facebookresearch/parlai,resnext101_32x16d_wsl_spatial,vision
github.com/facebookresearch/parlai,resnext101_32x32d_wsl_spatial,vision
github.com/facebookresearch/parlai,resnext101_32x48d_wsl_spatial,vision
github.com/facebookresearch/parlai,bert-base-uncased,nlp
github.com/facebookresearch/parlai,t5-small,nlp
github.com/facebookresearch/parlai,t5-base,nlp
github.com/facebookresearch/parlai,t5-large,nlp
github.com/facebookresearch/parlai,Vamsi/T5_Paraphrase_Paws,nlp
github.com/facebookresearch/parlai,meta-llama/Llama-2-7b-hf,nlp
github.com/facebookresearch/tart,facebook/tart-full-flan-t0-3b,nlp
github.com/facebookresearch/tart,facebook/tart-full-flan-t5-xl,nlp
github.com/farizrahman4u/loopgpt,sentence-transformers/all-roberta-large-v1,nlp
github.com/farizrahman4u/loopgpt,huggyllama/llama-7b,nlp
github.com/farizrahman4u/loopgpt,stabilityai/stablelm-tuned-alpha-7b,nlp
github.com/fastnlp/cpt,fnlp/cpt-base,nlp
github.com/fishaudio/fish-diffusion,hubert_soft,speech
github.com/fishaudio/fish-diffusion,TencentGameMate/chinese-hubert-base,speech
github.com/fishaudio/fish-diffusion,TencentGameMate/chinese-hubert-large,speech
github.com/fishaudio/fish-diffusion,meta-llama/Llama-2-7b-hf,nlp
github.com/fishaudio/fish-diffusion,baichuan-inc/Baichuan2-7B-Base,nlp
github.com/flowersteam/grounding_llms_with_online_rl,resnet18,vision
github.com/flowersteam/grounding_llms_with_online_rl,hf-internal-testing/mrpc-bert-base-cased,nlp
github.com/flowersteam/grounding_llms_with_online_rl,bert-base-cased,nlp
github.com/flowersteam/grounding_llms_with_online_rl,resnet50d,vision
github.com/flowersteam/grounding_llms_with_online_rl,EleutherAI/gpt-j-6B,nlp
github.com/geeks-of-data/knowledge-gpt,emrecan/bert-base-turkish-cased-mean-nli-stsb-tr,nlp
github.com/geeks-of-data/knowledge-gpt,en_core_web_sm,nlp
github.com/geeks-of-data/knowledge-gpt,sentence-transformers/all-MiniLM-L6-v2,nlp
github.com/gitmylo/audio-webui,sanchit-gandhi/clap-htsat-unfused-m-full,multi_modal
github.com/gitmylo/audio-webui,bert-base-multilingual-cased,nlp
github.com/gitmylo/audio-webui,facebook/hubert-base-ls960,speech
github.com/gitmylo/audio-webui,cvssp/audioldm-m-full,audio
github.com/guifachild/text_to_vedio,THUDM/chatglm-6b,nlp
github.com/guzpenha/transformer_rankers,bert-base-nli-stsb-mean-tokens,nlp
github.com/guzpenha/transformer_rankers,all-MiniLM-L6-v2,nlp
github.com/guzpenha/transformer_rankers,t5-small,nlp
github.com/guzpenha/transformer_rankers,t5-11b,nlp
github.com/guzpenha/transformer_rankers,t5-3b,nlp
github.com/guzpenha/transformer_rankers,t5-base,nlp
github.com/guzpenha/transformer_rankers,t5-large,nlp
github.com/haoheliu/audioldm,bert-base-uncased,nlp
github.com/haoheliu/audioldm,facebook/bart-base,nlp
github.com/haoheliu/audioldm,roberta-base,nlp
github.com/hazyresearch/domino,gpt2,nlp
github.com/hazyresearch/domino,bert-large-cased,nlp
github.com/hhousen/transformersum,distil,nlp
github.com/hhousen/transformersum,roberta,nlp
github.com/hhousen/transformersum,en_core_web_sm,nlp
github.com/hhousen/transformersum,longformer,nlp
github.com/hhousen/transformersum,bert-base-uncased,nlp
github.com/huggingface/instruction-tuned-sd,timbrooks/instruct-pix2pix,multi_modal
github.com/huggingface/instruction-tuned-sd,runwayml/stable-diffusion-v1-5,multi_modal
github.com/ibm/zshot,ibm/smxm,nlp
github.com/ibm/zshot,gpt2,nlp
github.com/ibm/zshot,en_core_web_sm,nlp
github.com/ink-usc/mhgrn,en_core_web_sm,nlp
github.com/ink-usc/mhgrn,RoBERTa,nlp
github.com/ink-usc/mhgrn,BERT,nlp
github.com/ink-usc/mhgrn,XLNet,nlp
github.com/ink-usc/mhgrn,GPT,nlp
github.com/isl-org/dpt,resnext101_32x8d_wsl,vision
github.com/isl-org/dpt,vit_base_resnet50,vision
github.com/jcharis/machine-learning-web-apps,en_core_web_sm,nlp
github.com/jim-schwoebel/allie,bert-base-nli-mean-tokens,nlp
github.com/jim-schwoebel/allie,facebook/hubert-large-ls960-ft,speech
github.com/jim-schwoebel/allie,en_core_web_sm,nlp
github.com/johannakarras/dreampose,openai/clip-vit-base-patch32,multi_modal
github.com/johannakarras/dreampose,CompVis/stable-diffusion-v1-4,multi_modal
github.com/jonatasgrosman/wav2vec2-sprint,en_core_web_sm,nlp
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large,speech
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large-100k-voxpopuli,speech
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large-10k-voxpopuli,speech
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large-es-voxpopuli,speech
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large-fr-voxpopuli,speech
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large-it-voxpopuli,speech
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large-nl-voxpopuli,speech
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large-sv-voxpopuli,speech
github.com/jonatasgrosman/wav2vec2-sprint,facebook/wav2vec2-large-xlsr-53,speech
github.com/jonatasgrosman/wav2vec2-sprint,ja_core_news_sm,nlp
github.com/junshutang/make-it-3d,resnext101_32x8d_wsl,vision
github.com/junshutang/make-it-3d,openai/clip-vit-large-patch14,multi_modal
github.com/junshutang/make-it-3d,runwayml/stable-diffusion-v1-5,multi_modal
github.com/junshutang/make-it-3d,Salesforce/blip2-opt-2.7b,multi_modal
github.com/junshutang/make-it-3d,stabilityai/stable-diffusion-2-base,multi_modal
github.com/jzbjyb/flare,en_core_web_sm,nlp
github.com/kangyeolk/paint-by-sketch,openai/clip-vit-large-patch14,multi_modal
github.com/kangyeolk/paint-by-sketch,CompVis/stable-diffusion-safety-checker,vision
github.com/karthikv792/llms-planning,bigscience/bloom,nlp
github.com/kengz/spacy-nlp,en_core_web_md,nlp
github.com/kmeng01/memit,gpt2-xl,nlp
github.com/kmeng01/memit,EleutherAI/gpt-j-6B,nlp
github.com/kmeng01/memit,gpt-2,nlp
github.com/krr-oxford/deeponto,bert-base-uncased,nlp
github.com/ku-cvlab/3dfuse,rwightman/gen-efficientnet-pytorch,vision
github.com/ku-cvlab/3dfuse,openai/clip-vit-large-patch14,multi_modal
github.com/ku-cvlab/3dfuse,resnext101_32x8d_wsl,vision
github.com/ku-cvlab/3dfuse,ViT-H-14,vision
github.com/ku-cvlab/3dfuse,kakaobrain/karlo-v1-alpha,multi_modal
github.com/ku-cvlab/3dfuse,google/t5-v1_1-large,nlp
github.com/ku-cvlab/3dfuse,tf_efficientnet_lite3,vision
github.com/ku-cvlab/3dfuse,runwayml/stable-diffusion-v1-5,multi_modal
github.com/labteral/ernie,bert-base-uncased,nlp
github.com/lavis-nlp/spert,allenai/scibert_scivocab_uncased,nlp
github.com/learnedvector/a-hackers-ai-voice-assistant,bert-base-uncased,nlp
github.com/lensterxyz/lenster,bert,nlp
github.com/leonnnop/gmmseg,ResNet,vision
github.com/leonnnop/gmmseg,ResNeXt,vision
github.com/leonnnop/gmmseg,ResNeSt,vision
github.com/leonnnop/gmmseg,MobileNetV2,vision
github.com/leonnnop/gmmseg,MobileNetV3,vision
github.com/leonnnop/gmmseg,ViT,vision
github.com/leonnnop/gmmseg,Swin,vision
github.com/leonnnop/gmmseg,Twins,vision
github.com/leonnnop/gmmseg,MIT,vision
github.com/lipurple/grounded-diffusion,deeplabv2_resnet101,vision
github.com/lipurple/grounded-diffusion,openai/clip-vit-large-patch14,multi_modal
github.com/lipurple/grounded-diffusion,MiDaS,vision
github.com/lipurple/grounded-diffusion,bert-base-uncased,nlp
github.com/locuslab/convmixer,timm,vision
github.com/loubnabnl/santacoder-finetuning,bigcode/santacoder,nlp
github.com/luogen1996/mcn,en_vectors_web_lg,nlp
github.com/lupantech/scienceqa,nlpconnect/vit-gpt2-image-captioning,multi_modal
github.com/memray/opennmt-kpg-release,en_core_web_sm,nlp
github.com/michiyasunaga/linkbert,llama-7b-emb-zh,nlp
github.com/microsoft/dialogpt,microsoft/DialoGPT-large,nlp
github.com/microsoft/dialogpt,gpt,nlp
github.com/microsoft/jericho,en_core_web_sm,nlp
github.com/mikubill/naifu-diffusion,stabilityai/stable-diffusion-xl-base-1.0,multi_modal
github.com/mikubill/naifu-diffusion,PixArt-alpha/PixArt-XL-2-1024-MS,multi_modal
github.com/mikubill/naifu-diffusion,madebyollin/sdxl-vae-fp16-fix,multi_modal
github.com/mikubill/naifu-diffusion,stabilityai/sd-vae-ft-mse,multi_modal
github.com/mikubill/naifu-diffusion,facebook/opt-350m,nlp
github.com/mikubill/naifu-diffusion,microsoft/phi-1,nlp
github.com/mikubill/naifu-diffusion,SimianLuo/LCM_Dreamshaper_v7,multi_modal
github.com/msg-systems/holmes-extractor,en_core_web_trf,nlp
github.com/nabeel-oz/qlik-py-tools,en_core_web_sm,nlp
github.com/naver/gdc,gpt2,nlp
github.com/naver/gdc,gpt2-medium,nlp
github.com/naver/gdc,microsoft/DialogRPT-updown,nlp
github.com/naver/gdc,en_core_web_sm,nlp
github.com/neulab/knn-transformers,neulab/gpt2-finetuned-wikitext103,nlp
github.com/neulab/knn-transformers,gpt2,nlp
github.com/neutralzz/billa,llama,nlp
github.com/norskregnesentral/skweak,en_core_web_md,nlp
github.com/norskregnesentral/skweak,en_core_web_sm,nlp
github.com/norskregnesentral/skweak,nb_core_news_md,nlp
github.com/nvlabs/a-vit,avit_tiny_patch16_224,vision
github.com/nyannyanovich/nyan,openai/clip-vit-base-patch32,multi_modal
github.com/robustbench/robustbench,intfloat/multilingual-e5-base,nlp
github.com/okojoalg/sequencer,dpn92,vision
github.com/omerbt/multidiffusion,stabilityai/sd-vae-ft-mse,multi_modal
github.com/omerbt/multidiffusion,monster-labs/control_v1p_sd15_qrcode_monster,multi_modal
github.com/omerbt/multidiffusion,lllyasviel/sd-controlnet-canny,multi_modal
github.com/omerbt/multidiffusion,runwayml/stable-diffusion-v1-5,multi_modal
github.com/omerbt/multidiffusion,stabilityai/stable-diffusion-2-base,multi_modal
github.com/openaudiosearch/openaudiosearch,silero_vad,audio
github.com/openaudiosearch/openaudiosearch,de_core_news_lg,nlp
github.com/peterwilli/sd-leap-booster,stabilityai/stable-diffusion-2-1-base.,multi_modal
github.com/phellonchen/x-llm,bert-base-chinese,nlp
github.com/prakhar21/textaugmentation-gpt2,gpt2-medium,nlp
github.com/princeton-nlp/mezo,facebook/opt-125m,nlp
github.com/qanastek/hugsvision,facebook/detr-resnet-50,vision
github.com/qanastek/hugsvision,DeiT,vision
github.com/qanastek/hugsvision,google/vit-base-patch16-224-in21k,vision
github.com/qiuhuachuan/smile,qiuhuachuan/MeChat,nlp
github.com/qiuhuachuan/smile,THUDM/chatglm2-6b,nlp
github.com/renatoviolin/semantic-search,bert-base-nli-stsb-mean-tokens,nlp
github.com/renatoviolin/semantic-search,roberta-base-nli-stsb-mean-tokens,nlp
github.com/renatoviolin/semantic-search,bert-base-uncased,nlp
github.com/rinnakk/japanese-pretrained-models,RoBERTa,nlp
github.com/rinnakk/japanese-pretrained-models,GPT-2,nlp
github.com/robustness-gym/summvis,google/pegasus-cnn_dailymail,nlp
github.com/robustness-gym/summvis,google/pegasus-multi_news,nlp
github.com/robustness-gym/summvis,google/pegasus-newsroom,nlp
github.com/robustness-gym/summvis,google/pegasus-xsum,nlp
github.com/robustness-gym/summvis,facebook/bart-large-cnn,nlp
github.com/robustness-gym/summvis,facebook/bart-large-xsum,nlp
github.com/rongjiehuang/generspeech,facebook/wav2vec2-base-960h,speech
github.com/rpryzant/neutralizing-bias,en_core_web_sm,nlp
github.com/ruc-gsai/yulan-ir,bart,nlp
github.com/ruc-gsai/yulan-ir,T5,nlp
github.com/ruc-gsai/yulan-ir,GPT3,nlp
github.com/rucaibox/unisrec,bert-base-uncased,nlp
github.com/sail-sg/editanything,tf_efficientnet_lite3,vision
github.com/sail-sg/editanything,resnext101_32x8d_wsl,vision
github.com/sail-sg/editanything,Salesforce/blip2-opt-2.7b,multi_modal
github.com/salesforce/coderl,Salesforce/codet5-base,nlp
github.com/salesforce/coderl,Salesforce/codet5-large,nlp
github.com/salesforce/unicontrol,tf_efficientnet_lite3,vision
github.com/salesforce/unicontrol,resnext101_32x8d_wsl,vision
github.com/salesforce/unicontrol,CompVis/stable-diffusion-safety-checker,vision
github.com/sanchit-gandhi/whisper-jax,openai/whisper-large-v2,speech
github.com/sanster/lama-cleaner,runwayml/stable-diffusion-inpainting,multi_modal
github.com/sanster/lama-cleaner,Sanster/anything-4.0-inpainting,multi_modal
github.com/sanster/lama-cleaner,Sanster/Realistic_Vision_V1.4-inpainting,multi_modal
github.com/sanster/lama-cleaner,stabilityai/stable-diffusion-2-inpainting,multi_modal
github.com/scalaconsultants/aspect-based-sentiment-analysis,en_core_web_sm,nlp
github.com/scutcyr/soulchat,scutcyr/SoulChat,nlp
github.com/scutlihaoyu/open-chat-video-editor,stabilityai/stable-diffusion-2-1,multi_modal
github.com/scutlihaoyu/open-chat-video-editor,M-CLIP/XLM-Roberta-Large-Vit-L-14,multi_modal
github.com/seujung/kobart-summarization,gogamza/kobart-base-v1,nlp
github.com/seujung/kobart-summarization,digit82/kobart-summarization,nlp
github.com/shark-nlp/cont,google/pegasus-xsum,nlp
github.com/shark-nlp/cont,t5,nlp
github.com/shark-nlp/cont,codet5,nlp
github.com/shi-labs/compact-transformers,resnet101,vision
github.com/shibing624/medicalgpt,Qwen/Qwen2.5-0.5B-Instruct,nlp
github.com/shibing624/medicalgpt,Qwen/Qwen2.5-0.5B,nlp
github.com/shibing624/medicalgpt,01-ai/Yi-6B-Chat,nlp
github.com/shmsw25/factscore,llama-7B,nlp
github.com/siddk/voltron-robotics,distilbert-base-uncased,nlp
github.com/skeskinen/bert.cpp,all-MiniLM-L6-v2,nlp
github.com/snap-stanford/greaselm,roberta-large,nlp
github.com/snap-stanford/greaselm,en_core_web_sm,nlp
github.com/songhaoyu/bob,bart.large.cnn,nlp
github.com/songhaoyu/bob,bert-base-uncased,nlp
github.com/songrise/avatarcraft,runwayml/stable-diffusion-v1-5,multi_modal
github.com/songrise/avatarcraft,stabilityai/stable-diffusion-2-depth,multi_modal
github.com/ssundaram21/dreamsim,dino_vits8,vision
github.com/ssundaram21/dreamsim,dino_vits16,vision
github.com/ssundaram21/dreamsim,dino_vitb8,vision
github.com/ssundaram21/dreamsim,dino_vitb16,vision
github.com/ssundaram21/dreamsim,clip_vitb16,multi_modal
github.com/ssundaram21/dreamsim,clip_vitb32,multi_modal
github.com/ssundaram21/dreamsim,clip_vitl14,multi_modal
github.com/ssundaram21/dreamsim,mae_vitb16,vision
github.com/ssundaram21/dreamsim,mae_vitl16,vision
github.com/ssundaram21/dreamsim,mae_vith14,vision
github.com/ssundaram21/dreamsim,open_clip_vitb16,multi_modal
github.com/ssundaram21/dreamsim,open_clip_vitb32,multi_modal
github.com/ssundaram21/dreamsim,open_clip_vitl14,multi_modal
github.com/ssundaram21/dreamsim,facebook/vit-mae-base,vision
github.com/ssymmetry/bbt-fincuge-applications,BBT-FinT5,nlp
github.com/stanfordnlp/chirpycardinal,bert-large-uncased-whole-word-masking-finetuned-squad,nlp
github.com/stanfordnlp/chirpycardinal,facebook/bart-large,nlp
github.com/stanfordnlp/chirpycardinal,microsoft/DialogRPT-updown,nlp
github.com/stanfordnlp/chirpycardinal,bert-base-uncased,nlp
github.com/stanleylsx/entity_extractor_by_pointer,bert-base-chinese,nlp
github.com/sturdy-dev/semantic-code-search,krlvi/sentence-msmarco-bert-base-dot-v5-nlpl-code_search_net,nlp
github.com/sylinrl/truthfulqa,EleutherAI/gpt-neo-2.7B,nlp
github.com/tabtoyou/kollava,vicuna-7b,nlp
github.com/tabtoyou/kollava,facebook/opt-125m,nlp
github.com/tabtoyou/kollava,llama-vicuna-13b,nlp
github.com/tcapelle/apple_m1_pro_python,bert-base-cased,nlp
github.com/tencentarc/animesr,vit_base_patch8_224,vision
github.com/the-crypt-keeper/can-ai-code,google/gemma-2-9b-it,nlp
github.com/the-crypt-keeper/can-ai-code,ModelCloud/gemma-2-27b-it-gptq-4bit,nlp
github.com/theovercomer8/captionr,microsoft/git-large-r-textcaps,multi_modal
github.com/thu-ml/controlvideo,tf_efficientnet_lite3,vision
github.com/thu-ml/controlvideo,resnext101_32x8d_wsl,vision
github.com/thu-ml/controlvideo,resnet50,vision
github.com/thu-ml/controlvideo,stable-diffusion-v1-5,multi_modal
github.com/thunlp/kernelgat,allenai/scibert_scivocab_uncased,nlp
github.com/thunlp/kernelgat,RoBERTa,nlp
github.com/thunlp/openbackdoor,bert-base-uncased,nlp
github.com/thunlp/openbackdoor,gpt2,nlp
github.com/thunlp/openbackdoor,gpt2-large,nlp
github.com/timothybrooks/instruct-pix2pix,bert-base-uncased,nlp
github.com/timothybrooks/instruct-pix2pix,openai clip,multi_modal
github.com/timothybrooks/instruct-pix2pix,CompVis/stable-diffusion-v1-4,multi_modal
github.com/timothybrooks/instruct-pix2pix,CompVis/stable-diffusion-safety-checker,vision
github.com/twistedcubic/attention-rank-collapse,bert-base-uncased,nlp
github.com/twistedcubic/attention-rank-collapse,bert-base-cased,nlp
github.com/ucinlp/autoprompt,bert-base-cased,nlp
github.com/ucinlp/autoprompt,roberta,nlp
github.com/uminosachi/inpaint-anything,runwayml/stable-diffusion-inpainting,multi_modal
github.com/uminosachi/inpaint-anything,stabilityai/stable-diffusion-2-inpainting,multi_modal
github.com/uminosachi/inpaint-anything,Uminosachi/deliberate_v3-inpainting,multi_modal
github.com/uminosachi/inpaint-anything,Uminosachi/dreamshaper_8Inpainting,multi_modal
github.com/uminosachi/inpaint-anything,Uminosachi/realisticVisionV51_v51VAE-inpainting,multi_modal
github.com/uminosachi/inpaint-anything,Uminosachi/revAnimated_v121Inp-inpainting,multi_modal
github.com/usc-isi-i2/cskg,bert-base-nli-mean-tokens,nlp
github.com/usc-isi-i2/cskg,bert-large-nli-cls-token,nlp
github.com/usc-isi-i2/cskg,roberta,nlp
github.com/usc-isi-i2/cskg,en_core_web_lg,nlp
github.com/v-mipeng/lexiconaugmentedner,bert-base-chinese,nlp
github.com/vahe1994/spqr,gpt2,nlp
github.com/veekaybee/viberary,sentence-transformers/msmarco-distilbert-base-v3,nlp
github.com/videocrafter/videocrafter,google/t5-v1_1-large,nlp
github.com/videocrafter/videocrafter,ViT-H-14,vision
github.com/vijishmadhavan/skindeep,thepowefuldeez/sd21-controlnet-canny,multi_modal
github.com/vinairesearch/xphonebert,vinai/xphonebert-base,nlp
github.com/wangyuxinwhy/uniem,moka-ai/m3e-base,nlp
github.com/wangyuxinwhy/uniem,all-MiniLM-L6-v2,nlp
github.com/wangyuxinwhy/uniem,IDEA-CCNL/Erlangshen-SimCSE-110M-Chinese,nlp
github.com/wangyuxinwhy/uniem,silk-road/luotuo-bert,nlp
github.com/winddori2002/triaan-vc,facebook/wav2vec2-large-960h-lv60-self,speech
github.com/wzhouad/atlop,bert-base-cased,nlp
github.com/xpitfire/symbolicai,blip,multi_modal
github.com/xpitfire/symbolicai,clip,multi_modal
github.com/yahshibu/nested-ner-tacl2020-transformers,bert-large-uncased,nlp
github.com/yangheng95/pyabsa,facebook/bart-base,nlp
github.com/yangheng95/pyabsa,roberta-base,nlp
github.com/yangheng95/pyabsa,Salesforce/codet5-base,nlp
github.com/yangheng95/pyabsa,Salesforce/codet5-small,nlp
github.com/yangheng95/pyabsa,t5-base,nlp
github.com/ylsung/vl_adapter,facebook/bart-base,nlp
github.com/ylsung/vl_adapter,t5-base,nlp
github.com/ylsung/vl_adapter,bert-base-uncased,nlp
github.com/ypwhs/creativechatglm,glm-4,nlp
github.com/ypwhs/creativechatglm,chatglm3,nlp
github.com/ypwhs/creativechatglm,chatglm2,nlp
github.com/ypwhs/creativechatglm,chatglm,nlp
github.com/ypwhs/creativechatglm,gptq,nlp
github.com/ypwhs/creativechatglm,llama,nlp
github.com/yuangongnd/whisper-at,facebook/hubert-large-ls960-ft,speech
github.com/yuangongnd/whisper-at,facebook/hubert-xlarge-ls960-ft,speech
github.com/yuangongnd/whisper-at,facebook/wav2vec2-base-960h,speech
github.com/yuangongnd/whisper-at,facebook/wav2vec2-large-robust-ft-swbd-300h,speech
github.com/yuchenlin/swiftsage,paraphrase-MiniLM-L6-v2,nlp
github.com/yuchenlin/swiftsage,Meta-Llama-3.1-8B-Instruct,nlp
github.com/yui010206/sevila,ResNet-50,vision
github.com/yui010206/sevila,ViT-B-16,vision
github.com/yui010206/sevila,ViT-B-32,vision
github.com/yui010206/sevila,ViT-L-14-336,vision
github.com/yui010206/sevila,ViT-L-14,vision
github.com/yui010206/sevila,google/flan-t5-xl,nlp
github.com/yuxinwenrick/tree-ring-watermark,stabilityai/stable-diffusion-2-1-base,multi_modal
github.com/zerorin/bertgcn,roberta-base,nlp
github.com/zetaalphavector/inpars,EleutherAI/gpt-j-6B,nlp
github.com/zetaalphavector/inpars,t5-base,nlp
github.com/zetaalphavector/inpars,cross-encoder/ms-marco-MiniLM-L-6-v2,nlp
github.com/zetaalphavector/inpars,castorini/monot5-base-msmarco-10k,nlp
github.com/zhoudaquan/refiner_vit,resnet101,vision
github.com/zhoujx4/nlp-series-sentence-embeddings,bert,nlp
github.com/zhoujx4/nlp-series-sentence-embeddings,openai/clip-vit-base-patch32,multi_modal
github.com/zhoujx4/nlp-series-sentence-embeddings,t5,nlp
github.com/zhoujx4/nlp-series-sentence-embeddings,roberta,nlp
github.com/zhoujx4/nlp-series-sentence-embeddings,distilbert-base-nli-mean-tokens,nlp
github.com/zhvng/open-musiclm,bert-base-uncased,nlp
github.com/zhvng/open-musiclm,facebook/bart-base,nlp
github.com/zhvng/open-musiclm,m-a-p/MERT-v0,audio
github.com/zhvng/open-musiclm,roberta-base,nlp
github.com/zjunlp/ontoprotein,PubMedBERT,nlp
github.com/zjunlp/ontoprotein,ProtBERT,nlp
github.com/acht7111020/dsmap,vgg16,vision
github.com/activevisionlab/nope-nerf,vitb_rn50_384,vision
github.com/activevisionlab/nope-nerf,vitb16_384,vision
github.com/activevisionlab/nope-nerf,vitl16_384,vision
github.com/adamkortylewski/compositionalnets,resnet50,vision
github.com/adamkortylewski/compositionalnets,resnext50_32x4d,vision
github.com/adamkortylewski/compositionalnets,vgg16,vision
github.com/ai4ce/sscbench,tf_efficientnet_b7_ns,vision
github.com/aimagelab/multimodal-garment-designer,mgd,multi_modal
github.com/aimerykong/predictive-filter-flow,resnet50,vision
github.com/aimerykong/predictive-filter-flow,resnet18,vision
github.com/akanazawa/cmr,resnet18,vision
github.com/akanazawa/vgan,inception_v3,vision
github.com/akshitagupta15june/face-x,resnet34,vision
github.com/amanchadha/iseebetter,vgg16,vision
github.com/andrewsonga/total-recon,alexnet,vision
github.com/andrewsonga/total-recon,densenet201,vision
github.com/andrewsonga/total-recon,vgg16,vision
github.com/andrewsonga/total-recon,vgg19,vision
github.com/andrewsonga/total-recon,resnet101,vision
github.com/andrewsonga/total-recon,resnet152,vision
github.com/andrewsonga/total-recon,resnet18,vision
github.com/andrewsonga/total-recon,resnet34,vision
github.com/andrewsonga/total-recon,resnet50,vision
github.com/andrewsonga/total-recon,squeezenet1_1,vision
github.com/anything-of-anything/anything-3d,tf_efficientnet_lite3,vision
github.com/anything-of-anything/anything-3d,resnext101_32x8d_wsl,vision
github.com/apple/ml-finerecon,efficientnet_v2_s,vision
github.com/arpitbansal297/universal-guided-diffusion,fasterrcnn_resnet50_fpn_v2,vision
github.com/arpitbansal297/universal-guided-diffusion,lraspp_mobilenet_v3_large,vision
github.com/arunmallya/piggyback,densenet121,vision
github.com/arunmallya/piggyback,resnet50,vision
github.com/arunmallya/piggyback,vgg16,vision
github.com/arunmallya/piggyback,vgg16_bn,vision
github.com/ashawkey/nerf2mesh,tf_efficientnet_lite3,vision
github.com/ashawkey/stable-dreamfusion,resnext101_32x8d_wsl,vision
github.com/ashawkey/stable-dreamfusion,resnet101,vision
github.com/ashawkey/stable-dreamfusion,resnet152,vision
github.com/ashawkey/stable-dreamfusion,resnet18,vision
github.com/ashawkey/stable-dreamfusion,resnet50,vision
github.com/ashawkey/stable-dreamfusion,alexnet,vision
github.com/ashawkey/stable-dreamfusion,squeezenet1_1,vision
github.com/ashawkey/stable-dreamfusion,vgg16,vision
github.com/ashual/scene_generation,resnet152,vision
github.com/ashual/scene_generation,resnet18,vision
github.com/ashual/scene_generation,resnet34,vision
github.com/ashual/scene_generation,resnet50,vision
github.com/ashual/scene_generation,resnet101,vision
github.com/ashual/scene_generation,vgg19,vision
github.com/autonise/craft-remade,vgg16_bn,vision
github.com/autonise/craft-remade,resnet50,vision
github.com/autonomousvision/occupancy_networks,resnet50,vision
github.com/autonomousvision/occupancy_networks,resnet18,vision
github.com/autonomousvision/occupancy_networks,resnet34,vision
github.com/autonomousvision/texture_fields,inception_v3,vision
github.com/autonomousvision/texture_fields,resnet18,vision
github.com/avanetten/yoltv5,yolov5s,vision
github.com/biubug6/pytorch_retinaface,resnet50,vision
github.com/boschresearch/unetgan,inception_v3,vision
github.com/bshall/hubert,hubert_soft,speech
github.com/cap-ntu/video-to-retail-platform,resnet50,vision
github.com/cap-ntu/video-to-retail-platform,alexnet,vision
github.com/cap-ntu/video-to-retail-platform,resnet18,vision
github.com/captaineven/videocaption,resnet50,vision
github.com/carolineec/informative-drawings,inception_v3,vision
github.com/cassiepython/cddfm3d,alexnet,vision
github.com/cassiepython/cddfm3d,inception_v3,vision
github.com/cassiepython/cddfm3d,resnet101,vision
github.com/cassiepython/cddfm3d,resnet152,vision
github.com/cassiepython/cddfm3d,resnet18,vision
github.com/cassiepython/cddfm3d,resnet34,vision
github.com/cassiepython/cddfm3d,resnet50,vision
github.com/cassiepython/cddfm3d,squeezenet1_1,vision
github.com/cassiepython/cddfm3d,vgg16,vision
github.com/castacks/dytanvo,MiDaS,vision
github.com/chenyanglei/flash-reflection-removal,vgg19,vision
github.com/choosehappy/pytorchdigitalpathology,alexnet,vision
github.com/choosehappy/pytorchdigitalpathology,vgg19,vision
github.com/chungyiweng/humannerf,alexnet,vision
github.com/chungyiweng/humannerf,resnet101,vision
github.com/chungyiweng/humannerf,resnet152,vision
github.com/chungyiweng/humannerf,resnet18,vision
github.com/chungyiweng/humannerf,resnet34,vision
github.com/chungyiweng/humannerf,resnet50,vision
github.com/chungyiweng/humannerf,squeezenet1_1,vision
github.com/chungyiweng/humannerf,vgg16,vision
github.com/cientgu/giqa,inception_v3,vision
github.com/cleinc/bts,resnet101,vision
github.com/cleinc/bts,resnet50,vision
github.com/cleinc/bts,resnext101_32x8d,vision
github.com/cleinc/bts,resnext50_32x4d,vision
github.com/cleinc/bts,densenet121,vision
github.com/cleinc/bts,densenet161,vision
github.com/cleinc/bts,mobilenet_v2,vision
github.com/clovaai/fewshot-font-generation,resnet50,vision
github.com/cmu-create-lab/deep-smoke-machine,resnet101,vision
github.com/cmu-create-lab/deep-smoke-machine,googlenet,vision
github.com/cmu-create-lab/deep-smoke-machine,alexnet,vision
github.com/cnyvfang/labelgo-yolov5autolabelimg,yolov5s,vision
github.com/codeslake/color_transfer_histogram_analogy,vgg19,vision
github.com/codeslake/color_transfer_histogram_analogy,vgg19_bn,vision
github.com/coincheung/pytorch-loss,resnet50,vision
github.com/collabora/whisper-live,silero_vad,audio
github.com/cross-domain-compositing/cross-domain-compositing,vgg16,vision
github.com/csjliang/ppr10k,resnet18,vision
github.com/csjliang/ppr10k,resnet34,vision
github.com/cszn/kair,resnet50,vision
github.com/cszn/kair,vgg19,vision
github.com/cszn/kair,vgg19_bn,vision
github.com/cuiaiyu/dressing-in-order,inception_v3,vision
github.com/cuiaiyu/dressing-in-order,vgg19,vision
github.com/cvg/hierarchical-localization,vgg16_netvlad,vision
github.com/cvg/hierarchical-localization,mberton/MegaLoc,vision
github.com/cvg/limap,vgg16,vision
github.com/cvqluu/simple_diarizer,silero_vad,audio
github.com/daizuozhuo/batch-dropblock-network,resnet50,vision
github.com/dariopavllo/convmesh,inception_v3,vision
github.com/datamol-io/molfeat,esm1b_t33_650M_UR50S,nlp
github.com/dchen236/fairface,resnet34,vision
github.com/debeshjha/2020-cbms-doubleu-net,vgg19,vision
github.com/declare-lab/reccon,resnet152,vision
github.com/dingmyu/d4lcn,resnet101,vision
github.com/dingmyu/d4lcn,resnet50,vision
github.com/dingmyu/dapn,resnet50,vision
github.com/dl-practise/yoloall,yolov5s,vision
github.com/donsetpg/narya,resnet50,vision
github.com/duanzhiihao/rapid,resnet101,vision
github.com/duanzhiihao/rapid,resnet34,vision
github.com/duanzhiihao/rapid,resnet50,vision
github.com/duguyifei/yolov5_fps_aicheatprinciple,yolov5n,vision
github.com/duguyifei/yolov5_fps_aicheatprinciple,yolov5s,vision
github.com/dukebw/loho,alexnet,vision
github.com/dukebw/loho,resnet101,vision
github.com/dukebw/loho,resnet152,vision
github.com/dukebw/loho,resnet18,vision
github.com/dukebw/loho,resnet34,vision
github.com/dukebw/loho,resnet50,vision
github.com/dukebw/loho,squeezenet1_1,vision
github.com/dukebw/loho,vgg16,vision
github.com/dukebw/loho,vgg19,vision
github.com/eezkni/uegan,vgg19,vision
github.com/egorsmkv/speech-recognition-uk,silero_stt,audio
github.com/egorzakharov/perceptualgan,vgg19,vision
github.com/eps696/aphantasia,ViT-B/16,vision
github.com/executedone/chinese-fastspeech2,load_melgan,audio
github.com/facebookresearch/vlpart,dino_vits8,vision
github.com/facebookresearch/vlpart,dino_vits16,vision
github.com/facebookresearch/vlpart,dino_vitb8,vision
github.com/facebookresearch/vlpart,dino_vitb16,vision
github.com/facebookresearch/vlpart,vit_small_patch8_224,vision
github.com/facebookresearch/vlpart,vit_small_patch16_224,vision
github.com/facebookresearch/vlpart,vit_base_patch8_224,vision
github.com/facebookresearch/vlpart,vit_base_patch16_224,vision
github.com/fudanvi/fudanocr,vgg16,vision
github.com/garfield-kh/posetriplet,resnet18,vision
github.com/gcucurull/visual-compatibility,resnet50,vision
github.com/geekyutao/inpaint-anything,alexnet,vision
github.com/geekyutao/inpaint-anything,inception_v3,vision
github.com/geekyutao/inpaint-anything,vgg19,vision
github.com/geekyutao/inpaint-anything,resnet101,vision
github.com/geekyutao/inpaint-anything,resnet152,vision
github.com/geekyutao/inpaint-anything,resnet18,vision
github.com/geekyutao/inpaint-anything,resnet34,vision
github.com/geekyutao/inpaint-anything,resnet50,vision
github.com/geekyutao/inpaint-anything,squeezenet1_1,vision
github.com/geekyutao/inpaint-anything,vgg16,vision
github.com/gm19900510/pytorch_retina_license_plate,resnet50,vision
github.com/google-research/3d-moments,vgg16,vision
github.com/google-research/3d-moments,vgg19,vision
github.com/grigorisg9gr/polynomial_nets,inception_v3,vision
github.com/grip-unina/dmimagedetection,resnet18,vision
github.com/hejingwenhejingwen/csrnet,vgg19,vision
github.com/hejingwenhejingwen/csrnet,vgg19_bn,vision
github.com/hendrycks/imagenet-r,resnet101,vision
github.com/hendrycks/imagenet-r,resnet152,vision
github.com/hendrycks/imagenet-r,resnet50,vision
github.com/hezhangsprinter/did-mdn,vgg19_bn,vision
github.com/hongwenzhang/pymaf-x,resnet50,vision
github.com/ianyeung/realvsr,vgg19,vision
github.com/ianyeung/realvsr,vgg19_bn,vision
github.com/isl-org/midas,tf_efficientnet_lite3,vision
github.com/isl-org/midas,resnext101_32x8d_wsl,vision
github.com/isl-org/vi-depth,tf_efficientnet_lite3,vision
github.com/isl-org/vi-depth,DPT_BEiT_L_512,vision
github.com/isl-org/vi-depth,DPT_SwinV2_L_384,vision
github.com/isl-org/vi-depth,DPT_Large,vision
github.com/isl-org/vi-depth,DPT_SwinV2_T_256,vision
github.com/isl-org/vi-depth,DPT_LeViT_224,vision
github.com/isl-org/vi-depth,MiDaS_small,vision
github.com/j-min/dalleval,inception_v3,vision
github.com/jakaria08/eesrgan,fasterrcnn_resnet50_fpn,vision
github.com/jakaria08/eesrgan,vgg19,vision
github.com/jakaria08/eesrgan,vgg19_bn,vision
github.com/jansonyuan/pytorch-camp,deeplabv3_resnet101,vision
github.com/jansonyuan/pytorch-camp,alexnet,vision
github.com/jansonyuan/pytorch-camp,fasterrcnn_resnet50_fpn,vision
github.com/jayleicn/tvretrieval,resnet152,vision
github.com/jiawei-yang/freenerf,clip_rn50,multi_modal
github.com/jiawei-yang/freenerf,clip_vit,multi_modal
github.com/jiawei-yang/freenerf,clip_vit_b16,multi_modal
github.com/jiawei-yang/freenerf,resnext50_32x4d,vision
github.com/jinghuizhou/awesome_face_antispoofing,resnet18,vision
github.com/jinghuizhou/awesome_face_antispoofing,densenet121,vision
github.com/jinghuizhou/awesome_face_antispoofing,densenet161,vision
github.com/jinghuizhou/awesome_face_antispoofing,densenet169,vision
github.com/jinghuizhou/awesome_face_antispoofing,densenet201,vision
github.com/jinghuizhou/awesome_face_antispoofing,inception_v3,vision
github.com/jinghuizhou/awesome_face_antispoofing,resnet34,vision
github.com/jinghuizhou/awesome_face_antispoofing,resnet50,vision
github.com/jinghuizhou/awesome_face_antispoofing,squeezenet1_0,vision
github.com/jinghuizhou/awesome_face_antispoofing,squeezenet1_1,vision
github.com/jinghuizhou/awesome_face_antispoofing,vgg11_bn,vision
github.com/jinghuizhou/awesome_face_antispoofing,vgg13_bn,vision
github.com/jinghuizhou/awesome_face_antispoofing,vgg16_bn,vision
github.com/jinghuizhou/awesome_face_antispoofing,vgg19_bn,vision
github.com/jingyang2017/face-and-image-super-resolution,inception_v3,vision
github.com/junukcha/multiperson,resnet101,vision
github.com/junukcha/multiperson,resnet18,vision
github.com/junukcha/multiperson,resnet34,vision
github.com/junukcha/multiperson,resnet50,vision
github.com/kair-bair/dycheck,mttr_refer_youtube_vos,multi_modal
github.com/kaleido-lab/dolphin,tf_efficientnet_lite3,vision
github.com/kaleido-lab/dolphin,resnext101_32x8d_wsl,vision
github.com/kamalesh0406/audio-classification,resnet50,vision
github.com/kamalesh0406/audio-classification,densenet201,vision
github.com/keonlee9420/comprehensive-transformer-tts,load_melgan,audio
github.com/keonlee9420/portaspeech,load_melgan,audio
github.com/kexinyi/ns-vqa,resnet34,vision
github.com/korrawe/grasping_field,resnet18,vision
github.com/lancopku/livebot,resnet18,vision
github.com/layumi/university1652-baseline,vgg16_bn,vision
github.com/layumi/university1652-baseline,resnet50,vision
github.com/leftthomas/srgan,vgg16,vision
github.com/lelechen63/talking-head-generation-with-rhythmic-head-motion,vgg19,vision
github.com/levyfan/reid-mgn,resnet50,vision
github.com/limacv/deblur-nerf,resnet101,vision
github.com/limacv/deblur-nerf,resnet152,vision
github.com/limacv/deblur-nerf,resnet18,vision
github.com/limacv/deblur-nerf,resnet34,vision
github.com/limacv/deblur-nerf,resnet50,vision
github.com/linto-ai/whisper-timestamped,silero_vad,audio
github.com/lkeab/gsnet,densenet121,vision
github.com/lkeab/gsnet,densenet161,vision
github.com/lkeab/gsnet,densenet169,vision
github.com/lkeab/gsnet,densenet201,vision
github.com/lkeab/gsnet,squeezenet1_1,vision
github.com/longcw/motdt,squeezenet1_1,vision
github.com/lornatang/srgan-pytorch,vgg19,vision
github.com/lotayou/face-renovation,alexnet,vision
github.com/lotayou/face-renovation,inception_v3,vision
github.com/lotayou/face-renovation,resnet101,vision
github.com/lotayou/face-renovation,resnet152,vision
github.com/lotayou/face-renovation,resnet18,vision
github.com/lotayou/face-renovation,resnet34,vision
github.com/lotayou/face-renovation,resnet50,vision
github.com/lotayou/face-renovation,squeezenet1_1,vision
github.com/lotayou/face-renovation,vgg16,vision
github.com/lotayou/face-renovation,vgg19,vision
github.com/lturing/orb_slam3_modified,DPT_BEiT_L_384,vision
github.com/lturing/orb_slam3_modified,ZoeD_K,vision
github.com/lturing/orb_slam3_modified,ZoeD_N,vision
github.com/lturing/orb_slam3_modified,ZoeD_NK,vision
github.com/lucidrains/muse-maskgit-pytorch,vgg16,vision
github.com/luzhisheng/js_reverse,yolov5n,vision
github.com/luzhisheng/js_reverse,yolov5s,vision
github.com/lyndonzheng/pluralistic-inpainting,inception_v3,vision
github.com/lyp-deeplearning/mos-multi-task-face-detect,mobilenet_v2,vision
github.com/lyp-deeplearning/mos-multi-task-face-detect,shufflenet_v2_x1_0,vision
github.com/manurare/360monodepth,MiDaS,vision
github.com/manurare/360monodepth,DPT_Large,vision
github.com/manurare/360monodepth,MiDaS_small,vision
github.com/manurare/360monodepth,DPT_Hybrid,vision
github.com/marcalcarazf/realtime-2d-to-3d-faces,densenet121,vision
github.com/marcalcarazf/realtime-2d-to-3d-faces,inception_v3,vision
github.com/marcalcarazf/realtime-2d-to-3d-faces,resnet152,vision
github.com/marcalcarazf/realtime-2d-to-3d-faces,squeezenet1_0,vision
github.com/marcalcarazf/realtime-2d-to-3d-faces,alexnet,vision
github.com/marcelampc/d3net_depth_estimation,alexnet,vision
github.com/marcelampc/d3net_depth_estimation,vgg16,vision
github.com/marcelampc/d3net_depth_estimation,vgg19,vision
github.com/marshuang80/gloria,densenet121,vision
github.com/marshuang80/gloria,densenet161,vision
github.com/marshuang80/gloria,densenet169,vision
github.com/marshuang80/gloria,resnet18,vision
github.com/marshuang80/gloria,resnet34,vision
github.com/marshuang80/gloria,resnet50,vision
github.com/marshuang80/gloria,resnext101_32x8d,vision
github.com/marshuang80/gloria,resnext50_32x4d,vision
github.com/maryamboneh/vehicle-detection,yolov5s,vision
github.com/michael-ovo/burn-detection-classification,WongKinYiu/yolov7,vision
github.com/midasklr/yolov5ds,yolov5s,vision
github.com/ming71/dal,resnet101,vision
github.com/ming71/dal,resnet152,vision
github.com/ming71/dal,resnet34,vision
github.com/ming71/dal,resnet50,vision
github.com/ming71/dal,resnext50_32x4d,vision
github.com/mratsim/amazon-forest-computer-vision,resnet101,vision
github.com/mratsim/amazon-forest-computer-vision,resnet152,vision
github.com/mratsim/amazon-forest-computer-vision,densenet121,vision
github.com/mratsim/amazon-forest-computer-vision,resnet50,vision
github.com/mratsim/amazon-forest-computer-vision,vgg16,vision
github.com/msight-tech/research-ms-loss,resnet50,vision
github.com/neptune-ai/open-solution-mapping-challenge,resnet34,vision
github.com/neptune-ai/open-solution-mapping-challenge,vgg11,vision
github.com/neptune-ai/open-solution-mapping-challenge,resnet101,vision
github.com/neptune-ai/open-solution-mapping-challenge,resnet152,vision
github.com/neptune-ai/open-solution-mapping-challenge,vgg16,vision
github.com/nikolazubic/2dimageto3dmodel,inception_v3,vision
github.com/nileshkulkarni/csm,alexnet,vision
github.com/nileshkulkarni/csm,vgg16,vision
github.com/nileshkulkarni/csm,resnet18,vision
github.com/nirvanalan/e3dge,resnet18,vision
github.com/nirvanalan/e3dge,resnet34,vision
github.com/nirvanalan/e3dge,resnet50,vision
github.com/nirvanalan/e3dge,alexnet,vision
github.com/nirvanalan/e3dge,squeezenet1_1,vision
github.com/nirvanalan/e3dge,vgg16,vision
github.com/niwhskal/srnet,vgg19,vision
github.com/nmilosev/pytorch-arm-builds,shufflenet_v2_x1_0,vision
github.com/nv-tlabs/meta-sim,inception_v3,vision
github.com/nvlabs/few-shot-vid2vid,vgg19,vision
github.com/nvlabs/prismer,resnext101_32x8d_wsl,vision
github.com/nvlabs/prismer,resnext101_32x8d,vision
github.com/nvlabs/ssv,alexnet,vision
github.com/nvlabs/umr,alexnet,vision
github.com/nvlabs/umr,resnet101,vision
github.com/nvlabs/umr,resnet152,vision
github.com/nvlabs/umr,resnet18,vision
github.com/nvlabs/umr,resnet34,vision
github.com/nvlabs/umr,resnet50,vision
github.com/nvlabs/umr,squeezenet1_1,vision
github.com/nvlabs/umr,vgg16,vision
github.com/ofa-sys/daflow,alexnet,vision
github.com/ofa-sys/daflow,resnet101,vision
github.com/ofa-sys/daflow,resnet152,vision
github.com/ofa-sys/daflow,resnet18,vision
github.com/ofa-sys/daflow,resnet34,vision
github.com/ofa-sys/daflow,resnet50,vision
github.com/ofa-sys/daflow,squeezenet1_1,vision
github.com/ofa-sys/daflow,vgg16,vision
github.com/ofa-sys/daflow,vgg19,vision
github.com/opengvlab/internvideo,resnet50,vision
github.com/osu-nlp-group/magicbrush,dino_vits16,vision
github.com/pbcquoc/vietocr,vgg19_bn,vision
github.com/pbcquoc/vietocr,vgg11_bn,vision
github.com/pliang279/multibench,resnet50,vision
github.com/pliang279/multibench,r3d_18,vision
github.com/pliang279/multibench,resnet18,vision
github.com/pliang279/multibench,vgg19,vision
github.com/pliang279/multibench,vgg11_bn,vision
github.com/pliang279/multibench,vgg16_bn,vision
github.com/potterhsu/easy-faster-rcnn.pytorch,resnet101,vision
github.com/potterhsu/easy-faster-rcnn.pytorch,resnet18,vision
github.com/potterhsu/easy-faster-rcnn.pytorch,resnet50,vision
github.com/prabhuomkar/iris,efficientnet_b7,vision
github.com/prabhuomkar/iris,resnet152,vision
github.com/pranoyr/cnn-lstm,resnet101,vision
github.com/pudae/kaggle-understanding-clouds,resnext101_32x8d_wsl,vision
github.com/pudae/kaggle-understanding-clouds,resnext101_32x16d_wsl,vision
github.com/pudae/kaggle-understanding-clouds,resnext101_32x32d_wsl,vision
github.com/pudae/kaggle-understanding-clouds,resnext101_32x48d_wsl,vision
github.com/rowanz/r2c,resnet50,vision
github.com/rozumden/defmo,resnet50,vision
github.com/rsummers11/cadlab,vgg16_bn,vision
github.com/sagizty/insight,resnet101,vision
github.com/sagizty/insight,resnet34,vision
github.com/sagizty/insight,resnet50,vision
github.com/sagizty/insight,maskrcnn_resnet50_fpn,vision
github.com/sagizty/insight,resnet18,vision
github.com/salesforce/unicontrol,tf_efficientnet_lite3,vision
github.com/salesforce/unicontrol,resnext101_32x8d_wsl,vision
github.com/sczhou/davanet,vgg19,vision
github.com/seokju-cho/volumetric-aggregation-transformer,resnet101,vision
github.com/seokju-cho/volumetric-aggregation-transformer,resnet50,vision
github.com/seokju-cho/volumetric-aggregation-transformer,vgg16,vision
github.com/seoungwugoh/rgmp,resnet50,vision
github.com/servicenow/lcfcn,resnet50,vision
github.com/seutao/humpback-whale-identification,resnet101,vision
github.com/sharpiless/yolov5-deepsort,yolov5s,vision
github.com/shihaozhaozsh/uni-controlnet,tf_efficientnet_lite3,vision
github.com/shihaozhaozsh/uni-controlnet,resnext101_32x8d_wsl,vision
github.com/shubhtuls/factored3d,resnet18,vision
github.com/skhadem/3d-boundingbox,vgg19_bn,vision
github.com/stability-ai/stablediffusion,tf_efficientnet_lite3,vision
github.com/stability-ai/stablediffusion,resnext101_32x8d_wsl,vision
github.com/sungfeng-huang/meta-tts,load_melgan,audio
github.com/taconite/metaavatar-release,resnet152,vision
github.com/taeu/style-your-hair,alexnet,vision
github.com/taeu/style-your-hair,resnet101,vision
github.com/taeu/style-your-hair,resnet152,vision
github.com/taeu/style-your-hair,resnet18,vision
github.com/taeu/style-your-hair,resnet34,vision
github.com/taeu/style-your-hair,resnet50,vision
github.com/taeu/style-your-hair,squeezenet1_1,vision
github.com/taeu/style-your-hair,vgg16,vision
github.com/taeu/style-your-hair,vgg19,vision
github.com/taohan10200/iim,resnet50,vision
github.com/tding1/cdfi,vgg16,vision
github.com/tekakutli/anime_translation,silero_vad,audio
github.com/ternaus/robot-surgery-segmentation,resnet34,vision
github.com/ternaus/robot-surgery-segmentation,vgg11,vision
github.com/ternaus/robot-surgery-segmentation,vgg16,vision
github.com/thygate/stable-diffusion-webui-depthmap-script,DPT_BEiT_L_384,vision
github.com/thygate/stable-diffusion-webui-depthmap-script,tf_efficientnet_lite3,vision
github.com/thygate/stable-diffusion-webui-depthmap-script,resnext101_32x8d_wsl,vision
github.com/tri-ml/vidar,resnet101,vision
github.com/tri-ml/vidar,resnet50,vision
github.com/tri-ml/vidar,resnext101_32x8d,vision
github.com/tri-ml/vidar,resnext50_32x4d,vision
github.com/tri-ml/vidar,densenet121,vision
github.com/tri-ml/vidar,densenet161,vision
github.com/tum-vision/tandem,resnet18,vision
github.com/udacity/cvnd---image-captioning-project,resnet50,vision
github.com/universome/alis,wide_resnet50_2,vision
github.com/utra-robosoccer/soccerbot,ultralytics/yolov5,vision
github.com/valeoai/lost,resnet50,vision
github.com/valeoai/lost,vgg16,vision
github.com/vikashplus/robohive,resnet18,vision
github.com/vikashplus/robohive,resnet34,vision
github.com/vikashplus/robohive,resnet50,vision
github.com/vinid/path_eval,resnet101,vision
github.com/vinid/path_eval,resnet18,vision
github.com/vinid/path_eval,resnet50,vision
github.com/vinid/path_eval,vit_b_16,vision
github.com/vinid/path_eval,vit_b_32,vision
github.com/vinthony/deep-blind-watermark-removal,vgg16,vision
github.com/vinthony/deep-blind-watermark-removal,vgg19,vision
github.com/visdrone/dronecrowd,vgg16,vision
github.com/voidism/diffcse,roberta,nlp
github.com/voidism/diffcse,bert,nlp
github.com/wanggrun/sysu-30k,densenet121,vision
github.com/wanggrun/sysu-30k,resnet50,vision
github.com/wangguanan/aligngan,resnet50,vision
github.com/wangguanan/pytorch-person-reid-baseline-pcb-beyond-part-models,resnet50,vision
github.com/wanglibo1995/geoseg,resnet18,vision
github.com/wanglibo1995/geoseg,resnet34,vision
github.com/wangt-cn/vc-r-cnn,resnet50,vision
github.com/wangwenhao0716/isc-track1-submission,yolov5s,vision
github.com/we0091234/car_recognition,resnet18,vision
github.com/williamyang1991/styleganex,alexnet,vision
github.com/williamyang1991/styleganex,resnet101,vision
github.com/williamyang1991/styleganex,resnet152,vision
github.com/williamyang1991/styleganex,resnet18,vision
github.com/williamyang1991/styleganex,resnet34,vision
github.com/williamyang1991/styleganex,resnet50,vision
github.com/williamyang1991/styleganex,squeezenet1_1,vision
github.com/williamyang1991/styleganex,vgg16,vision
github.com/woozzu/dong_iccv_2017,vgg16_bn,vision
github.com/woozzu/dong_iccv_2017,vgg16,vision
github.com/wutianyirosun/cgnet,resnet18,vision
github.com/wxj630/visual-chatgpt-zh,tf_efficientnet_lite3,vision
github.com/wxj630/visual-chatgpt-zh,resnext101_32x8d_wsl,vision
github.com/wzmiaomiao/deep-learning-for-image-processing,mobilenet_v3_large,vision
github.com/wzmiaomiao/deep-learning-for-image-processing,vgg16_bn,vision
github.com/wzmiaomiao/deep-learning-for-image-processing,yolov5s,vision
github.com/wzzheng/tpvformer,tf_efficientnet_b7_ns,vision
github.com/xichenpan/arldm,inception_v3,vision
github.com/xinranzh/projectaim-apexlegends,yolov5s,vision
github.com/xinzelee/polygonobjectdetection,yolov5s,vision
github.com/xpixelgroup/ranksrgan,vgg19,vision
github.com/xpixelgroup/ranksrgan,vgg19_bn,vision
github.com/xuebinqin/basnet,resnet34,vision
github.com/xuxy09/texformer,resnet50,vision
github.com/yeexiaozheng/multimodal-sentiment-analysis,resnet50,vision
github.com/yejin0111/add-gcn,resnet101,vision
github.com/ygjwd12345/transdepth,densenet121,vision
github.com/ygjwd12345/transdepth,densenet161,vision
github.com/ygjwd12345/transdepth,resnet101,vision
github.com/ygjwd12345/transdepth,resnet50,vision
github.com/ygjwd12345/transdepth,resnext101_32x8d,vision
github.com/ygjwd12345/transdepth,resnext50_32x4d,vision
github.com/ygtxr1997/celebbasis,mobilenet_v2,vision
github.com/ygtxr1997/celebbasis,resnet101,vision
github.com/ygtxr1997/celebbasis,resnet18,vision
github.com/ygtxr1997/celebbasis,resnet50,vision
github.com/ygtxr1997/celebbasis,bert-base-uncased,nlp
github.com/ygtxr1997/celebbasis,clip,multi_modal
github.com/yhjo09/ciplab-ntire-2020,alexnet,vision
github.com/yhjo09/ciplab-ntire-2020,resnet101,vision
github.com/yhjo09/ciplab-ntire-2020,resnet152,vision
github.com/yhjo09/ciplab-ntire-2020,resnet18,vision
github.com/yhjo09/ciplab-ntire-2020,resnet34,vision
github.com/yhjo09/ciplab-ntire-2020,resnet50,vision
github.com/yhjo09/ciplab-ntire-2020,squeezenet1_1,vision
github.com/yhjo09/ciplab-ntire-2020,vgg16,vision
github.com/yiranran/unpaired-portrait-drawing,alexnet,vision
github.com/yiranran/unpaired-portrait-drawing,resnet101,vision
github.com/yiranran/unpaired-portrait-drawing,resnet152,vision
github.com/yiranran/unpaired-portrait-drawing,resnet18,vision
github.com/yiranran/unpaired-portrait-drawing,resnet34,vision
github.com/yiranran/unpaired-portrait-drawing,resnet50,vision
github.com/yiranran/unpaired-portrait-drawing,squeezenet1_1,vision
github.com/yiranran/unpaired-portrait-drawing,vgg16,vision
github.com/yiyang7/super_resolution_with_cnns_and_gans,vgg16,vision
github.com/yu45020/text_segmentation_image_inpainting,resnet50,vision
github.com/yu45020/text_segmentation_image_inpainting,vgg16,vision
github.com/yudewang/deeplabv3plus-pytorch,resnet101,vision
github.com/yudewang/deeplabv3plus-pytorch,resnet152,vision
github.com/yudewang/deeplabv3plus-pytorch,resnet18,vision
github.com/yudewang/deeplabv3plus-pytorch,resnet34,vision
github.com/yudewang/deeplabv3plus-pytorch,resnet50,vision
github.com/yufengm/adaptive,resnet152,vision
github.com/yuliangguo/pytorch_generalized_3d_lane_detection,vgg16,vision
github.com/yuliangguo/pytorch_generalized_3d_lane_detection,vgg16_bn,vision
github.com/yuliangxiu/icon,resnet50,vision
github.com/yuliangxiu/icon,resnet101,vision
github.com/yuliangxiu/icon,resnet18,vision
github.com/yuliangxiu/icon,resnet34,vision
github.com/yuliangxiu/icon,maskrcnn_resnet50_fpn,vision
github.com/yuliangxiu/icon,vgg19,vision
github.com/yunpengzhai/meb-net,densenet121,vision
github.com/yunpengzhai/meb-net,inception_v3,vision
github.com/yuval-alaluf/stylegan3-editing,resnet34,vision
github.com/yuvalkirstain/pickscore,inception_v3,vision
github.com/yuxiangsun/rtfnet,resnet101,vision
github.com/yuxiangsun/rtfnet,resnet152,vision
github.com/yuxiangsun/rtfnet,resnet18,vision
github.com/yuxiangsun/rtfnet,resnet50,vision
github.com/yuxiangsun/rtfnet,resnet34,vision
github.com/z1069614715/objectdetection_script,yolov5n,vision
github.com/z1069614715/objectdetection_script,yolov5s,vision
github.com/zheng222/dmfn,vgg19,vision
github.com/zhengyiluo/meva,resnet50,vision
github.com/zhengyiluo/meva,resnet18,vision
github.com/zhengyiluo/universalhumanoidcontrol,resnet18,vision
github.com/zhihou7/batchformer,resnet50,vision
github.com/zhihou7/batchformer,resnet18,vision
github.com/zhihou7/batchformer,vgg16,vision
github.com/zhihou7/batchformer,resnet152,vision
github.com/zhiyuanyou/safecount,resnet18,vision
github.com/zhiyuanyou/safecount,resnet50,vision
github.com/ziniuwan/maed,resnet50,vision
github.com/zju3dv/neuralrecon,mnasnet1_0,vision
github.com/zlckanata/deepglobe-road-extraction-challenge,resnet101,vision
github.com/zlckanata/deepglobe-road-extraction-challenge,resnet34,vision
github.com/zlckanata/deepglobe-road-extraction-challenge,resnet50,vision
github.com/zlckanata/deepglobe-road-extraction-challenge,vgg13,vision
github.com/zpdesu/barbershop,alexnet,vision
github.com/zpdesu/barbershop,resnet101,vision
github.com/zpdesu/barbershop,resnet152,vision
github.com/zpdesu/barbershop,resnet18,vision
github.com/zpdesu/barbershop,resnet34,vision
github.com/zpdesu/barbershop,resnet50,vision
github.com/zpdesu/barbershop,squeezenet1_1,vision
github.com/zpdesu/barbershop,vgg16,vision
github.com/zpdesu/barbershop,vgg19,vision
